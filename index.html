<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>PM Skills Library for Claude</title>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap');

  * { margin: 0; padding: 0; box-sizing: border-box; }

  :root {
    --bg-primary: #0a0a0f;
    --bg-secondary: #12121a;
    --bg-card: #16161f;
    --bg-card-hover: #1c1c28;
    --bg-sidebar: #0e0e16;
    --border: #1e1e2e;
    --text-primary: #e8e8ef;
    --text-secondary: #8b8ba0;
    --text-muted: #5a5a72;
    --accent: #7c5cfc;
    --accent-soft: rgba(124,92,252,0.12);
    --accent-glow: rgba(124,92,252,0.25);
    --green: #34d399;
    --green-soft: rgba(52,211,153,0.12);
    --amber: #fbbf24;
    --amber-soft: rgba(251,191,36,0.12);
    --rose: #fb7185;
    --rose-soft: rgba(251,113,133,0.12);
    --blue: #60a5fa;
    --blue-soft: rgba(96,165,250,0.12);
    --teal: #2dd4bf;
    --teal-soft: rgba(45,212,191,0.12);
    --orange: #fb923c;
    --orange-soft: rgba(251,146,60,0.12);
    --pink: #e879f9;
    --pink-soft: rgba(232,121,249,0.12);
    --cyan: #22d3ee;
    --cyan-soft: rgba(34,211,238,0.12);
    --lime: #a3e635;
    --lime-soft: rgba(163,230,53,0.12);
  }

  body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    background: var(--bg-primary);
    color: var(--text-primary);
    line-height: 1.5;
    min-height: 100vh;
  }

  /* Header */
  .header {
    position: sticky;
    top: 0;
    z-index: 100;
    background: rgba(10,10,15,0.85);
    backdrop-filter: blur(20px);
    border-bottom: 1px solid var(--border);
    padding: 0 32px;
  }
  .header-inner {
    max-width: 1400px;
    margin: 0 auto;
    display: flex;
    align-items: center;
    justify-content: space-between;
    height: 64px;
  }
  .logo {
    display: flex;
    align-items: center;
    gap: 10px;
    font-weight: 700;
    font-size: 18px;
    color: var(--text-primary);
    text-decoration: none;
  }
  .logo-icon {
    width: 32px;
    height: 32px;
    background: linear-gradient(135deg, var(--accent), #a78bfa);
    border-radius: 8px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 16px;
  }
  .header-search {
    position: relative;
    width: 320px;
  }
  .header-search input {
    width: 100%;
    padding: 8px 16px 8px 38px;
    background: var(--bg-secondary);
    border: 1px solid var(--border);
    border-radius: 8px;
    color: var(--text-primary);
    font-size: 14px;
    font-family: inherit;
    outline: none;
    transition: border-color 0.2s;
  }
  .header-search input:focus { border-color: var(--accent); }
  .header-search input::placeholder { color: var(--text-muted); }
  .header-search svg {
    position: absolute;
    left: 12px;
    top: 50%;
    transform: translateY(-50%);
    color: var(--text-muted);
  }
  .header-stats {
    display: flex;
    gap: 24px;
    font-size: 13px;
    color: var(--text-secondary);
  }
  .header-stats span strong {
    color: var(--text-primary);
    font-weight: 600;
  }

  /* Layout */
  .layout {
    max-width: 1400px;
    margin: 0 auto;
    display: flex;
    min-height: calc(100vh - 64px);
  }

  /* Sidebar */
  .sidebar {
    width: 240px;
    flex-shrink: 0;
    padding: 24px 16px;
    border-right: 1px solid var(--border);
    position: sticky;
    top: 64px;
    height: calc(100vh - 64px);
    overflow-y: auto;
  }
  .sidebar::-webkit-scrollbar { width: 4px; }
  .sidebar::-webkit-scrollbar-track { background: transparent; }
  .sidebar::-webkit-scrollbar-thumb { background: var(--border); border-radius: 4px; }

  .sidebar-section {
    margin-bottom: 28px;
  }
  .sidebar-title {
    font-size: 11px;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.08em;
    color: var(--text-muted);
    margin-bottom: 10px;
    padding: 0 8px;
  }
  .filter-btn {
    display: flex;
    align-items: center;
    gap: 8px;
    width: 100%;
    padding: 7px 8px;
    border: none;
    background: transparent;
    color: var(--text-secondary);
    font-size: 13px;
    font-family: inherit;
    border-radius: 6px;
    cursor: pointer;
    transition: all 0.15s;
    text-align: left;
  }
  .filter-btn:hover {
    background: var(--bg-card);
    color: var(--text-primary);
  }
  .filter-btn.active {
    background: var(--accent-soft);
    color: var(--accent);
    font-weight: 500;
  }
  .filter-count {
    margin-left: auto;
    font-size: 11px;
    color: var(--text-muted);
    background: var(--bg-card);
    padding: 1px 6px;
    border-radius: 4px;
  }
  .filter-btn.active .filter-count {
    background: var(--accent-soft);
    color: var(--accent);
  }
  .filter-dot {
    width: 8px;
    height: 8px;
    border-radius: 50%;
    flex-shrink: 0;
  }

  /* Main */
  .main {
    flex: 1;
    padding: 24px 32px;
    min-width: 0;
  }
  .main-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-bottom: 24px;
  }
  .main-title {
    font-size: 20px;
    font-weight: 700;
  }
  .main-subtitle {
    font-size: 13px;
    color: var(--text-secondary);
    margin-top: 2px;
  }
  .view-toggle {
    display: flex;
    gap: 4px;
    background: var(--bg-secondary);
    padding: 3px;
    border-radius: 8px;
    border: 1px solid var(--border);
  }
  .view-btn {
    padding: 6px 12px;
    border: none;
    background: transparent;
    color: var(--text-muted);
    font-size: 13px;
    font-family: inherit;
    border-radius: 5px;
    cursor: pointer;
    transition: all 0.15s;
  }
  .view-btn.active {
    background: var(--bg-card-hover);
    color: var(--text-primary);
    font-weight: 500;
  }

  /* Skills Grid */
  .skills-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
    gap: 16px;
  }

  /* Skill Card */
  .skill-card {
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 12px;
    padding: 20px;
    cursor: pointer;
    transition: all 0.2s;
    position: relative;
    overflow: hidden;
  }
  .skill-card:hover {
    border-color: #2a2a3e;
    background: var(--bg-card-hover);
    transform: translateY(-2px);
    box-shadow: 0 8px 32px rgba(0,0,0,0.3);
  }
  .skill-card-top {
    display: flex;
    align-items: flex-start;
    justify-content: space-between;
    margin-bottom: 12px;
  }
  .skill-badge {
    display: inline-flex;
    align-items: center;
    gap: 5px;
    padding: 3px 10px;
    border-radius: 20px;
    font-size: 11px;
    font-weight: 600;
    letter-spacing: 0.02em;
  }
  .skill-command {
    font-size: 12px;
    color: var(--text-muted);
    font-family: 'SF Mono', 'Fira Code', monospace;
    background: var(--bg-secondary);
    padding: 2px 8px;
    border-radius: 4px;
  }
  .skill-title {
    font-size: 16px;
    font-weight: 600;
    margin-bottom: 8px;
    line-height: 1.3;
  }
  .skill-desc {
    font-size: 13px;
    color: var(--text-secondary);
    line-height: 1.6;
    display: -webkit-box;
    -webkit-line-clamp: 3;
    -webkit-box-orient: vertical;
    overflow: hidden;
  }
  .skill-footer {
    display: flex;
    align-items: center;
    justify-content: space-between;
    margin-top: 16px;
    padding-top: 14px;
    border-top: 1px solid var(--border);
  }
  .skill-frameworks {
    display: flex;
    gap: 6px;
    flex-wrap: wrap;
  }
  .framework-tag {
    font-size: 11px;
    padding: 2px 7px;
    border-radius: 4px;
    background: var(--bg-secondary);
    color: var(--text-muted);
    border: 1px solid var(--border);
  }
  .skill-action {
    display: flex;
    align-items: center;
    gap: 4px;
    font-size: 12px;
    color: var(--accent);
    font-weight: 500;
    white-space: nowrap;
  }
  .skill-action svg { transition: transform 0.2s; }
  .skill-card:hover .skill-action svg { transform: translateX(2px); }

  /* Category colors */
  .cat-docs { background: var(--blue-soft); color: var(--blue); }
  .cat-docs .filter-dot { background: var(--blue); }
  .cat-research { background: var(--green-soft); color: var(--green); }
  .cat-research .filter-dot { background: var(--green); }
  .cat-strategy { background: var(--rose-soft); color: var(--rose); }
  .cat-strategy .filter-dot { background: var(--rose); }
  .cat-prioritization { background: var(--amber-soft); color: var(--amber); }
  .cat-prioritization .filter-dot { background: var(--amber); }
  .cat-metrics { background: var(--teal-soft); color: var(--teal); }
  .cat-metrics .filter-dot { background: var(--teal); }
  .cat-diagnostics { background: var(--orange-soft); color: var(--orange); }
  .cat-diagnostics .filter-dot { background: var(--orange); }
  .cat-design { background: var(--pink-soft); color: var(--pink); }
  .cat-design .filter-dot { background: var(--pink); }
  .cat-productivity { background: var(--cyan-soft); color: var(--cyan); }
  .cat-productivity .filter-dot { background: var(--cyan); }
  .cat-ai { background: var(--lime-soft); color: var(--lime); }
  .cat-ai .filter-dot { background: var(--lime); }

  /* Modal */
  .modal-overlay {
    display: none;
    position: fixed;
    inset: 0;
    background: rgba(0,0,0,0.7);
    backdrop-filter: blur(8px);
    z-index: 200;
    align-items: center;
    justify-content: center;
    padding: 40px;
  }
  .modal-overlay.open { display: flex; }
  .modal {
    background: var(--bg-secondary);
    border: 1px solid var(--border);
    border-radius: 16px;
    max-width: 800px;
    width: 100%;
    max-height: 80vh;
    overflow-y: auto;
    padding: 32px;
    position: relative;
  }
  .modal::-webkit-scrollbar { width: 6px; }
  .modal::-webkit-scrollbar-track { background: transparent; }
  .modal::-webkit-scrollbar-thumb { background: var(--border); border-radius: 6px; }
  .modal-close {
    position: absolute;
    top: 16px;
    right: 16px;
    background: var(--bg-card);
    border: 1px solid var(--border);
    color: var(--text-secondary);
    width: 32px;
    height: 32px;
    border-radius: 8px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 18px;
    transition: all 0.15s;
  }
  .modal-close:hover { background: var(--bg-card-hover); color: var(--text-primary); }

  /* Detail page header */
  .detail-header {
    display: flex;
    align-items: flex-start;
    gap: 16px;
    margin-bottom: 24px;
    padding-bottom: 24px;
    border-bottom: 1px solid var(--border);
  }
  .detail-icon {
    width: 56px;
    height: 56px;
    border-radius: 14px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 24px;
    flex-shrink: 0;
  }
  .detail-meta { flex: 1; min-width: 0; }
  .detail-badge {
    display: inline-flex;
    align-items: center;
    gap: 5px;
    padding: 3px 10px;
    border-radius: 20px;
    font-size: 11px;
    font-weight: 600;
    margin-bottom: 8px;
  }
  .detail-title {
    font-size: 26px;
    font-weight: 700;
    margin-bottom: 4px;
    line-height: 1.2;
  }
  .detail-command {
    font-size: 13px;
    color: var(--accent);
    font-family: 'SF Mono', 'Fira Code', monospace;
  }

  /* Detail sections */
  .detail-section {
    margin-bottom: 24px;
  }
  .detail-section-header {
    display: flex;
    align-items: center;
    gap: 8px;
    margin-bottom: 12px;
  }
  .detail-section-icon {
    width: 28px;
    height: 28px;
    border-radius: 7px;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 14px;
    flex-shrink: 0;
  }
  .detail-section-title {
    font-size: 13px;
    font-weight: 700;
    color: #7f849c;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    margin-bottom: 10px;
  }
  .detail-section-body {
    font-size: 13.5px;
    color: var(--text-secondary);
    line-height: 1.75;
    padding-left: 36px;
  }

  /* Problem card */
  .problem-card {
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-left: 3px solid var(--rose);
    border-radius: 8px;
    padding: 16px 18px;
    font-size: 14px;
    color: var(--text-secondary);
    line-height: 1.7;
  }

  /* Input/Output grid */
  .io-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 12px;
  }
  .io-card {
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 10px;
    padding: 16px;
  }
  .io-card-label {
    font-size: 11px;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.06em;
    margin-bottom: 10px;
    display: flex;
    align-items: center;
    gap: 6px;
  }
  .io-card-label.input { color: var(--blue); }
  .io-card-label.output { color: var(--green); }
  .io-card-title {
    font-size: 13px;
    font-weight: 600;
    color: var(--text-primary);
    margin-bottom: 10px;
    display: flex;
    align-items: center;
    gap: 6px;
  }
  .io-card-list {
    list-style: none;
    padding: 0;
    margin: 0;
  }
  .io-card-list li {
    font-size: 13px;
    color: var(--text-secondary);
    padding: 4px 0;
    display: flex;
    align-items: flex-start;
    gap: 8px;
    line-height: 1.5;
  }
  .io-card-list li::before {
    content: '';
    width: 5px;
    height: 5px;
    border-radius: 50%;
    background: var(--accent);
    margin-top: 7px;
    flex-shrink: 0;
  }
  .io-card ul {
    list-style: none;
    padding: 0;
  }
  .io-card li {
    font-size: 13px;
    color: var(--text-secondary);
    padding: 4px 0;
    display: flex;
    align-items: flex-start;
    gap: 8px;
    line-height: 1.5;
  }
  .io-card li::before {
    content: '';
    width: 5px;
    height: 5px;
    border-radius: 50%;
    margin-top: 7px;
    flex-shrink: 0;
  }
  .io-card.input-card li::before { background: var(--blue); }
  .io-card.output-card li::before { background: var(--green); }

  /* How-to steps */
  .steps-list {
    list-style: none;
    padding: 0;
    counter-reset: step;
  }
  .steps-list li {
    counter-increment: step;
    display: flex;
    align-items: flex-start;
    gap: 12px;
    padding: 10px 0;
    font-size: 13.5px;
    color: var(--text-secondary);
    line-height: 1.6;
  }
  .steps-list li + li { border-top: 1px solid rgba(30,30,46,0.5); }
  .step-num {
    width: 24px;
    height: 24px;
    border-radius: 50%;
    background: var(--accent-soft);
    color: var(--accent);
    font-size: 12px;
    font-weight: 700;
    display: flex;
    align-items: center;
    justify-content: center;
    flex-shrink: 0;
    margin-top: 1px;
  }

  /* Best for tags */
  .bestfor-grid {
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
  }
  .bestfor-tag {
    padding: 8px 14px;
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 8px;
    font-size: 13px;
    color: var(--text-secondary);
    display: flex;
    align-items: center;
    gap: 6px;
    line-height: 1.4;
  }
  .bestfor-tag .bf-icon {
    font-size: 15px;
    flex-shrink: 0;
  }

  /* Frameworks in detail */
  .detail-frameworks {
    display: flex;
    gap: 8px;
    flex-wrap: wrap;
  }
  .detail-framework {
    padding: 7px 14px;
    background: var(--accent-soft);
    border: 1px solid rgba(124,92,252,0.15);
    border-radius: 8px;
    font-size: 13px;
    color: var(--accent);
    font-weight: 500;
  }

  /* Triggers in detail */
  .detail-triggers {
    display: flex;
    gap: 6px;
    flex-wrap: wrap;
  }
  .detail-trigger {
    padding: 4px 10px;
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 4px;
    font-size: 12px;
    color: var(--text-muted);
    font-family: 'SF Mono', 'Fira Code', monospace;
  }

  /* Install box */
  .detail-install {
    margin-top: 24px;
    padding: 20px;
    background: linear-gradient(135deg, rgba(124,92,252,0.08), rgba(96,165,250,0.06));
    border: 1px solid rgba(124,92,252,0.2);
    border-radius: 12px;
  }
  .detail-install-title {
    font-size: 15px;
    font-weight: 600;
    margin-bottom: 4px;
    display: flex;
    align-items: center;
    gap: 8px;
  }
  .detail-install-sub {
    font-size: 13px;
    color: var(--text-muted);
    margin-bottom: 12px;
  }
  .detail-install code {
    display: block;
    padding: 14px;
    background: var(--bg-primary);
    border-radius: 8px;
    font-size: 13px;
    color: var(--green);
    font-family: 'SF Mono', 'Fira Code', monospace;
    overflow-x: auto;
    border: 1px solid var(--border);
  }
  .copy-btn {
    padding: 8px 16px;
    background: var(--accent);
    border: none;
    color: white;
    font-size: 12px;
    font-weight: 600;
    font-family: inherit;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.15s;
    white-space: nowrap;
    flex-shrink: 0;
  }
  .copy-btn:hover { background: #6b4be0; }
  .copy-btn.copied { background: var(--green); }
  .download-btn {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 10px;
    width: 100%;
    padding: 14px 20px;
    background: var(--accent);
    border: none;
    color: white;
    font-size: 15px;
    font-weight: 600;
    font-family: inherit;
    border-radius: 10px;
    cursor: pointer;
    transition: all 0.2s;
  }
  .download-btn:hover { background: #6b4be0; transform: translateY(-1px); }
  .download-btn:active { transform: translateY(0); }
  .install-alt {
    margin-top: 16px;
    padding-top: 16px;
    border-top: 1px solid var(--border);
  }
  .install-alt-label {
    font-size: 12px;
    color: #7f849c;
    display: block;
    margin-bottom: 8px;
  }
  .install-code-row {
    display: flex;
    align-items: center;
    gap: 10px;
    background: rgba(30,30,46,0.6);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 8px 12px;
  }
  .install-code-row code {
    flex: 1;
    font-size: 12px;
    color: var(--green);
    overflow-x: auto;
    white-space: nowrap;
  }
  .inline-code {
    background: rgba(30,30,46,0.8);
    border: 1px solid var(--border);
    padding: 2px 6px;
    border-radius: 4px;
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.85em;
    color: var(--green);
  }
  .steps-list-setup li {
    background: rgba(30,30,46,0.4);
  }

  /* Separator */
  .detail-divider {
    height: 1px;
    background: var(--border);
    margin: 20px 0;
  }

  @media (max-width: 600px) {
    .io-grid { grid-template-columns: 1fr; }
  }

  /* Hero */
  .hero {
    text-align: center;
    padding: 48px 32px 40px;
    position: relative;
  }
  .hero::before {
    content: '';
    position: absolute;
    top: 0;
    left: 50%;
    transform: translateX(-50%);
    width: 600px;
    height: 300px;
    background: radial-gradient(ellipse, var(--accent-glow), transparent 70%);
    pointer-events: none;
    opacity: 0.4;
  }
  .hero h1 {
    font-size: 40px;
    font-weight: 800;
    letter-spacing: -0.02em;
    margin-bottom: 12px;
    background: linear-gradient(135deg, var(--text-primary), var(--accent));
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
  }
  .hero p {
    font-size: 16px;
    color: var(--text-secondary);
    max-width: 560px;
    margin: 0 auto;
    line-height: 1.7;
  }

  /* Empty state */
  .empty-state {
    text-align: center;
    padding: 60px 20px;
    color: var(--text-muted);
  }
  .empty-state svg { margin-bottom: 16px; opacity: 0.3; }
  .empty-state h3 { font-size: 16px; color: var(--text-secondary); margin-bottom: 4px; }
  .empty-state p { font-size: 13px; }

  /* Responsive */
  @media (max-width: 900px) {
    .sidebar { display: none; }
    .header-stats { display: none; }
    .hero h1 { font-size: 28px; }
    .skills-grid { grid-template-columns: 1fr; }
    .main { padding: 16px; }
  }
</style>
</head>
<body>

<!-- Header -->
<header class="header">
  <div class="header-inner">
    <a href="#" class="logo">
      <div class="logo-icon">⚡</div>
      PM Skills Library
    </a>
    <div class="header-search">
      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.35-4.35"/></svg>
      <input type="text" id="searchInput" placeholder="Search skills, frameworks, keywords..." />
    </div>
    <div class="header-stats">
      <span><strong id="skillCount">37</strong> skills</span>
      <span><strong>9</strong> categories</span>
      <span><strong>6+</strong> frameworks</span>
    </div>
  </div>
</header>

<!-- Hero -->
<div class="hero">
  <h1>Claude Skills for Product Managers</h1>
  <p>37 production-ready skills powered by frameworks from Shreyas Doshi, Lenny Rachitsky, Aakash Gupta, Teresa Torres, and more. Drop into your <code>.claude/skills/</code> folder and go.</p>
</div>

<!-- Layout -->
<div class="layout">

  <!-- Sidebar -->
  <aside class="sidebar">
    <div class="sidebar-section">
      <div class="sidebar-title">Quick Filters</div>
      <button class="filter-btn active" data-filter="all">
        <span>All Skills</span>
        <span class="filter-count" id="allCount">37</span>
      </button>
    </div>

    <div class="sidebar-section">
      <div class="sidebar-title">Category</div>
      <button class="filter-btn" data-filter="docs">
        <span class="filter-dot" style="background:var(--blue)"></span>
        Docs & Planning
        <span class="filter-count">6</span>
      </button>
      <button class="filter-btn" data-filter="research">
        <span class="filter-dot" style="background:var(--green)"></span>
        Research & Discovery
        <span class="filter-count">4</span>
      </button>
      <button class="filter-btn" data-filter="strategy">
        <span class="filter-dot" style="background:var(--rose)"></span>
        Strategy & Competitive
        <span class="filter-count">4</span>
      </button>
      <button class="filter-btn" data-filter="prioritization">
        <span class="filter-dot" style="background:var(--amber)"></span>
        Prioritization
        <span class="filter-count">5</span>
      </button>
      <button class="filter-btn" data-filter="metrics">
        <span class="filter-dot" style="background:var(--teal)"></span>
        Metrics & Experiments
        <span class="filter-count">5</span>
      </button>
      <button class="filter-btn" data-filter="diagnostics">
        <span class="filter-dot" style="background:var(--orange)"></span>
        Analysis & Diagnostics
        <span class="filter-count">3</span>
      </button>
      <button class="filter-btn" data-filter="design">
        <span class="filter-dot" style="background:var(--pink)"></span>
        Design & UX
        <span class="filter-count">2</span>
      </button>
      <button class="filter-btn" data-filter="productivity">
        <span class="filter-dot" style="background:var(--cyan)"></span>
        Productivity & Ops
        <span class="filter-count">5</span>
      </button>
      <button class="filter-btn" data-filter="ai">
        <span class="filter-dot" style="background:var(--lime)"></span>
        AI Product Strategy
        <span class="filter-count">3</span>
      </button>
    </div>

    <div class="sidebar-section">
      <div class="sidebar-title">Framework</div>
      <button class="filter-btn" data-framework="shreyas">Shreyas Doshi</button>
      <button class="filter-btn" data-framework="lenny">Lenny Rachitsky</button>
      <button class="filter-btn" data-framework="aakash">Aakash Gupta</button>
      <button class="filter-btn" data-framework="torres">Teresa Torres</button>
      <button class="filter-btn" data-framework="biddle">Gibson Biddle</button>
      <button class="filter-btn" data-framework="nate">Nate B Jones</button>
    </div>
  </aside>

  <!-- Main content -->
  <main class="main">
    <div class="main-header">
      <div>
        <div class="main-title" id="mainTitle">All Skills</div>
        <div class="main-subtitle" id="mainSubtitle">37 skills across 9 categories</div>
      </div>
    </div>
    <div class="skills-grid" id="skillsGrid"></div>
    <div class="empty-state" id="emptyState" style="display:none;">
      <svg width="48" height="48" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.35-4.35"/></svg>
      <h3>No skills match your search</h3>
      <p>Try different keywords or clear filters</p>
    </div>
  </main>
</div>

<!-- Modal -->
<div class="modal-overlay" id="modalOverlay">
  <div class="modal" id="modal">
    <button class="modal-close" id="modalClose">&times;</button>
    <div id="modalContent"></div>
  </div>
</div>

<script>
const skillFiles = {"api-documentation": "---\nname: api-documentation\ndescription: >\n  Write clear, developer-friendly API documentation with authentication, endpoints,\n  request/response examples, error handling, and rate limits. Trigger this skill when\n  the user mentions API docs, API documentation, endpoint documentation, developer docs,\n  API reference, or says things like \"document this API,\" \"write docs for these endpoints,\"\n  or \"create a developer guide.\"\n---\n\n# API Documentation Generator\n\nGood API docs are the difference between a developer integrating in 30 minutes vs.\nfiling a support ticket. Write docs that a developer can follow without reading the\nsource code.\n\n## Before You Start\n\nGather:\n\n1. **API spec or endpoint list** — OpenAPI/Swagger file, route definitions, or verbal description.\n2. **Authentication method** — API key, OAuth 2.0, JWT, session-based.\n3. **Base URL and environments** — Production, staging, sandbox.\n4. **Rate limits** — Requests per minute/hour, burst limits.\n5. **Audience** — External developers, internal teams, or both.\n\n## Documentation Structure\n\n```\n# [API Name] Documentation\n\n## Overview\n[2-3 sentences: what this API does, who it's for, what you can build with it.]\n\n## Base URL\n```\nProduction: https://api.example.com/v1\nSandbox:    https://sandbox.api.example.com/v1\n```\n\n## Authentication\n\n[Explain the auth method with a concrete example]\n\n```bash\ncurl -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  https://api.example.com/v1/resource\n```\n\n**Getting your API key:** [Steps to obtain credentials]\n\n## Quick Start\n\nGet up and running in 3 steps:\n\n1. [Get credentials]\n2. [Make your first request — include a complete curl example]\n3. [Check the response — show what success looks like]\n\n---\n\n## Endpoints\n\n### [Resource Name]\n\n#### Create [Resource]\n`POST /v1/resources`\n\nCreates a new resource.\n\n**Request Headers:**\n| Header | Required | Description |\n|--------|----------|-------------|\n| Authorization | Yes | Bearer token |\n| Content-Type | Yes | application/json |\n\n**Request Body:**\n```json\n{\n  \"name\": \"string (required) — Display name, 1-255 characters\",\n  \"type\": \"string (required) — One of: typeA, typeB, typeC\",\n  \"metadata\": \"object (optional) — Key-value pairs, max 50 keys\"\n}\n```\n\n**Response (201 Created):**\n```json\n{\n  \"id\": \"res_abc123\",\n  \"name\": \"My Resource\",\n  \"type\": \"typeA\",\n  \"metadata\": {},\n  \"created_at\": \"2025-01-15T10:30:00Z\",\n  \"updated_at\": \"2025-01-15T10:30:00Z\"\n}\n```\n\n**Error Responses:**\n| Status | Code | Description |\n|--------|------|-------------|\n| 400 | invalid_request | Missing or invalid fields |\n| 401 | unauthorized | Invalid or missing API key |\n| 409 | conflict | Resource with this name already exists |\n| 422 | unprocessable | Valid JSON but business rule violation |\n| 429 | rate_limited | Too many requests |\n\n[Repeat for each endpoint: List, Get, Update, Delete]\n\n---\n\n## Pagination\n\nList endpoints return paginated results:\n```json\n{\n  \"data\": [...],\n  \"has_more\": true,\n  \"next_cursor\": \"cur_xyz789\"\n}\n```\nPass `?cursor=cur_xyz789` to get the next page. Default page size: 25. Max: 100.\n\n## Error Handling\n\nAll errors follow a consistent format:\n```json\n{\n  \"error\": {\n    \"code\": \"invalid_request\",\n    \"message\": \"Human-readable explanation\",\n    \"details\": { \"field\": \"name\", \"reason\": \"required\" }\n  }\n}\n```\n\n### Common Error Codes\n[Table of all error codes with descriptions and recommended actions]\n\n## Rate Limits\n\n| Plan | Requests/min | Burst |\n|------|-------------|-------|\n| Free | 60 | 10 |\n| Pro | 600 | 100 |\n| Enterprise | Custom | Custom |\n\nRate limit headers included in every response:\n- `X-RateLimit-Limit`: Max requests per window\n- `X-RateLimit-Remaining`: Requests remaining\n- `X-RateLimit-Reset`: Unix timestamp when window resets\n\n## Webhooks (if applicable)\n\n[Event types, payload format, retry policy, signature verification]\n\n## SDKs & Libraries\n\n[Links to official and community SDKs]\n\n## Changelog\n\n[Version history with breaking changes highlighted]\n```\n\n## Writing Guidelines\n\n- **Every endpoint needs a working example.** Curl is the universal language.\n- **Show real responses**, not schemas. Developers copy-paste from docs.\n- **Document error cases as thoroughly as success cases.** Developers spend more\n  time debugging errors than celebrating successes.\n- **Be explicit about required vs. optional fields.** Don't make developers guess.\n- **Include rate limit info per endpoint** if they vary.\n\n## Output\n\nSave as `API-DOCS-[api-name].md`.\n", "feature-brief": "---\nname: feature-brief\ndescription: >\n  Create a concise one-page feature brief for stakeholder alignment before committing to\n  a full PRD. Uses Shreyas Doshi's Three Levels of Product Work to ensure the brief\n  addresses impact, execution, and optics. Trigger this skill when the user mentions\n  feature brief, one-pager, feature proposal, feature pitch, concept doc, or says things\n  like \"quick write-up for leadership,\" \"summarize this feature idea,\" or \"I need to\n  pitch this before we spec it.\"\n---\n\n# Feature Brief Generator\n\nA feature brief is the \"should we even do this?\" document. It's shorter than a PRD,\nfaster to write, and designed to get a yes/no decision from leadership before you invest\nweeks in detailed requirements.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's the idea?** — One sentence.\n2. **Who asked for it?** — Customer request, data insight, strategic initiative, or your own hunch.\n3. **Who needs to approve it?** — This shapes the framing.\n\n## Feature Brief Template\n\n```\n# [Feature Name] — Feature Brief\n\n**Author:** [name] | **Date:** [today] | **Decision needed by:** [date]\n\n---\n\n## The Problem (2-3 sentences)\nWhat's broken, missing, or suboptimal? Use real data or customer quotes.\nDon't describe the solution here — just the pain.\n\n## The Opportunity (2-3 sentences)\nWhy is this worth doing now? Size the opportunity:\n- How many users are affected?\n- What's the revenue/retention/engagement impact?\n- What happens if we do nothing?\n\n## Proposed Approach (3-5 sentences)\nHigh-level solution direction. Not a spec — just enough for someone to picture\nwhat you're proposing. Include what this is NOT (scope boundaries).\n\n## Three Levels Check (Shreyas Doshi)\n\n**Impact:** What measurable outcome will this produce?\n[One sentence with a specific metric target]\n\n**Execution:** Can we build this with current resources in [timeframe]?\n[One sentence on feasibility and team capacity]\n\n**Optics:** How will stakeholders and customers perceive this?\n[One sentence on narrative/positioning]\n\n## Effort vs. Impact\n\n| | Low Effort | High Effort |\n|---|---|---|\n| **High Impact** | [Sweet spot — do this] | [Worth it if strategic] |\n| **Low Impact** | [Quick win or skip] | [Avoid] |\n\nWhere does this feature land? Be honest.\n\n## Key Risks\n1. [Risk 1 — one sentence]\n2. [Risk 2 — one sentence]\n3. [Risk 3 — one sentence]\n\n## Ask\nWhat specifically do you need from the reader?\n- [ ] Approval to write full PRD\n- [ ] Engineering spike to validate feasibility\n- [ ] Design exploration\n- [ ] Data analysis to size the opportunity\n- [ ] Other: [specify]\n```\n\n## Writing Guidelines\n\n- Keep it to ONE page. If you're going over, you're speccing too much.\n- Lead with the problem, not the solution. Decision-makers need to believe the\n  problem is real before they'll invest in solving it.\n- Include one compelling data point or customer quote. Not three — one.\n- The \"Ask\" section is the most important. Be crystal clear about what decision\n  you need and by when.\n\n## Output\n\nSave as `BRIEF-[feature-name].md`.\n", "prd-generator": "---\nname: prd-generator\ndescription: >\n  Transform a problem statement, feature idea, or rough brief into a structured, engineering-ready\n  Product Requirements Document (PRD). Uses Teresa Torres's Opportunity Solution Tree for framing\n  the problem space, Marty Cagan's empowered-team philosophy for solution design, and Shreyas Doshi's\n  Pre-Mortem technique to stress-test before handoff. Trigger this skill whenever the user mentions\n  PRD, product requirements, spec, feature spec, requirements document, or says things like\n  \"write up what we're building\", \"spec this out\", or \"turn this into a doc engineering can use.\"\n---\n\n# PRD Generator\n\nYou are a senior product manager writing a PRD that will align stakeholders and give engineering\neverything they need to start building. A great PRD isn't a feature list — it's a thinking tool\nthat forces clarity on the problem before jumping to solutions.\n\n## Before You Start\n\nAsk the user for whichever of these you don't already have from the conversation:\n\n1. **What problem are we solving?** — In the user's own words. One sentence is fine.\n2. **Who experiences this problem?** — Target user or persona.\n3. **Why now?** — What changed that makes this worth doing in this cycle.\n4. **Any constraints?** — Timeline, tech stack, dependencies, regulatory.\n\nIf the user gives you a vague prompt like \"write a PRD for notifications,\" don't guess. Ask\nwhat's broken about the current state and who's hurting. The problem framing is the most\nvaluable part of the document — get it right.\n\n## PRD Structure\n\nUse the following template. Every section matters; don't skip any unless the user explicitly\nsays to.\n\n```\n# [Feature Name] — PRD\n\n**Author:** [name]\n**Date:** [today]\n**Status:** Draft | In Review | Approved\n**Stakeholders:** [list]\n\n---\n\n## 1. Problem Statement\n\nWrite 2-3 sentences describing the problem from the user's perspective. Ground it in\nobservable behavior or data, not assumptions. Reference the Opportunity Solution Tree:\nwhat's the desired outcome, and what opportunity does this address?\n\nFormat: \"[User type] currently [pain point]. This results in [measurable impact].\nWe believe that [opportunity statement].\"\n\n## 2. Context & Evidence\n\nWhat data, research, or signals support this being a real problem worth solving?\n\n- Customer quotes or support tickets (cite sources)\n- Metrics showing the gap (conversion drop, NPS scores, churn data)\n- Competitive pressure or market shift\n- Internal strategic alignment (link to OKRs or roadmap themes)\n\nBe specific. \"Users are unhappy\" is not evidence. \"37% of trial users drop off at\nstep 3 of onboarding, citing 'too many fields' in exit surveys\" is evidence.\n\n## 3. Goals & Success Metrics\n\n### Primary Goal\nOne sentence. What does success look like 90 days after launch?\n\n### Key Metrics\n| Metric | Current | Target | Measurement Method |\n|--------|---------|--------|--------------------|\n| [metric] | [baseline] | [goal] | [how you'll measure] |\n\nUse Aakash Gupta's input/output methodology: identify the input metrics (actions you\ncan influence) that drive the output metric (business result you care about).\n\n### Non-Goals\nExplicitly list what this project is NOT trying to do. This prevents scope creep\nand aligns expectations. Be specific — \"not trying to redesign the entire dashboard\"\nis better than \"keep scope small.\"\n\n## 4. User Stories & Jobs to Be Done\n\nFrame requirements as JTBD statements first, then break into user stories.\n\n**JTBD:** \"When [situation], I want to [motivation], so I can [expected outcome].\"\n\n**User Stories (INVEST principles):**\n\n| ID | As a... | I want to... | So that... | Priority |\n|----|---------|-------------|-----------|----------|\n| US-1 | [persona] | [action] | [benefit] | P0/P1/P2 |\n\nFor each P0 story, include acceptance criteria:\n- Given [context], when [action], then [expected result]\n\n## 5. Proposed Solution\n\nDescribe what you're building. Include:\n\n- **Solution overview** — 2-3 paragraphs of the approach. Explain WHY this approach\n  over alternatives. Reference Marty Cagan: we're solving the problem, not just\n  shipping a feature.\n- **Key user flows** — Step-by-step walkthrough of the primary happy path.\n  Number each step.\n- **Edge cases** — What happens when things go wrong? Empty states, error handling,\n  permission boundaries.\n- **Wireframes / Mockups** — Link to designs if available. If not, describe the\n  key screens or interactions in enough detail for design to start.\n\n## 6. Technical Considerations\n\nThis section is for engineering to validate feasibility, not for you to architect\nthe solution.\n\n- **Dependencies** — APIs, services, third-party integrations\n- **Data requirements** — New data models, migrations, storage implications\n- **Performance** — Latency targets, scale expectations\n- **Security & Privacy** — PII handling, auth requirements, compliance\n- **Technical risks** — What might be harder than it looks?\n\n## 7. Pre-Mortem (Shreyas Doshi)\n\nBefore this goes to engineering, imagine it's 6 months from now and the project\nhas failed. What went wrong?\n\nList 3-5 realistic failure modes:\n\n| Failure Mode | Likelihood | Impact | Mitigation |\n|-------------|-----------|--------|------------|\n| [what could go wrong] | High/Med/Low | High/Med/Low | [how to prevent] |\n\nThink about: adoption risk (users don't care), execution risk (harder than expected),\nintegration risk (dependencies slip), political risk (stakeholder misalignment).\n\nThis section exists because teams that anticipate failure modes before launch\nhandle them 10x better than teams that discover them in production.\n\n## 8. Rollout Plan\n\n- **Phase 1:** [scope] — [timeline] — [audience]\n- **Phase 2:** [scope] — [timeline] — [audience]\n- **Kill criteria:** Under what conditions do we stop or roll back?\n- **Feature flags:** What can be toggled independently?\n\n## 9. Open Questions\n\nList anything unresolved. For each question, note:\n- Who needs to answer it\n- By when\n- What you'll do if the answer doesn't come (your default assumption)\n\nDon't bury uncertainty. Surface it. Stakeholders respect a PM who says \"I don't\nknow yet, here's how I'll find out\" far more than one who hand-waves.\n\n## 10. Appendix\n\n- Links to research, designs, prior art\n- Competitive screenshots or references\n- Data tables or detailed calculations\n```\n\n## Writing Quality Guidelines\n\nThe PRD should read like it was written by someone who deeply understands the problem,\nnot someone who filled in a template. Specific guidance:\n\n- **Problem section**: Should make a reader who knows nothing about the feature\n  immediately understand why this matters. Use real numbers.\n- **Solution section**: Should be specific enough that two engineers reading it\n  independently would build roughly the same thing.\n- **Pre-mortem**: Should surface genuinely uncomfortable truths, not softball risks.\n  If every risk is \"Low likelihood, Low impact,\" you're not trying hard enough.\n- **Open questions**: Should demonstrate intellectual honesty, not ignorance.\n  Every open question should have a path to resolution.\n\n## Opportunity Cost Check (Shreyas Doshi)\n\nAfter drafting the PRD, pause and ask the user: \"Before we finalize — is this the\nhighest-leverage thing your team could be working on right now? What are you saying\nno to by saying yes to this?\" This question alone often surfaces better alternatives\nor confirms conviction.\n\n## Output\n\nSave the PRD as a markdown file named `PRD-[feature-name].md`. If the user wants a\ndifferent format (Google Doc outline, Notion structure, etc.), adapt accordingly but\nkeep all sections.\n\n## References\n\nFor deeper frameworks referenced in this skill:\n- Read `references/opportunity-solution-tree.md` for Teresa Torres's OST methodology\n- Read `references/pre-mortem-guide.md` for the full Shreyas Doshi pre-mortem process\n- Read `references/input-output-metrics.md` for Aakash Gupta's metrics methodology\n", "release-notes": "---\nname: release-notes\ndescription: >\n  Write customer-facing release notes that highlight value over features. Translates\n  engineering changelogs into benefits users care about. Trigger this skill when the user\n  mentions release notes, changelog, what's new, product update, feature announcement,\n  ship notes, or says things like \"write the release notes for this sprint\" or\n  \"announce what we just shipped.\"\n---\n\n# Release Notes Generator\n\nRelease notes aren't a changelog for engineers — they're a marketing moment. Every\nrelease is a chance to remind customers why they chose your product. Write release\nnotes that make people excited to try the new thing.\n\n## Before You Start\n\nGather:\n\n1. **What shipped?** — Feature list, bug fixes, improvements. A Jira export, commit log,\n   or verbal list works.\n2. **Who's the audience?** — End users, admins, developers, enterprise buyers. This changes\n   the tone and detail level.\n3. **What format?** — In-app notification, email, blog post, help center article, or all of the above.\n4. **Any breaking changes?** — Migration steps, deprecated features, changed behaviors.\n\n## Release Notes Structure\n\n```\n# [Product Name] — [Version / Date] Release Notes\n\n## Highlights\n[1-2 sentence summary of the most exciting change. Lead with the benefit.]\n\n---\n\n### [Feature Name]: [Benefit-oriented headline]\n[2-3 sentences explaining what changed and WHY it matters to the user.\nFocus on the outcome, not the implementation.]\n\n**How to use it:** [1-2 sentences with specific steps or a link to docs]\n\n---\n\n### Improvements\n- **[Area]:** [What improved] — [Why you'll notice]\n- **[Area]:** [What improved] — [Why you'll notice]\n\n### Bug Fixes\n- Fixed an issue where [user-visible symptom] when [trigger condition]\n- Resolved [problem] that affected [who/when]\n\n### Breaking Changes (if any)\n⚠️ **[Change]:** [What's different, who's affected, what to do]\n- Migration steps: [numbered list]\n- Deadline: [date if applicable]\n\n### Coming Soon\n[Optional teaser of what's next — builds anticipation without committing]\n```\n\n## Writing Rules\n\n- **Lead with benefits, not features.** \"Save 2 hours per week with bulk actions\"\n  beats \"Added bulk action support to the admin panel.\"\n- **Use the user's language.** \"Your dashboard loads faster\" not \"Optimized\n  query performance on analytics endpoints.\"\n- **Be specific.** \"3x faster\" beats \"improved performance.\" Include numbers\n  when you have them.\n- **Bug fixes are one-liners.** Users don't need a post-mortem — just confirmation\n  that the annoying thing is gone.\n- **Breaking changes need empathy.** Acknowledge the inconvenience, provide clear\n  migration steps, and offer support.\n- **Skip internal changes.** Refactors, dependency upgrades, and CI improvements\n  are not customer-relevant.\n\n## Tone by Audience\n\n- **End users:** Warm, excited, focused on daily workflow improvements.\n- **Developers/API users:** Technical, precise, include code examples for API changes.\n- **Enterprise admins:** Professional, security-focused, emphasize compliance and control.\n\n## Output\n\nSave as `RELEASE-NOTES-[version-or-date].md`. If multiple formats requested,\ncreate each as a separate file.\n", "tech-spec": "---\nname: tech-spec\ndescription: >\n  Write technical specifications with architecture decisions, data models, API designs,\n  and implementation plans. Bridges the gap between PRD and code. Trigger this skill when\n  the user mentions tech spec, technical specification, architecture doc, system design,\n  engineering spec, implementation plan, or says things like \"how should we build this,\"\n  \"write the technical approach,\" or \"spec the backend for this.\"\n---\n\n# Technical Specification Generator\n\nYou write tech specs that give engineers a clear implementation path while leaving room\nfor smart decisions at the code level. A tech spec answers \"how do we build this?\" — the\nPRD answered \"what and why.\"\n\n## Before You Start\n\nGather:\n\n1. **PRD or feature description** — What are we building and why?\n2. **Current architecture** — What exists today? Tech stack, services, databases.\n3. **Scale requirements** — Expected users, requests/sec, data volume.\n4. **Timeline** — How much time do we have? This affects build-vs-buy decisions.\n5. **Team context** — Who's building this? Their strengths affect approach choices.\n\n## Tech Spec Structure\n\n```\n# [Feature Name] — Technical Specification\n\n**Author:** [name]\n**Date:** [today]\n**Status:** Draft | In Review | Approved\n**PRD:** [link to PRD]\n**Reviewers:** [engineering leads]\n\n---\n\n## 1. Overview\n\n2-3 sentences. What are we building and what's the technical challenge? Reference\nthe PRD for business context — don't repeat it here.\n\n## 2. Architecture\n\n### System Context\nHow does this feature fit into the existing system? What services does it touch?\n\nInclude a simple diagram (ASCII or Mermaid):\n```mermaid\ngraph LR\n  Client --> API_Gateway --> NewService --> Database\n  NewService --> ExistingService\n  NewService --> ThirdPartyAPI\n```\n\n### Key Design Decisions\n\nFor each significant decision, document:\n\n| Decision | Options Considered | Chosen | Rationale |\n|----------|-------------------|--------|-----------|\n| [what] | [option A, B, C] | [chosen] | [why this over alternatives] |\n\n### Components\nList each new or modified component:\n- **[Component name]** — Purpose, responsibilities, interfaces\n\n## 3. Data Model\n\n### New Tables / Collections\n```sql\nCREATE TABLE feature_name (\n  id UUID PRIMARY KEY,\n  user_id UUID NOT NULL REFERENCES users(id),\n  -- fields with types, constraints, indexes\n  created_at TIMESTAMP DEFAULT NOW(),\n  updated_at TIMESTAMP DEFAULT NOW()\n);\n```\n\n### Migrations\n- What changes to existing tables?\n- Is this backwards compatible?\n- What's the rollback plan?\n\n### Data Flow\nHow does data move through the system? Trace a request from client to storage and back.\n\n## 4. API Design\n\n### New Endpoints\n\nFor each endpoint:\n```\nPOST /api/v1/feature\nAuthorization: Bearer <token>\nContent-Type: application/json\n\nRequest:\n{\n  \"field\": \"value\"\n}\n\nResponse (200):\n{\n  \"id\": \"uuid\",\n  \"field\": \"value\",\n  \"created_at\": \"ISO8601\"\n}\n\nError Responses:\n- 400: Invalid input (detail: validation errors)\n- 401: Unauthorized\n- 404: Resource not found\n- 429: Rate limited\n```\n\n### API Versioning & Breaking Changes\nWill this break existing clients? What's the migration path?\n\n## 5. Security & Privacy\n\n- Authentication/authorization requirements\n- PII handling and data classification\n- Encryption at rest and in transit\n- Audit logging requirements\n- GDPR/compliance implications\n\n## 6. Performance\n\n- Latency targets (p50, p95, p99)\n- Throughput requirements\n- Caching strategy\n- Database query optimization notes\n- Load testing plan\n\n## 7. Observability\n\n- Key metrics to instrument\n- Alerting thresholds\n- Dashboard requirements\n- Logging strategy (what to log, what NOT to log)\n- Distributed tracing integration\n\n## 8. Testing Strategy\n\n- Unit test coverage expectations\n- Integration test scenarios\n- End-to-end test critical paths\n- Performance/load test plan\n- Manual QA scope\n\n## 9. Rollout Plan\n\n- Feature flags and gradual rollout\n- Canary deployment strategy\n- Rollback procedure\n- Data backfill requirements (if any)\n\n## 10. Open Questions & Risks\n\n| Question | Owner | Deadline | Default if Unresolved |\n|----------|-------|----------|-----------------------|\n| [question] | [who] | [when] | [assumption we'll use] |\n\n## 11. Dependencies & Timeline\n\n| Milestone | Estimated Duration | Dependencies |\n|-----------|-------------------|--------------|\n| [phase] | [days/weeks] | [blockers] |\n```\n\n## Writing Guidelines\n\n- Be specific about technology choices but explain WHY, not just WHAT.\n- Include code snippets where they clarify intent (schemas, API shapes, algorithms).\n- Don't over-specify implementation details that the engineer should decide.\n- Flag risks early. An honest \"I'm not sure this will scale past 10K RPM\"\n  is more valuable than a confident-but-wrong performance section.\n- Diagrams are worth 1000 words. Use Mermaid syntax for easy rendering.\n\n## Output\n\nSave as `TECH-SPEC-[feature-name].md`.\n", "user-stories": "---\nname: user-stories\ndescription: >\n  Write clear, engineering-ready user stories with acceptance criteria following the INVEST\n  principles. Uses Clayton Christensen's Jobs-to-be-Done as the framing layer and breaks\n  each job into testable stories. Trigger this skill when the user mentions user stories,\n  acceptance criteria, story writing, backlog items, sprint stories, or says things like\n  \"break this into stories,\" \"write tickets for this,\" or \"what should engineering build first.\"\n---\n\n# User Stories Generator\n\nYou write user stories that engineering teams can pick up and build without coming back\nwith 10 clarifying questions. A good user story is a promise: \"If you build this, the user\ngets this benefit, and here's exactly how to verify it works.\"\n\n## Before You Start\n\nGather from the user or the conversation:\n\n1. **What feature or capability are we breaking down?** — PRD link, feature brief, or verbal description.\n2. **Who are the users?** — Specific personas, not \"users.\" A power user and a first-time visitor need different stories.\n3. **What's the scope?** — MVP, full release, or a specific sprint goal.\n4. **Any technical constraints?** — API limitations, platform differences, performance budgets.\n\n## Process\n\n### Step 1: Frame with Jobs-to-be-Done\n\nBefore writing individual stories, identify the job(s) the user is hiring this feature to do.\n\n**JTBD Format:** \"When [situation], I want to [motivation], so I can [expected outcome].\"\n\nExample: \"When I'm reviewing my team's weekly progress, I want to see who's blocked,\nso I can unblock them before standup tomorrow.\"\n\nEach JTBD becomes an **epic**. Stories break down the epic into buildable increments.\n\n### Step 2: Write Stories Using INVEST\n\nEvery story must pass the INVEST checklist:\n\n- **I**ndependent — Can be built and shipped without waiting on other stories\n- **N**egotiable — Describes the what and why, not the how\n- **V**aluable — Delivers observable value to the user or business\n- **E**stimable — Engineering can size it (if they can't, it's too vague or too big)\n- **S**mall — Fits in a single sprint (1-5 days of work typically)\n- **T**estable — Has clear pass/fail acceptance criteria\n\n### Story Template\n\n```\n## [Story ID]: [Short descriptive title]\n\n**As a** [specific persona],\n**I want to** [action/capability],\n**So that** [benefit/outcome].\n\n### Acceptance Criteria\n\n- [ ] Given [precondition], when [action], then [expected result]\n- [ ] Given [precondition], when [action], then [expected result]\n- [ ] Given [edge case], when [action], then [graceful handling]\n\n### Notes\n- [Technical context, design links, dependencies]\n\n### Priority: P0 / P1 / P2\n### Estimate: [S / M / L or story points if team uses them]\n```\n\n### Step 3: Cover the Full Surface\n\nFor each epic/JTBD, make sure you have stories for:\n\n- **Happy path** — The primary flow works as expected\n- **Error states** — What happens when things go wrong (network failure, invalid input, permissions)\n- **Edge cases** — Empty states, max limits, concurrent users, first-time vs. returning\n- **Accessibility** — Keyboard navigation, screen reader support, color contrast\n- **Analytics** — Events to track for measuring success (connect to PRD metrics)\n\n### Step 4: Prioritize and Sequence\n\nOrder stories by:\n1. **P0 (Must have)** — Feature is broken without these. Ship in first sprint.\n2. **P1 (Should have)** — Feature is usable but incomplete without these. Ship in second sprint.\n3. **P2 (Nice to have)** — Polish and delight. Ship if time allows.\n\nFlag any **dependencies** between stories. If Story B can't start until Story A is done,\ncall it out explicitly.\n\n## Quality Check\n\nBefore presenting stories, verify:\n- No story is larger than one sprint of work. If it is, break it down further.\n- Every story has at least 2 acceptance criteria. One is never enough.\n- Acceptance criteria use Given/When/Then format consistently.\n- No story duplicates another. If two stories feel similar, merge or differentiate.\n- The full set of stories covers the JTBD end-to-end. Walk through the user journey mentally.\n\n## Output\n\nPresent stories in a structured markdown document grouped by epic/JTBD, ordered by priority.\nName the file `USER-STORIES-[feature-name].md`.\n", "customer-feedback": "---\nname: customer-feedback\ndescription: >\n  Analyze and categorize customer feedback into actionable themes using affinity mapping.\n  Works with support tickets, NPS responses, app reviews, survey results, or interview\n  notes. Trigger this skill when the user mentions customer feedback, support tickets,\n  NPS analysis, app reviews, survey responses, feedback analysis, voice of customer,\n  VOC, or says things like \"what are customers saying,\" \"categorize this feedback,\"\n  or \"find patterns in these support tickets.\"\n---\n\n# Customer Feedback Analyzer\n\nYou turn messy, unstructured customer feedback into clear themes with prioritized\naction items. The goal is to go from \"we have 500 support tickets\" to \"here are the\n3 things that matter most and why.\"\n\n## Before You Start\n\nAsk the user:\n\n1. **What's the data source?** — Support tickets, NPS comments, app reviews, survey\n   open-ends, interview notes, Slack messages, social media.\n2. **How much data?** — Volume affects approach (10 responses vs. 1000).\n3. **Any hypotheses?** — What does the team already suspect? This helps validate or\n   challenge existing assumptions.\n4. **What's the decision this informs?** — Roadmap prioritization, churn reduction,\n   onboarding improvement. Knowing the decision shapes the analysis.\n\n## Analysis Process\n\n### Step 1: Clean and Normalize\n\n- Remove duplicates and spam\n- Standardize language (e.g., \"app crashes\" and \"keeps crashing\" are the same issue)\n- Note metadata: date, user segment, channel, sentiment\n\n### Step 2: Code Feedback into Themes\n\nUse a bottom-up affinity mapping approach:\n\n1. Read each piece of feedback and assign a **descriptive code** — a short phrase\n   capturing the core issue (e.g., \"slow load times,\" \"confusing pricing page,\"\n   \"missing export feature\").\n2. Group related codes into **themes** — higher-level categories that tell a story\n   (e.g., \"Performance frustrations,\" \"Pricing transparency,\" \"Data portability gaps\").\n3. Each theme should contain 3+ pieces of supporting evidence.\n\nDo NOT start with predetermined categories. Let the themes emerge from the data.\n\n### Step 3: Quantify and Prioritize\n\nFor each theme, capture:\n\n| Theme | Frequency | Severity | User Segment | Trend | Representative Quotes |\n|-------|-----------|----------|-------------- |-------|----------------------|\n| [theme] | [count / %] | High/Med/Low | [who] | Rising/Stable/Declining | [2-3 verbatim quotes] |\n\n**Severity assessment:**\n- **High:** Users churning, revenue impact, or blocking core workflows\n- **Medium:** Users frustrated but finding workarounds\n- **Low:** Nice-to-have improvements, minor friction\n\n### Step 4: Identify Insights\n\nFor each major theme, write an insight statement:\n\n**Format:** \"[User segment] is [experiencing/struggling with] [specific problem]\nbecause [root cause]. This is causing [business impact]. Evidence: [N] mentions\nacross [channels], trending [direction].\"\n\nGood insight: \"Enterprise trial users are abandoning onboarding at step 3 because\nthe team invitation flow requires admin permissions they don't have yet. This is\ncontributing to 23% trial drop-off. 47 mentions in support tickets this quarter,\nup from 12 last quarter.\"\n\nBad insight: \"Users don't like the onboarding.\"\n\n### Step 5: Recommend Actions\n\nFor each insight, suggest:\n- **Quick win** — Something fixable in under a week\n- **Strategic investment** — A larger initiative that addresses the root cause\n- **Further research needed** — What you'd want to validate before committing\n\n## Output Structure\n\n```\n# Customer Feedback Analysis — [Source/Date Range]\n\n## Executive Summary\n[3-4 sentences: top themes, biggest surprise, recommended priority]\n\n## Theme Breakdown\n\n### Theme 1: [Name] — [Frequency] mentions ([X]%)\n**Insight:** [insight statement]\n**Severity:** [High/Med/Low] | **Trend:** [Rising/Stable/Declining]\n**Key Quotes:**\n- \"[verbatim quote]\" — [user type, date]\n- \"[verbatim quote]\" — [user type, date]\n**Recommendations:**\n- Quick win: [action]\n- Strategic: [action]\n\n[Repeat for each theme]\n\n## Sentiment Overview\n- Positive: [X]%\n- Neutral: [X]%\n- Negative: [X]%\n- [Notable shift from previous period, if known]\n\n## Methodology\n- Data source: [source]\n- Volume: [N] pieces of feedback\n- Date range: [range]\n- Coding approach: Bottom-up affinity mapping\n```\n\nSave as `FEEDBACK-ANALYSIS-[source]-[date].md`.\n", "interview-synthesis": "---\nname: interview-synthesis\ndescription: >\n  Transform interview transcripts into structured insights with quotes, patterns, and\n  recommendations. Based on Teresa Torres's Continuous Discovery framework. Handles\n  single interviews or batch analysis of 5-10 interviews in parallel. Trigger this skill\n  when the user mentions interview synthesis, research synthesis, interview analysis,\n  transcript analysis, user interviews, customer interviews, or says things like\n  \"summarize these interviews,\" \"what did we learn from research,\" or \"pull insights\n  from these transcripts.\"\n---\n\n# Interview Synthesis\n\nYou transform raw interview transcripts into structured insights that product teams\ncan act on. The goal is to surface patterns across conversations, not just summarize\neach one individually.\n\n## Before You Start\n\nAsk the user:\n\n1. **How many interviews?** — Single interview vs. batch (5-10).\n2. **Research questions** — What were you trying to learn? This focuses the analysis.\n3. **Interview guide** — If available, helps identify which questions map to which themes.\n4. **Participant context** — User segments, experience levels, account types.\n\n## Single Interview Synthesis\n\n### Step 1: Extract Key Moments\n\nRead the transcript and tag every meaningful moment:\n\n- **Pain points** — Frustrations, struggles, workarounds\n- **Unmet needs** — Wishes, desired outcomes, gaps\n- **Behaviors** — What they actually do (vs. what they say they do)\n- **Emotions** — Strong reactions, enthusiasm, resignation\n- **Surprises** — Anything that contradicts your assumptions\n- **Verbatim quotes** — Capture exact words for the most compelling moments\n\n### Step 2: Structure the Synthesis\n\n```\n# Interview Synthesis — [Participant ID/Name]\n\n**Date:** [date] | **Duration:** [minutes] | **Segment:** [user type]\n**Interviewer:** [name] | **Research questions:** [1-2 sentence summary]\n\n## Key Insights\n\n### Insight 1: [Descriptive title]\n[2-3 sentences explaining the insight]\n**Quote:** \"[verbatim]\"\n**Implication:** [What this means for the product]\n\n### Insight 2: [Descriptive title]\n[repeat pattern]\n\n## Behavioral Observations\n- [What they do, not what they say]\n- [Workarounds observed]\n- [Tools/processes mentioned]\n\n## Jobs to be Done (identified)\n- \"When [situation], I want to [motivation], so I can [outcome].\"\n\n## Surprises / Contradictions\n- [Things that challenged assumptions]\n\n## Follow-up Questions\n- [What you'd ask next time based on what you learned]\n\n## Raw Tags\n[Flat list of all coded moments for cross-interview analysis]\n```\n\n## Batch Synthesis (5-10 Interviews)\n\n### Step 1: Synthesize Each Interview Individually\nRun the single interview process for each transcript.\n\n### Step 2: Cross-Interview Pattern Analysis\n\nBuild a theme matrix:\n\n| Theme | P1 | P2 | P3 | P4 | P5 | Total | Strength |\n|-------|----|----|----|----|-----|-------|----------|\n| [theme] | x | x | | x | | 3/5 | Strong |\n\n**Pattern strength:**\n- **Strong (60%+):** Mentioned by most participants — likely a real pattern\n- **Moderate (40-59%):** Worth investigating further\n- **Emerging (20-39%):** May be segment-specific — check who mentioned it\n- **Anecdotal (<20%):** Interesting but don't build a roadmap on it\n\n### Step 3: Insight Hierarchy\n\nOrganize findings using Teresa Torres's framework:\n\n```\nDesired Outcome: [What we're trying to achieve]\n├── Opportunity 1: [Pattern/theme] — Strong (4/6 participants)\n│   ├── Evidence: [key quotes across participants]\n│   ├── Implication: [what to build or change]\n│   └── Confidence: High / Medium / Low\n├── Opportunity 2: [Pattern/theme] — Moderate (3/6 participants)\n│   └── ...\n└── Opportunity 3: [Pattern/theme] — Emerging (2/6 participants)\n    └── ...\n```\n\n### Step 4: Batch Output\n\n```\n# Research Synthesis — [Study Name]\n\n## Study Overview\n- **Research questions:** [what we set out to learn]\n- **Participants:** [N] interviews, [segments represented]\n- **Date range:** [when interviews conducted]\n\n## Executive Summary\n[3-5 sentences: top findings, biggest surprise, confidence level, recommended next steps]\n\n## Top Findings\n\n### Finding 1: [Theme] — [Strong/Moderate/Emerging]\n**Pattern:** [2-3 sentences describing what you found]\n**Evidence:** Mentioned by [N/total] participants\n- \"[Quote]\" — P1 ([segment])\n- \"[Quote]\" — P3 ([segment])\n**Segment differences:** [Any variation by user type]\n**Recommendation:** [What to do with this insight]\n\n[Repeat for each major finding]\n\n## Opportunity Map\n[Teresa Torres opportunity tree connecting findings to desired outcomes]\n\n## Methodology Notes\n- Potential biases: [sampling bias, leading questions, small sample]\n- Confidence level: [High/Medium/Low] and why\n- Recommended follow-up: [What to validate quantitatively]\n```\n\nSave as `RESEARCH-SYNTHESIS-[study-name].md`.\n", "jtbd-extractor": "---\nname: jtbd-extractor\ndescription: >\n  Extract Jobs-to-be-Done statements from research data to uncover innovation opportunities.\n  Uses Clayton Christensen's JTBD framework to move beyond feature requests to underlying\n  motivations. Trigger this skill when the user mentions jobs to be done, JTBD, customer\n  jobs, outcome-driven innovation, job stories, or says things like \"what job is the user\n  hiring our product for,\" \"find the underlying need,\" or \"go beyond feature requests.\"\n---\n\n# JTBD Extractor\n\nFeature requests tell you what customers think they want. Jobs-to-be-Done tell you\nwhat they actually need. This skill extracts JTBD statements from raw research data\nso you can build for motivations, not wish lists.\n\n## Core Concept\n\nA \"job\" is the progress someone is trying to make in a particular circumstance.\nPeople don't buy products — they hire them to do a job. Understanding the job unlocks\nsolutions the customer couldn't have articulated.\n\n## Before You Start\n\nAsk the user:\n\n1. **What research data do we have?** — Interview transcripts, survey responses, support\n   tickets, behavioral data, feature requests.\n2. **What domain?** — Product area, user type, business context.\n3. **What decision will this inform?** — New feature ideation, positioning, roadmap\n   prioritization, competitive differentiation.\n\n## Extraction Process\n\n### Step 1: Identify Job Signals in Raw Data\n\nScan the data for signals that reveal underlying jobs:\n\n- **Struggle moments:** \"I always have to...\" \"It's frustrating when...\"\n- **Workarounds:** \"What I usually do instead is...\" \"I use [competitor/hack] for...\"\n- **Desired outcomes:** \"I wish I could...\" \"If only it would...\"\n- **Context switches:** \"When I'm in [situation]...\" \"Right before [event]...\"\n- **Hiring/firing moments:** \"I started using...\" \"I stopped using...\"\n\n### Step 2: Formulate Job Statements\n\n**Main Job Statement Format:**\n\"When [situation/trigger], I want to [motivation/progress], so I can [desired outcome].\"\n\n**Rules for good job statements:**\n- The job exists independent of your product. It existed before your product and\n  will exist if your product disappears.\n- Focus on the progress, not the solution. \"Quickly understand my team's status\"\n  not \"view a dashboard.\"\n- Include the circumstance. The same person has different jobs in different contexts.\n- One job per statement. If it has \"and\" in the motivation, it's probably two jobs.\n\n### Step 3: Map the Job Layers\n\nFor each main job, identify:\n\n**Functional job:** The practical task they're trying to accomplish.\n\"Quickly identify which deals are at risk of slipping this quarter.\"\n\n**Emotional job:** How they want to feel during and after.\n\"Feel confident I'm not missing anything when I report to the VP.\"\n\n**Social job:** How they want to be perceived.\n\"Be seen as the manager who always has their finger on the pulse.\"\n\n### Step 4: Identify Outcome Expectations\n\nFor each job, list the desired outcomes using this format:\n\n**\"Minimize the [time/likelihood/effort] of [undesired outcome]\"**\nor\n**\"Maximize the [speed/accuracy/completeness] of [desired outcome]\"**\n\nExamples:\n- Minimize the time it takes to identify at-risk deals\n- Minimize the likelihood of being surprised by a missed target\n- Maximize the accuracy of my forecast to leadership\n\n### Step 5: Assess Opportunity\n\nRate each outcome on two dimensions:\n\n| Outcome | Importance (1-10) | Satisfaction (1-10) | Opportunity Score |\n|---------|-------------------|--------------------|--------------------|\n| [outcome] | [how much they care] | [how well current solutions work] | Importance + (Importance - Satisfaction) |\n\n**Opportunity score > 15:** Underserved — high innovation potential\n**Opportunity score 10-15:** Appropriately served — incremental improvement\n**Opportunity score < 10:** Overserved — don't invest here\n\n## Output Structure\n\n```\n# Jobs-to-be-Done Analysis — [Domain/Product Area]\n\n## Summary\n[2-3 sentences: how many jobs identified, biggest unmet need, key insight]\n\n## Job Map\n\n### Job 1: [Main job statement]\n**Functional:** [practical task]\n**Emotional:** [desired feeling]\n**Social:** [desired perception]\n\n**Context/Trigger:** [when does this job arise?]\n**Current Solutions:** [what they hire today — competitors, workarounds, manual processes]\n\n**Desired Outcomes:**\n| Outcome | Importance | Satisfaction | Opportunity |\n|---------|-----------|-------------|-------------|\n| [outcome] | [1-10] | [1-10] | [score] |\n\n**Evidence:** [N] data points\n- \"[supporting quote]\" — [source]\n- [behavioral data point]\n\n[Repeat for each job]\n\n## Opportunity Ranking\n[Rank all outcomes by opportunity score — highest = biggest unmet need]\n\n## Strategic Implications\n[What should we build, stop building, or explore further based on this analysis?]\n```\n\nSave as `JTBD-ANALYSIS-[domain].md`.\n", "user-personas": "---\nname: user-personas\ndescription: >\n  Create data-backed user personas that go beyond demographics to capture behaviors,\n  motivations, and jobs-to-be-done. Trigger this skill when the user mentions personas,\n  user profiles, user segments, user archetypes, customer profiles, or says things like\n  \"who are our users,\" \"create personas for this product,\" or \"define our target users.\"\n---\n\n# User Persona Generator\n\nA useful persona isn't a stock photo with a fake name and hobbies. It's a behavioral\nmodel that helps the team make better product decisions by asking \"what would [persona]\ndo?\" Personas work when they're rooted in real data and capture what drives behavior.\n\n## Before You Start\n\nAsk the user:\n\n1. **What data do we have?** — Interview transcripts, analytics segments, survey data,\n   support patterns, sales notes.\n2. **How many personas?** — 3-5 is the sweet spot. More than 5 means the team will\n   ignore most of them. Fewer than 3 means you're probably missing a segment.\n3. **What decisions will these inform?** — Feature prioritization, onboarding flows,\n   marketing messaging, pricing tiers.\n4. **Current assumptions** — Who does the team think the users are? This helps\n   identify blind spots.\n\n## Persona Structure\n\n```\n# [Persona Name] — [One-line descriptor]\n\n> \"[A quote that captures this persona's core motivation]\"\n\n## Overview\n**Role/Title:** [professional context]\n**Experience level:** [with the product/domain]\n**Segment size:** [% of user base, if known]\n**Revenue contribution:** [% of revenue, if known]\n\n## Behavioral Profile\n\n### Goals (what they're trying to achieve)\n1. [Primary goal — the job they're hiring your product for]\n2. [Secondary goal]\n3. [Tertiary goal]\n\n### Frustrations (what gets in their way)\n1. [Primary pain point — with evidence]\n2. [Secondary pain point]\n3. [What they complain about but isn't actually the real problem]\n\n### Behaviors (what they actually do)\n- **Frequency:** [How often they use the product]\n- **Key workflows:** [Top 3 things they do]\n- **Workarounds:** [Hacks they've built around product limitations]\n- **Tools in ecosystem:** [What else they use alongside your product]\n- **Decision process:** [How they evaluate and adopt new features]\n\n### Motivations (why they care)\n\n**Functional:** [Practical outcome they need]\n**Emotional:** [How they want to feel]\n**Social:** [How they want to be perceived]\n\n## Context & Triggers\n\n**When do they reach for your product?**\n- [Trigger moment 1] — [situation that creates the need]\n- [Trigger moment 2]\n\n**When do they NOT use your product (but could)?**\n- [Missed opportunity 1] — [what they use instead]\n\n## Success Metrics (from their perspective)\n- [How THEY measure whether your product is working for them]\n- [Not your metrics — their metrics]\n\n## Relationship with Product\n- **Acquisition channel:** [How they typically find you]\n- **Aha moment:** [When they first see value]\n- **Retention driver:** [Why they keep coming back]\n- **Churn risk:** [What would make them leave]\n\n## Design Implications\n- [Specific product/design decisions this persona drives]\n- [Features to prioritize FOR this persona]\n- [Features to de-prioritize — they don't care about these]\n\n## Anti-patterns\n- [Common mistake teams make when designing for this persona]\n```\n\n## Quality Checks\n\nBefore presenting personas, verify:\n\n- **Behavioral, not demographic.** Age, gender, and location should only appear if\n  they genuinely drive different product behavior. \"35-year-old in San Francisco\"\n  tells you nothing useful. \"Power user who manages 50+ projects and needs keyboard\n  shortcuts\" tells you everything.\n- **Evidence-backed.** Every claim should trace to data. If you can't cite evidence,\n  mark it as an assumption that needs validation.\n- **Distinct.** If two personas would make the same product decisions, merge them.\n  The test: would the team build something different for Persona A vs. Persona B?\n- **Actionable.** Each persona should drive at least one product decision.\n  If a persona doesn't change any decisions, it's not useful.\n\n## Output\n\nSave as `PERSONAS-[product-name].md`. Create one document with all personas\nfor easy comparison. Include a summary comparison table at the top.\n", "competitive-landscape": "---\nname: competitive-landscape\ndescription: >\n  Map multiple competitors into a unified landscape with positioning matrices, feature\n  comparisons, and market segment analysis. Broader than single-competitor analysis —\n  this covers the entire competitive field. Trigger this skill when the user mentions\n  competitive landscape, market map, competitive overview, market analysis, industry\n  landscape, or says things like \"map out all our competitors,\" \"who's in this space,\"\n  or \"create a market overview.\"\n---\n\n# Competitive Landscape Mapper\n\nYou map entire competitive markets, not just individual competitors. This skill produces\na bird's-eye view of who's playing in the space, how they're positioned, where the\nwhite space is, and where the market is headed.\n\n## Before You Start\n\nAsk the user:\n\n1. **What market/category?** — Define the boundaries of the landscape.\n2. **How many competitors?** — Known players to include, plus a request to identify\n   ones the user might be missing.\n3. **What's the purpose?** — Board presentation, investor deck, strategic planning,\n   new market entry assessment.\n4. **Time horizon** — Current state only, or include trajectory/predictions.\n\n## Landscape Structure\n\n```\n# Competitive Landscape — [Market/Category]\n\n**Date:** [today] | **Author:** [name]\n\n## Market Overview\n[3-5 sentences: market size, growth rate, key trends reshaping the space, maturity stage]\n\n## Player Map\n\n### Tier 1: Market Leaders\n| Company | Est. Revenue | Funding | Key Strength | Target Segment |\n|---------|-------------|---------|-------------|----------------|\n| [company] | [range] | [total] | [one phrase] | [who they serve] |\n\n### Tier 2: Strong Challengers\n[Same table format]\n\n### Tier 3: Emerging / Niche\n[Same table format]\n\n### Adjacent Threats\n[Companies not directly competing today but could enter — and why]\n\n## Positioning Matrix\n\n### Matrix 1: [Primary dimensions]\n[2x2 grid with all players plotted — choose dimensions that reveal the most\nstrategic insight for the user's context]\n\n### Matrix 2: [Secondary dimensions]\n[Different 2x2 that reveals a different strategic truth]\n\n## Feature Landscape\n\n| Capability | Leader A | Leader B | Challenger C | Us | Market Table Stakes |\n|-----------|---------|---------|-------------|-----|-------------------|\n| [feature] | [status] | [status] | [status] | [status] | Yes/No |\n\nColor-code or tag: ✅ Strong | ⚡ Competitive | ⭕ Gap | ❌ Not offered\n\n**Table stakes** = capabilities customers expect from any player. If you don't\nhave these, you're not in the conversation.\n\n## Market Segments\n\n| Segment | Dominant Player | Underserved? | Our Opportunity |\n|---------|----------------|-------------|-----------------|\n| [segment] | [who wins here] | [yes/no + why] | [what we could do] |\n\n## Trend Analysis\n\n### Where the market is headed (12-18 months)\n1. [Trend] — [Impact on competitive dynamics]\n2. [Trend] — [Impact]\n3. [Trend] — [Impact]\n\n### Potential disruptions\n- [Technology shift, regulatory change, or business model innovation that could\n  reshape the landscape]\n\n## White Space Analysis\n[Where is there opportunity that no one is serving well?]\n- [Opportunity 1]: [why it exists, who would value it, barrier to entry]\n- [Opportunity 2]: [same]\n\n## Strategic Implications for [Our Product]\n1. [Where to compete — and where NOT to]\n2. [Differentiation opportunities]\n3. [Partnership or acquisition targets]\n4. [Timeline urgency — is the window closing?]\n```\n\nSave as `LANDSCAPE-[market]-[date].md`.\n", "competitor-analysis": "---\nname: competitor-analysis\ndescription: >\n  Create comprehensive competitor analysis using Gibson Biddle's DHM framework (Delight,\n  Hard-to-copy, Margin-enhancing) with strategic positioning insights. Trigger this skill\n  when the user mentions competitor analysis, competitive analysis, competitive intelligence,\n  battlecard, DHM framework, or says things like \"analyze this competitor,\" \"how do we\n  compare to [company],\" or \"build a competitive brief.\"\n---\n\n# Competitor Analysis (DHM Framework)\n\nYou build competitor analyses that go beyond feature comparison checklists. Using Gibson\nBiddle's DHM framework, you evaluate whether competitive advantages are sustainable —\nnot just whether a feature exists.\n\n## Before You Start\n\nAsk the user:\n\n1. **Which competitor(s)?** — Specific companies or \"the competitive landscape.\"\n2. **What's the context?** — Sales battlecard, strategic planning, board prep, feature gap analysis.\n3. **What data do we have?** — Product demos, pricing pages, customer feedback mentioning\n   competitors, G2/Capterra reviews, analyst reports.\n4. **Our positioning** — What's our current value prop so we can compare honestly.\n\n## Analysis Framework\n\n### Step 1: Product Capability Mapping\n\n| Capability | Us | Competitor A | Competitor B | Notes |\n|-----------|-----|-------------|-------------|-------|\n| [capability] | [rating/status] | [rating/status] | [rating/status] | [context] |\n\nUse specific ratings: \"Best in class\" / \"Competitive\" / \"Gap\" / \"Not offered\"\n— not vague 1-5 scales.\n\n### Step 2: DHM Assessment (Gibson Biddle)\n\nFor each competitor, evaluate their product through three lenses:\n\n**Delight:** What do their users genuinely love about the product?\n- What generates word-of-mouth?\n- What would users miss most if they switched away?\n- What do G2/Capterra reviewers praise most?\n\n**Hard-to-copy:** Which advantages are defensible?\n- Network effects (more users = more value)\n- Proprietary data or technology\n- Brand and trust accumulated over years\n- Ecosystem/integration lock-in\n- Switching costs (data, workflow, training)\n- Economies of scale\n\n**Margin-enhancing:** Does their model sustain investment?\n- Pricing power (can they raise prices without losing customers?)\n- Unit economics trajectory\n- Expansion revenue vs. new customer dependency\n- Funding/cash position for sustained competition\n\n### Step 3: Strategic Positioning Map\n\nPlace competitors on a 2x2 matrix. Choose the two dimensions most relevant to\nthe user's context:\n\nCommon axis pairs:\n- **Ease of use** vs. **Power/Depth**\n- **SMB focus** vs. **Enterprise focus**\n- **Point solution** vs. **Platform**\n- **Low price** vs. **Premium**\n\n```\n                    High [Dimension 2]\n                         |\n          Competitor A   |   Us\n                         |\n  Low [Dim 1] ----------+---------- High [Dim 1]\n                         |\n          Competitor C   |   Competitor B\n                         |\n                    Low [Dimension 2]\n```\n\n### Step 4: Win/Loss Analysis\n\n| Scenario | We Win When... | We Lose When... |\n|----------|---------------|-----------------|\n| vs. Competitor A | [conditions] | [conditions] |\n| vs. Competitor B | [conditions] | [conditions] |\n\n### Step 5: Strategic Implications\n\nFor each competitor, answer:\n- **What should we copy?** — Features or approaches clearly working for them.\n- **What should we counter?** — Where we can differentiate instead of matching.\n- **What should we ignore?** — Their moves that are irrelevant to our strategy.\n- **What's their likely next move?** — Based on their trajectory, hiring, and messaging.\n\n## Output Structure\n\n```\n# Competitive Analysis — [Competitor(s)] vs. [Our Product]\n\n## Executive Summary\n[3-4 sentences: key finding, biggest threat, biggest opportunity, recommended action]\n\n## Feature Comparison Matrix\n[Detailed capability table]\n\n## DHM Assessment\n### [Competitor A]\n**Delight:** [what users love]\n**Hard-to-copy:** [defensible advantages]\n**Margin-enhancing:** [business model strength]\n**Overall threat level:** High / Medium / Low\n\n[Repeat per competitor]\n\n## Positioning Map\n[2x2 visual + narrative explanation]\n\n## Win/Loss Patterns\n[Table with conditions]\n\n## Strategic Recommendations\n1. [Action item with rationale]\n2. [Action item with rationale]\n3. [Action item with rationale]\n\n## Sources\n[List all data sources used in the analysis]\n```\n\nSave as `COMPETITOR-ANALYSIS-[competitor]-[date].md`.\n", "gtm-strategy": "---\nname: gtm-strategy\ndescription: >\n  Build a complete Go-to-Market strategy with target segments, channels, messaging,\n  pricing approach, launch timeline, and success metrics. Incorporates Aakash Gupta's\n  PLG layers where relevant. Trigger this skill when the user mentions go to market,\n  GTM, launch strategy, market entry, product launch plan, or says things like \"how\n  do we bring this to market,\" \"plan the launch,\" or \"GTM for this feature.\"\n---\n\n# Go-to-Market Strategy Generator\n\nA GTM strategy is the bridge between \"we built it\" and \"people use it and pay for it.\"\nThis skill produces a structured GTM plan that covers who you're targeting, how you'll\nreach them, what you'll say, and how you'll measure success.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's launching?** — New product, new feature, new market expansion, pricing change.\n2. **Target audience** — Who are the first users? Be specific about the segment.\n3. **Business model** — PLG/self-serve, sales-led, hybrid. This fundamentally changes the GTM.\n4. **Timeline** — Hard launch date or flexible? Any external dependencies (conference, funding round)?\n5. **Budget/resources** — Marketing budget, sales team size, growth engineering capacity.\n6. **Competitive context** — Who else serves this audience? What's our differentiation?\n\n## GTM Strategy Structure\n\n```\n# Go-to-Market Strategy — [Product/Feature Name]\n\n**Launch date:** [date] | **Author:** [name] | **Status:** Draft\n\n---\n\n## 1. Market Opportunity\n\n**Target segment:** [Specific description — not \"everyone\"]\n**Segment size:** [TAM/SAM/SOM with sources]\n**Why now:** [Market timing, competitive window, internal readiness]\n**Key insight:** [The one thing we know about this market that others don't]\n\n## 2. Positioning & Messaging\n\n### Positioning Statement\nFor [target audience] who [need/pain point], [Product] is a [category]\nthat [key benefit]. Unlike [competitive alternative], we [key differentiator].\n\n### Messaging Hierarchy\n\n**Primary message (headline):** [10 words max — the benefit that matters most]\n\n**Supporting messages:**\n1. [Message pillar 1] — [proof point]\n2. [Message pillar 2] — [proof point]\n3. [Message pillar 3] — [proof point]\n\n**Objection handling:**\n| Objection | Response |\n|-----------|----------|\n| \"[common pushback]\" | [how to address it] |\n\n## 3. Growth Model\n\n### If Product-Led Growth (Aakash Gupta's 7 Layers):\n\n**Acquisition:** How do users discover the product?\n- [Channel 1]: [approach + expected contribution]\n- [Channel 2]: [approach + expected contribution]\n\n**Activation:** What's the \"aha moment\" and how fast do users reach it?\n- [Define the activation event]\n- [Target: X% of signups activate within Y days]\n\n**Retention:** What brings users back?\n- [Core loop description]\n- [Target: X% D7 retention, Y% D30 retention]\n\n**Revenue:** What triggers the upgrade from free to paid?\n- [Monetization trigger]\n- [Pricing strategy: freemium / free trial / reverse trial]\n\n**Referral:** How do users bring other users?\n- [Viral mechanism — built-in sharing, team invites, etc.]\n- [Target viral coefficient]\n\n### If Sales-Led:\n- **ICP definition:** [Firmographics + technographics + buying signals]\n- **Sales motion:** [Inbound, outbound, partner, or hybrid]\n- **Deal size target:** [$X ACV]\n- **Sales cycle:** [Expected length]\n- **Required collateral:** [Demo script, case studies, ROI calculator, etc.]\n\n## 4. Channel Strategy\n\n| Channel | Role | Investment | Expected Outcome | Timeline |\n|---------|------|-----------|-----------------|----------|\n| [channel] | [awareness/acquisition/activation] | [budget/effort] | [metric] | [when] |\n\n## 5. Launch Plan\n\n### Pre-launch (T-4 weeks)\n- [ ] [Action item] — [owner] — [date]\n\n### Launch week\n- [ ] [Action item] — [owner] — [date]\n\n### Post-launch (T+4 weeks)\n- [ ] [Action item] — [owner] — [date]\n\n## 6. Success Metrics\n\n| Metric | Target (30 days) | Target (90 days) | Measurement |\n|--------|-----------------|-----------------|-------------|\n| [metric] | [target] | [target] | [tool/method] |\n\n### Kill Criteria\nIf [condition] by [date], we will [pivot/sunset/re-evaluate].\n\n## 7. Risks & Mitigations\n\n| Risk | Likelihood | Impact | Mitigation |\n|------|-----------|--------|------------|\n| [risk] | High/Med/Low | High/Med/Low | [plan] |\n```\n\nSave as `GTM-STRATEGY-[product-name].md`.\n", "stakeholder-simulation": "---\nname: stakeholder-simulation\ndescription: >\n  Simulate how different stakeholders will react to a proposal and prepare responses.\n  Anticipate objections from engineering, design, leadership, sales, legal, and finance\n  before the meeting happens. Trigger this skill when the user mentions stakeholder\n  management, stakeholder simulation, objection handling, meeting prep for proposal,\n  anticipate pushback, or says things like \"how will [person/team] react to this,\"\n  \"prepare me for pushback,\" or \"what questions will the exec team ask.\"\n---\n\n# Stakeholder Simulation\n\nYou help PMs prepare for the hardest part of the job: getting alignment from people\nwith different priorities, incentives, and mental models. This skill simulates how\neach stakeholder will react to a proposal so you can prepare responses before the\nmeeting, not during it.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's the proposal?** — PRD, feature brief, roadmap change, resource request, or strategic shift.\n2. **Who are the stakeholders?** — Names, roles, and their known priorities/concerns.\n3. **What's the ask?** — Approval, resources, timeline commitment, or just buy-in.\n4. **Any known tensions?** — Political dynamics, competing priorities, history.\n5. **Meeting format** — 1:1, small group, leadership review, all-hands.\n\n## Simulation Process\n\n### Step 1: Build Stakeholder Profiles\n\nFor each key stakeholder:\n\n```\n### [Name] — [Role]\n\n**What they care about most:** [Their #1 priority]\n**How they evaluate proposals:** [Data-driven? Vision-driven? Customer-driven? Cost-driven?]\n**Known concerns:** [What keeps them up at night]\n**Communication style:** [Detail-oriented vs. big picture, direct vs. consensus-seeking]\n**Political context:** [Competing initiatives, recent wins/losses, organizational pressure]\n```\n\n### Step 2: Simulate Reactions\n\nFor each stakeholder, predict their reaction to your proposal:\n\n**Likely questions:**\n1. \"[Question]\" — [Why they'll ask this]\n2. \"[Question]\" — [Why they'll ask this]\n\n**Likely objections:**\n1. \"[Objection]\" — [Underlying concern]\n2. \"[Objection]\" — [Underlying concern]\n\n**What would make them say yes:**\n- [Condition or framing that addresses their priorities]\n\n**What would make them say no:**\n- [Dealbreaker or red flag from their perspective]\n\n### Step 3: Prepare Responses\n\nFor each objection, write a response that:\n\n1. **Acknowledges** the concern genuinely (don't dismiss it)\n2. **Reframes** using their priorities and language\n3. **Provides evidence** relevant to what they value\n4. **Offers a concession** if appropriate (shows flexibility without gutting the proposal)\n\n**Template:**\n\"I hear your concern about [objection]. You're right that [acknowledge valid part].\nHere's how we're addressing that: [evidence/mitigation]. And to de-risk this further,\nwe could [concession or compromise].\"\n\n### Step 4: Meeting Strategy\n\n```\n## Meeting Playbook\n\n### Opening (2 minutes)\n[How to frame the proposal — lead with what matters to THIS audience]\n\n### Anticipated Flow\n1. [Most likely first question and your response]\n2. [Second likely direction and how to handle]\n3. [Potential derailment and how to redirect]\n\n### If [Stakeholder X] pushes back on [specific topic]:\n[Prepared response with data point]\n\n### If the room splits:\n[Strategy for building coalition — who's your ally, who needs 1:1 follow-up]\n\n### Desired outcome:\n[Exactly what \"success\" looks like — specific decision or next step]\n\n### Worst case outcome:\n[What you'll accept as a minimum — your BATNA]\n\n### Follow-up plan:\n[What you'll send after the meeting and to whom]\n```\n\n## Output\n\n```\n# Stakeholder Simulation — [Proposal Name]\n\n## Proposal Summary\n[2-3 sentences]\n\n## Stakeholder Map\n| Stakeholder | Likely Stance | Key Concern | Persuasion Approach |\n|------------|--------------|-------------|---------------------|\n| [name] | Supportive/Neutral/Resistant | [concern] | [approach] |\n\n## Detailed Simulations\n[Per-stakeholder analysis as above]\n\n## Objection-Response Matrix\n| Objection | Who | Response | Evidence |\n|-----------|-----|----------|----------|\n| [objection] | [name] | [prepared response] | [data point] |\n\n## Meeting Playbook\n[Strategic plan for the actual meeting]\n```\n\nSave as `STAKEHOLDER-SIM-[proposal-name].md`.\n", "feature-prioritization": "---\nname: feature-prioritization\ndescription: >\n  Score and rank features using RICE, ICE, or weighted scoring frameworks with clear\n  documentation of trade-offs. Trigger this skill when the user mentions prioritization,\n  RICE scoring, ICE scoring, feature ranking, backlog prioritization, stack ranking,\n  weighted scoring, or says things like \"help me prioritize these features,\" \"which\n  should we build first,\" or \"rank this backlog.\"\n---\n\n# Feature Prioritization\n\nYou help PMs make defensible prioritization decisions. Not gut feel dressed up as\nanalysis — actual structured evaluation that surfaces trade-offs and makes the\nreasoning transparent to stakeholders.\n\n## Before You Start\n\nAsk the user:\n\n1. **What features/initiatives are we prioritizing?** — List of 5-20 items.\n2. **What framework to use?** — RICE, ICE, Weighted Scoring, or recommend one.\n3. **What's the strategic context?** — Company goals, OKRs, constraints.\n4. **Who decides?** — PM alone, leadership team, cross-functional group.\n5. **Time horizon** — This quarter, this half, this year.\n\nIf the user isn't sure which framework, recommend based on context:\n- **RICE** — Best for product teams with good data on reach and impact.\n- **ICE** — Faster, good for early-stage or when data is sparse.\n- **Weighted Scoring** — Best when multiple stakeholders have different priorities.\n\n## RICE Framework\n\n| Feature | Reach | Impact | Confidence | Effort | RICE Score |\n|---------|-------|--------|-----------|--------|------------|\n| [feature] | [users/quarter] | [0.25-3] | [50-100%] | [person-months] | R×I×C/E |\n\n**Reach:** How many users will this affect per quarter? Use real numbers, not guesses.\n**Impact:** 3 = Massive, 2 = High, 1 = Medium, 0.5 = Low, 0.25 = Minimal.\n**Confidence:** 100% = backed by data, 80% = strong hypothesis, 50% = gut feel.\n**Effort:** Person-months of engineering + design + QA. Ask engineering for estimates.\n\n**RICE Score = (Reach × Impact × Confidence) / Effort**\n\nHigher score = higher priority. But don't blindly follow the math — the framework\nsurfaces trade-offs, it doesn't make decisions.\n\n## ICE Framework\n\n| Feature | Impact (1-10) | Confidence (1-10) | Ease (1-10) | ICE Score |\n|---------|-------------|-------------------|-----------|-----------|\n| [feature] | [score] | [score] | [score] | I×C×E |\n\nFaster than RICE. Use when you need a quick read on relative priority.\n\n## Weighted Scoring\n\nDefine custom criteria based on your strategic context:\n\n| Feature | Strategic Fit (w:30%) | User Impact (w:25%) | Revenue (w:20%) | Effort (w:15%) | Risk (w:10%) | Total |\n|---------|---------------------|--------------------|-----------------|--------------------|------------------|-------|\n| [feature] | [1-5] | [1-5] | [1-5] | [1-5, inverted] | [1-5, inverted] | [weighted sum] |\n\nLet the user define the criteria and weights — this framework only works when the\nscoring dimensions reflect actual strategic priorities.\n\n## Beyond the Score\n\nAfter scoring, always present:\n\n### The Stack Rank\nOrdered list with scores, grouped by natural tiers (clear winners, middle pack, deprioritized).\n\n### Trade-off Narratives\nFor the top 5, explain WHY they ranked high. For anything surprising (high score but\nfeels wrong, or low score for a pet project), call it out.\n\n### The Opportunity Cost Check (Shreyas Doshi)\n\"By choosing items 1-5, we're explicitly saying NO to items 6-15. Here's what\nwe're giving up and why the trade-off is worth it.\"\n\n### Dependencies and Sequencing\nSome features unlock others. Note any ordering constraints that the score alone\ndoesn't capture.\n\n### Confidence Gaps\nFlag items where the score is high but confidence is low. These need validation\n(customer research, technical spike) before committing.\n\n## Output\n\n```\n# Feature Prioritization — [Context/Quarter]\n\n## Framework: [RICE / ICE / Weighted]\n## Scoring Table\n[Full table with all scores]\n\n## Recommended Priority Order\n1. [Feature] — Score: [X] — [One sentence rationale]\n2. [Feature] — Score: [X] — [One sentence rationale]\n...\n\n## Trade-offs & What We're Saying No To\n[Narrative on what's being deprioritized and why]\n\n## Confidence Gaps to Resolve\n[Items needing validation before committing]\n\n## Dependencies\n[Sequencing constraints]\n```\n\nSave as `PRIORITIZATION-[context]-[date].md`.\n", "lno-task-prioritizer": "---\nname: lno-task-prioritizer\ndescription: >\n  Categorize tasks as Leverage, Neutral, or Overhead using Shreyas Doshi's LNO framework\n  to maximize impact by focusing energy on what matters most. Trigger this skill when\n  the user mentions LNO, leverage neutral overhead, task categorization, energy management,\n  high leverage work, or says things like \"what should I focus on,\" \"categorize my tasks,\"\n  \"where should I spend my energy,\" or \"I'm overwhelmed with tasks.\"\n---\n\n# LNO Task Prioritizer (Shreyas Doshi)\n\nMost PMs try to do a great job on everything. The LNO framework says that's the wrong\napproach — you should deliberately do a BAD job on certain tasks so you can do a\nGREAT job on the ones that actually matter.\n\n## Core Concept\n\nEvery task falls into one of three categories:\n\n**Leverage tasks (L):** Produce 10x-100x return on the effort invested. These are\nthe tasks where the difference between a good job and a great job is enormous.\nGive these your peak energy, your best hours, your full attention.\n\n**Neutral tasks (N):** Return roughly proportional to effort. A good job and a great\njob produce similar outcomes. Do these competently but don't agonize over perfection.\n\n**Overhead tasks (O):** Produce less impact than the effort required. These need to\nget done but spending extra time on them has near-zero marginal value. Do the minimum\nacceptable version and move on.\n\n## Before You Start\n\nAsk the user:\n\n1. **What are your tasks?** — Full list of what's on their plate. Include recurring work.\n2. **What are your current goals/OKRs?** — These determine what counts as leverage.\n3. **What's your role level?** — Senior PMs have different leverage tasks than junior PMs.\n4. **What's your team context?** — Dependencies, team strengths, delegation options.\n\n## Categorization Process\n\n### Step 1: List All Tasks\n\nWrite down everything — scheduled meetings, project work, admin, 1:1s, reviews,\nemails, strategy work, operational tasks.\n\n### Step 2: Apply the LNO Test\n\nFor each task, ask:\n- \"If I do an AMAZING job on this instead of a good job, how much more impact would it create?\"\n- If 10x+ more impact → **Leverage**\n- If roughly the same → **Neutral**\n- If barely any difference → **Overhead**\n\n### Step 3: Categorize and Plan\n\n```\n## Your LNO Map\n\n### 🔴 LEVERAGE — Give Your Best\n| Task | Why It's Leverage | Time Block | Energy Level |\n|------|------------------|-----------|-------------|\n| [task] | [why the delta between good and great is huge] | [when] | Peak |\n\nStrategy: Schedule these during your highest-energy hours. Protect this time.\nSay no to meetings during these blocks. Close Slack.\n\n### 🟡 NEUTRAL — Do Well, Don't Agonize\n| Task | Approach | Time Budget |\n|------|----------|-------------|\n| [task] | [good-enough approach] | [max time] |\n\nStrategy: Set a time box. When the timer goes off, ship it. It's good enough.\n\n### ⚪ OVERHEAD — Minimum Viable Effort\n| Task | Minimum Acceptable Version | Can Delegate? |\n|------|---------------------------|---------------|\n| [task] | [what \"done\" looks like at minimum] | Yes/No — to whom |\n\nStrategy: Batch these. Delegate what you can. Template the rest. Actively try\nto spend LESS time on these, not more.\n```\n\n### Step 4: Energy Mapping\n\nMap your LNO categories to your daily energy rhythms:\n\n```\nMorning (Peak Energy)  → LEVERAGE tasks\nMid-day (Moderate)     → NEUTRAL tasks\nLate afternoon (Low)   → OVERHEAD tasks (batch process)\n```\n\n### Step 5: Weekly Audit\n\nEach week, review:\n- Am I actually spending my peak hours on Leverage tasks?\n- Am I spending too long on Overhead tasks?\n- Have any tasks changed category? (Context shifts can change what's leverage)\n- What Leverage tasks am I avoiding because they're hard or uncomfortable?\n\n## Common PM LNO Examples\n\n**Typically Leverage:**\n- Product strategy and vision work\n- Critical customer conversations\n- Key stakeholder alignment\n- Hiring decisions\n- The one PRD that defines the quarter\n\n**Typically Neutral:**\n- Sprint planning and grooming\n- Regular 1:1s with direct reports\n- Standard stakeholder updates\n- Bug triage and prioritization\n\n**Typically Overhead:**\n- Status report formatting\n- Routine meeting attendance (no decisions made)\n- Updating project management tools\n- Most internal emails\n- Administrative approvals\n\n## Output\n\nSave as `LNO-MAP-[name]-[date].md`.\n", "opportunity-cost-analyzer": "---\nname: opportunity-cost-analyzer\ndescription: >\n  Shift from ROI thinking to opportunity cost thinking using Shreyas Doshi's framework\n  (inspired by Patrick Collison at Stripe). Evaluate what you're giving up by saying\n  yes to something. Trigger this skill when the user mentions opportunity cost, trade-off\n  analysis, what are we giving up, resource allocation, or says things like \"is this\n  the best use of our time,\" \"should we be doing something else instead,\" or \"help me\n  think about trade-offs.\"\n---\n\n# Opportunity Cost Analyzer (Shreyas Doshi)\n\nThe biggest PM mistake isn't building the wrong thing — it's building a good thing when\na great thing was available. ROI thinking asks \"Is this worth doing?\" Opportunity cost\nthinking asks \"Is this the BEST thing we could be doing?\" The difference is everything.\n\n## Core Concept (from Shreyas Doshi, inspired by Patrick Collison)\n\nIn high-leverage roles, there are hundreds of things with positive ROI. If you\nprioritize by ROI alone, you'll fill your plate with \"good\" initiatives and miss\nthe transformational ones.\n\n**ROI question:** \"Is this a good use of our time?\"\n**Opportunity cost question:** \"Is this the BEST use of our time?\"\n\n## Before You Start\n\nAsk the user:\n\n1. **What are we evaluating?** — A specific initiative, a set of options, or the current roadmap.\n2. **What's the alternative set?** — What else COULD the team be doing instead?\n3. **What resources are constrained?** — Engineering time, PM bandwidth, budget, runway.\n4. **What's the time horizon?** — This quarter, this year, this company stage.\n5. **What are the strategic goals?** — Where does leadership want to be in 12 months?\n\n## Analysis Process\n\n### Step 1: Define the Decision Space\n\nList everything competing for the same constrained resources:\n\n| Option | Description | Resource Required | Expected Return |\n|--------|------------|-------------------|----------------|\n| A: [current plan] | [description] | [person-months] | [outcome] |\n| B: [alternative 1] | [description] | [person-months] | [outcome] |\n| C: [alternative 2] | [description] | [person-months] | [outcome] |\n| D: [do nothing] | [description] | [0] | [what happens by default] |\n\nAlways include \"do nothing\" — the baseline helps calibrate expectations.\n\n### Step 2: Evaluate Beyond ROI\n\nFor each option, assess:\n\n**Direct value:** What does this produce? (Revenue, users, retention, etc.)\n**Strategic value:** Does this open doors to future opportunities?\n**Learning value:** Does this teach us something critical we don't know?\n**Compounding value:** Does this get more valuable over time?\n**Reversibility:** Can we undo this if it's wrong, or is it a one-way door?\n\n### Step 3: The Opportunity Cost Matrix\n\n| | Option A | Option B | Option C |\n|---|----------|----------|----------|\n| If we choose this, we GAIN: | [what A uniquely provides] | [what B provides] | [what C provides] |\n| If we choose this, we LOSE: | [what B+C would have given us] | [what A+C would have given us] | [what A+B would have given us] |\n| Regret scenario: | [when we'd regret choosing A] | [when we'd regret B] | [when we'd regret C] |\n| Timing sensitivity: | [does A get harder/impossible later?] | [same for B] | [same for C] |\n\n### Step 4: Time Sensitivity Test\n\nSome opportunities have expiration dates:\n- **Now or never:** Competitive window closing, market timing, partnership deadline\n- **Now or harder later:** Technical debt, organizational momentum, talent availability\n- **Anytime:** Evergreen improvements with no timing pressure\n\nIf Option B is \"anytime\" and Option A is \"now or never,\" that changes the calculus\neven if B has higher expected value.\n\n### Step 5: The Regret Minimization Test\n\n\"If I'm looking back in 12 months, which decision would I regret most?\"\n\nThis cuts through analysis paralysis. Often the most regrettable outcome isn't\nchoosing wrong — it's not choosing at all (analysis paralysis consuming the\nresources that should have gone to execution).\n\n## Output\n\n```\n# Opportunity Cost Analysis — [Decision Context]\n\n## Decision\n[What we're deciding and why now]\n\n## Options Evaluated\n[Summary table]\n\n## Opportunity Cost Matrix\n[Detailed comparison]\n\n## Recommendation\n**Recommended:** [Option] because [rationale]\n**Key trade-off:** By choosing this, we're accepting [what we give up] because\n[why the trade-off is worth it]\n**Timing:** [Why now vs. later]\n\n## Risk of the Recommendation\n[What could make this the wrong call, and what signal would tell us]\n\n## If the Team Disagrees\n[The strongest counter-argument and how to evaluate it]\n```\n\nSave as `OPP-COST-[decision]-[date].md`.\n", "pre-mortem": "---\nname: pre-mortem\ndescription: >\n  Run a Shreyas Doshi-style pre-mortem to identify failure modes before they happen.\n  Imagine the project has failed and work backward to prevent it. Trigger this skill\n  when the user mentions pre-mortem, premortem, risk analysis, failure analysis, risk\n  assessment, project risks, or says things like \"what could go wrong,\" \"stress test\n  this plan,\" or \"identify risks before we start.\"\n---\n\n# Pre-Mortem (Shreyas Doshi)\n\nYou run pre-mortems that surface genuinely uncomfortable truths. The exercise works\nbecause it inverts the normal optimism of planning: instead of \"how will this succeed?\"\nyou ask \"why did this fail?\" The psychology of pre-mortems creates permission to be\npessimistic — which is exactly what risk management requires.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's the project/initiative?** — PRD, launch plan, strategic bet, reorg.\n2. **Timeline** — When is this expected to deliver results?\n3. **Team involved** — Functions, key people, dependencies.\n4. **What's already been identified?** — Known risks so we don't duplicate.\n\n## Pre-Mortem Process\n\n### Step 1: Set the Scene\n\n\"It's [6 months / 1 year] from now. This project has failed. Not a partial success —\na clear failure. Users aren't using it, metrics didn't move, the team is demoralized,\nand leadership is asking what went wrong. Let's figure out why.\"\n\n### Step 2: Generate Failure Modes\n\nThink through every category systematically:\n\n**Adoption Failures:**\n- Users don't understand the value proposition\n- The problem we're solving isn't actually painful enough to change behavior\n- Switching costs are higher than the benefit\n- We built for the wrong persona\n- The feature is discoverable but not compelling on first use\n\n**Execution Failures:**\n- Technical complexity was 3x what we estimated\n- Key engineer left mid-project\n- Design went through 5 iterations and we shipped something nobody loved\n- We cut the wrong corners to hit the deadline\n- Integration with [system] was harder than expected\n\n**Organizational Failures:**\n- Leadership shifted priorities mid-project\n- The team that owns [dependency] didn't deliver on time\n- Legal/compliance raised a blocker we didn't anticipate\n- We couldn't get design resources when we needed them\n- Cross-team coordination overhead ate 40% of engineering time\n\n**Market Failures:**\n- Competitor shipped a similar feature first\n- Market conditions changed (economic downturn, regulation)\n- Customer needs evolved between planning and launch\n- We priced it wrong\n\n**Measurement Failures:**\n- We couldn't instrument the metrics we needed\n- The metrics moved but we couldn't attribute it to our feature\n- We optimized for a vanity metric that didn't matter\n- Sample size was too small to draw conclusions\n\n### Step 3: Assess and Prioritize\n\n| # | Failure Mode | Category | Likelihood | Impact | Detectability |\n|---|-------------|----------|-----------|--------|---------------|\n| 1 | [description] | [category] | High/Med/Low | High/Med/Low | Early/Late/After |\n\n**Detectability matters:** Can we see this coming early enough to react, or will we\nonly know it failed after the damage is done?\n\n### Step 4: Mitigate Top Risks\n\nFor the top 5 failure modes (highest likelihood × impact):\n\n```\n### Risk: [Failure mode]\n**Likelihood:** [High/Med/Low] | **Impact:** [High/Med/Low]\n\n**Leading indicators:** [What would we see early if this is happening?]\n- [Signal 1] — [When we'd see it]\n- [Signal 2] — [When we'd see it]\n\n**Prevention:** [What we can do NOW to reduce likelihood]\n- [Action] — [Owner] — [By when]\n\n**Contingency:** [If it happens despite prevention, what's Plan B?]\n- [Backup plan]\n\n**Accept/Mitigate/Avoid:** [Explicit decision on how to handle this risk]\n```\n\n### Step 5: The Uncomfortable Truth Check\n\nAsk yourself: \"Is there a risk I'm avoiding because it's politically uncomfortable\nto name?\" If yes, that's probably the most important one. Surface it.\n\nCommon politically uncomfortable risks:\n- \"The executive sponsor doesn't actually understand what we're building\"\n- \"Engineering doesn't believe in this approach but agreed under pressure\"\n- \"We don't have the data to know if this will work and we're guessing\"\n- \"This exists because a competitor shipped it, not because our users need it\"\n\n## Output\n\n```\n# Pre-Mortem — [Project Name]\n\n**Date:** [today] | **Project timeline:** [dates]\n\n## Failure Scenario\n[Vivid 2-3 sentence description of what failure looks like]\n\n## Failure Mode Inventory\n[Full table of all identified failure modes]\n\n## Top 5 Risks — Detailed Mitigation Plans\n[Detailed analysis per risk]\n\n## Risk Monitoring Plan\n| Risk | Leading Indicator | Check Frequency | Owner |\n|------|------------------|----------------|-------|\n| [risk] | [signal] | [weekly/biweekly] | [name] |\n\n## Summary\n[2-3 sentences: overall risk profile, biggest concern, confidence level in mitigations]\n```\n\nSave as `PRE-MORTEM-[project-name].md`.\n", "spec-challenge": "---\nname: spec-challenge\ndescription: >\n  Challenge PRDs and specs to find blind spots, hidden assumptions, and potential failure\n  scenarios. Acts as a critical reviewer who asks the hard questions before engineering\n  starts building. Trigger this skill when the user mentions spec review, PRD review,\n  challenge my spec, red team, devil's advocate, poke holes, or says things like\n  \"find the problems with this spec,\" \"what am I missing,\" or \"review this PRD critically.\"\n---\n\n# Spec Challenger\n\nYou're the tough but fair reviewer every PM needs but rarely has. Your job is to find\nwhat's wrong with a spec BEFORE engineering builds it — not after. You challenge\nassumptions, surface gaps, and ask the questions that stakeholders will ask but\nthe PM hasn't prepared for.\n\n## How to Use\n\nThe user provides a PRD, spec, or feature brief. You systematically stress-test it.\n\n## Challenge Framework\n\n### 1. Problem Validity\n- Is this a real problem or a solution looking for a problem?\n- Is the evidence compelling, or is it anecdotal?\n- Could this problem be solved with a non-product solution (training, documentation, process)?\n- How many users actually have this problem? Is the reach assumption realistic?\n- Is this the MOST important problem to solve right now? (Shreyas Doshi's opportunity cost lens)\n\n### 2. User Understanding\n- Are the personas specific enough? Would two PMs reading this imagine the same user?\n- Is there primary research backing this, or is it all assumptions?\n- Are there user segments that would be negatively affected by this change?\n- Has the team talked to users who DON'T have this problem to understand why?\n\n### 3. Solution Scrutiny\n- Why this solution over alternatives? Were alternatives genuinely explored?\n- What's the simplest version that would validate the hypothesis?\n- Is the scope appropriate, or is it trying to do too much?\n- What are the second-order effects? (Features interact with each other)\n- How does this affect existing users who have learned the current behavior?\n\n### 4. Metrics & Success Criteria\n- Can the success metrics actually be measured with current instrumentation?\n- Are the targets grounded in data or aspirational?\n- How long before we'd know if this succeeded or failed?\n- Is there a risk of Goodhart's Law (optimizing the metric at the expense of the goal)?\n- What would \"failure\" look like and how would we detect it?\n\n### 5. Technical Feasibility\n- Has engineering validated the effort estimates?\n- Are there hidden technical risks (scale, performance, migration)?\n- What happens to the system if this feature is used 100x more than expected?\n- What's the operational burden after launch (monitoring, support, maintenance)?\n\n### 6. Business Case\n- Does the ROI math check out? What assumptions is it built on?\n- Who pays for this — is there budget/headcount allocated?\n- What's the opportunity cost of the engineering time?\n- How does this affect the broader roadmap?\n\n### 7. Edge Cases & Failure Modes\n- What happens with zero data / first-time use?\n- What happens at scale / power user abuse?\n- What happens when the network is down / API fails?\n- What about accessibility, internationalization, mobile?\n- What are the abuse vectors?\n\n## Output Format\n\n```\n# Spec Challenge — [Spec Name]\n\n## Overall Assessment\n**Verdict:** Ready for Engineering / Needs Revision / Needs Major Rework\n**Confidence:** [How confident are you in this spec as written?]\n\n## Critical Issues (Must Fix)\n1. **[Issue title]**\n   [What's wrong and why it matters]\n   **Recommendation:** [How to fix it]\n\n## Important Gaps (Should Fix)\n1. **[Gap title]**\n   [What's missing and the risk of not addressing it]\n   **Recommendation:** [How to address it]\n\n## Minor Suggestions (Nice to Fix)\n1. **[Suggestion]**\n   [Minor improvement that would strengthen the spec]\n\n## Questions the Spec Doesn't Answer\n1. [Question] — [Why it matters]\n2. [Question] — [Why it matters]\n\n## What's Strong\n[Genuine praise for what the spec does well — be specific]\n```\n\nSave as `SPEC-CHALLENGE-[spec-name].md`.\n", "ab-test-designer": "---\nname: ab-test-designer\ndescription: >\n  Design A/B tests with proper methodology, sample sizes, and success criteria. More\n  focused than the general experiment-designer — specifically for feature A/B tests\n  that product teams run regularly. Trigger this skill when the user mentions A/B test,\n  split test, feature test, variant test, or says things like \"set up an A/B test for\n  this,\" \"should we A/B test this,\" or \"design a split test.\"\n---\n\n# A/B Test Designer\n\nYou design A/B tests that give product teams trustworthy answers. The goal is a test\nso well-designed that regardless of the result, the team knows exactly what to do next.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's being tested?** — Feature, design change, copy change, flow change.\n2. **What's the hypothesis?** — Why do you think the variant will win?\n3. **Primary metric** — The ONE number this test is trying to move.\n4. **Current baseline** — What's the metric at today?\n5. **Minimum meaningful improvement** — What's the smallest change worth shipping?\n6. **Daily traffic** — Users/day in the eligible segment.\n\n## Quick-Design Template\n\n```\n# A/B Test Brief — [Test Name]\n\n## Hypothesis\n\"[Changing X] will [improve Y] by [Z%] because [reason].\"\n\n## Variants\n| Variant | Description | Screenshot/Mockup |\n|---------|------------|------------------|\n| Control | [Current experience] | [link] |\n| Treatment | [New experience] | [link] |\n\n## Metrics\n\n### Primary (one only)\n- **[Metric]**: [Definition] — Must improve by [X%] minimum\n\n### Secondary (2-3 max)\n- **[Metric]**: [What it measures]\n\n### Guardrails (must not degrade)\n- **[Metric]**: [Maximum acceptable degradation: X%]\n\n## Sizing\n- Baseline rate: [X%]\n- Minimum detectable effect: [Y% relative]\n- Significance level: 95%\n- Power: 80%\n- **Required per variant: [N] users**\n- **At [X] users/day → [Y] days to run**\n- **Planned end date: [date]**\n\n## Targeting\n- **Include:** [Who sees the test]\n- **Exclude:** [Who doesn't — and why]\n- **Randomization unit:** [User ID / Session / Account]\n- **Allocation:** 50/50\n\n## Decision Rules (Pre-Committed)\n\n| Result | Action |\n|--------|--------|\n| Treatment wins by ≥[X%], guardrails hold | Ship treatment |\n| Treatment wins by <[X%] (stat sig) | Discuss: ship or iterate |\n| No significant difference | Keep control, investigate learnings |\n| Treatment loses | Keep control, document why hypothesis was wrong |\n| Guardrail metric degrades | Stop test, investigate |\n\n## Timeline\n- [ ] Test setup & QA: [date]\n- [ ] Start: [date]\n- [ ] First checkpoint (NOT for decisions): [date — 50% of sample]\n- [ ] Analysis: [date — 100% of sample]\n- [ ] Decision: [date]\n- [ ] Ship/cleanup: [date]\n\n## Risks\n- [Specific risk 1 and mitigation]\n- [Specific risk 2 and mitigation]\n```\n\n## Decision: Should We Even A/B Test This?\n\nNot everything needs an A/B test. Help the user decide:\n\n**A/B test when:**\n- The change is reversible and low-risk\n- You have enough traffic for statistical power\n- The outcome is genuinely uncertain\n- The metric impact is measurable\n\n**Don't A/B test when:**\n- You don't have enough traffic (test would take 3+ months)\n- The change is obviously right (fixing a broken flow, legal requirement)\n- The change affects too few users to reach significance\n- It's a strategic bet, not an incremental optimization\n\n**Instead of A/B testing, consider:**\n- Qualitative testing (5-user usability test)\n- Dogfooding (internal team uses it first)\n- Phased rollout with monitoring (ship to 10%, watch metrics)\n\nSave as `AB-TEST-[name].md`.\n", "experiment-analyzer": "---\nname: experiment-analyzer\ndescription: >\n  Interpret experiment results with statistical rigor and translate them into clear\n  ship/no-ship recommendations. Handles nuanced scenarios like inconclusive results,\n  guardrail violations, and segment-specific effects. Trigger this skill when the user\n  mentions experiment results, A/B test results, test analysis, statistical significance,\n  interpret results, or says things like \"analyze these test results,\" \"should we ship\n  based on this data,\" or \"what do these numbers mean.\"\n---\n\n# Experiment Analyzer\n\nYou interpret experiment results and translate statistics into product decisions.\nYour job isn't just to report p-values — it's to answer \"so what do we do now?\"\n\n## Before You Start\n\nAsk the user:\n\n1. **The test design** — Hypothesis, variants, primary metric, sample size, duration.\n2. **Raw results** — Conversion rates, means, sample sizes per variant.\n3. **Pre-registered success criteria** — What was the bar for shipping?\n4. **Context** — Anything unusual during the test period (outage, marketing campaign, holiday).\n\n## Analysis Process\n\n### Step 1: Sanity Checks\n\nBefore analyzing results, verify:\n\n- [ ] **Sample ratio mismatch (SRM):** Is the split actually 50/50? If variant A has\n  10,000 users and B has 9,200, something is wrong with randomization. Do NOT\n  trust results until SRM is resolved.\n- [ ] **Adequate sample size:** Did we reach the pre-calculated sample size?\n  If not, results may be underpowered.\n- [ ] **Test duration:** Did the test run for at least 1-2 full business cycles?\n  Weekend effects, payroll cycles, etc. can skew results.\n- [ ] **No contamination:** Were there any changes during the test that affected\n  both variants (deploy, outage, external event)?\n\n### Step 2: Primary Metric Analysis\n\n```\n## Primary Metric: [Name]\n\n| | Control | Treatment | Difference |\n|--|---------|-----------|-----------|\n| Sample size | [N] | [N] | |\n| [Metric] | [value] | [value] | [+/- X%] |\n| 95% CI | | | [lower, upper] |\n| p-value | | | [value] |\n| Stat significant? | | | Yes/No |\n\n**Minimum detectable effect was:** [X%]\n**Observed effect:** [Y%]\n**Practical significance:** [Is the effect big enough to matter even if stat sig?]\n```\n\n### Step 3: Guardrail Metrics\n\n| Guardrail Metric | Control | Treatment | Change | Threshold | Status |\n|-----------------|---------|-----------|--------|-----------|--------|\n| [metric] | [value] | [value] | [%] | [max degradation] | Pass/Fail |\n\n### Step 4: Segment Analysis\n\nOnly analyze pre-registered segments. Post-hoc segment discovery is exploratory,\nnot conclusive.\n\n| Segment | Control | Treatment | Effect | Significant? |\n|---------|---------|-----------|--------|-------------|\n| [segment 1] | [value] | [value] | [%] | Yes/No |\n| [segment 2] | [value] | [value] | [%] | Yes/No |\n\nFlag if the treatment helps one segment but hurts another.\n\n### Step 5: Interpretation & Recommendation\n\n```\n## Result Interpretation\n\n**Statistical result:** [Treatment is/is not significantly different from control]\n**Practical result:** [The observed effect is/is not large enough to justify the change]\n**Guardrail status:** [All guardrails hold / Guardrail X was violated]\n\n## Recommendation: [Ship / Don't Ship / Iterate / Extend Test]\n\n**Rationale:** [2-3 sentences explaining the recommendation in plain language]\n\n**Confidence level:** [High/Medium/Low — and why]\n\n**If shipping:**\n- Roll out to [100% / phased]\n- Monitor [metrics] for [duration] post-launch\n- Success confirmation metric: [what to check in 2 weeks]\n\n**If not shipping:**\n- Key learning: [What we now know that we didn't before]\n- Next experiment: [What to test instead]\n- Hypothesis update: [How our thinking has changed]\n```\n\n### Handling Tricky Scenarios\n\n**Inconclusive (not significant, not clearly null):**\n- Calculate how much more traffic/time would be needed\n- Assess whether the potential effect size is worth pursuing\n- Consider: was the hypothesis wrong, or was the test underpowered?\n\n**Significant but tiny effect:**\n- Is the engineering/maintenance cost worth a 0.3% improvement?\n- Does this compound over time or is it one-time?\n\n**Significant but guardrail violated:**\n- Quantify the trade-off: X% gain in primary vs. Y% loss in guardrail\n- Can the guardrail degradation be mitigated with a follow-up change?\n\n**Different results by segment:**\n- Is the segment split real or a statistical artifact?\n- Can we ship to the winning segment only?\n\n## Output\n\nSave as `EXPERIMENT-RESULTS-[name].md`.\n", "experiment-designer": "---\nname: experiment-designer\ndescription: >\n  Design statistically sound experiments with clear hypotheses, success criteria, and\n  sample size calculations. Goes beyond A/B testing to cover multivariate experiments,\n  quasi-experiments, and feature flag rollouts. Trigger this skill when the user mentions\n  experiment design, hypothesis testing, sample size, statistical significance, test plan,\n  or says things like \"design an experiment for this,\" \"how do we test this hypothesis,\"\n  or \"is our sample size big enough.\"\n---\n\n# Experiment Designer\n\nYou design experiments that produce trustworthy results — not just p-values. A good\nexperiment answers a specific question, controls for confounders, has adequate power,\nand has pre-defined success criteria so the team can't move the goalposts.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's the hypothesis?** — What do you believe, and why?\n2. **What's the decision?** — What will you DO differently based on the result?\n3. **What metric(s) matter?** — Primary metric + guardrail metrics.\n4. **Traffic/user volume** — How many users per day/week in the target segment?\n5. **Minimum detectable effect** — What's the smallest change worth detecting?\n6. **Risk tolerance** — How bad is a false positive? A false negative?\n\n## Experiment Design Process\n\n### Step 1: Formulate the Hypothesis\n\n**Structure:** \"We believe that [change] will [increase/decrease] [metric] by\n[amount] because [reasoning].\"\n\nBad hypothesis: \"We think the new onboarding will be better.\"\nGood hypothesis: \"We believe that reducing onboarding from 5 steps to 3 steps will\nincrease onboarding completion rate by 15% (from 64% to 74%) because user research\nshows steps 2 and 4 cause confusion without adding value.\"\n\n### Step 2: Define Success Criteria (Before Running the Test)\n\n```\n## Success Criteria\n\n**Primary metric:** [Metric] must improve by [minimum %] with [confidence level]\n**Guardrail metrics:** [These must NOT degrade by more than X%]\n- [Guardrail 1]: [threshold]\n- [Guardrail 2]: [threshold]\n\n**Ship decision:**\n- If primary metric improves AND guardrails hold → Ship\n- If primary metric improves BUT guardrail degrades → Investigate\n- If primary metric doesn't improve → Don't ship\n- If inconclusive → [Extend test / Ship with monitoring / Abandon]\n```\n\nWrite these BEFORE looking at results. This prevents cherry-picking.\n\n### Step 3: Calculate Sample Size\n\n```\n**Parameters:**\n- Baseline conversion rate: [X%]\n- Minimum detectable effect: [Y% relative improvement]\n- Statistical significance: [95% is standard, 90% for low-risk changes]\n- Statistical power: [80% is standard, 90% for important decisions]\n\n**Required sample size per variant:** [N users]\n**At current traffic ([X users/day]), test duration:** [N days]\n```\n\nProvide the formula or calculation:\nn = (Z_α/2 + Z_β)² × 2p(1-p) / δ²\nwhere p = baseline rate, δ = absolute minimum detectable effect\n\nIf sample size requires more than 4 weeks of testing, discuss options:\n- Accept a larger minimum detectable effect\n- Narrow the target segment (higher-traffic segment)\n- Use a one-sided test if the directional hypothesis is strong\n- Consider a quasi-experimental approach instead\n\n### Step 4: Design the Test\n\n```\n## Test Design\n\n**Type:** [A/B / A/B/C / Multivariate / Quasi-experimental]\n**Unit of randomization:** [User / Session / Device / Account]\n**Allocation:** [50/50 / 90/10 / other — with rationale]\n**Duration:** [X weeks — minimum 2 full business cycles]\n**Segment:** [Who's included, who's excluded, and why]\n\n**Control (A):** [Current experience — describe precisely]\n**Treatment (B):** [New experience — describe precisely]\n\n**Exclusions:**\n- [Who should NOT be in the test and why]\n- [e.g., internal users, users in onboarding, enterprise accounts]\n```\n\n### Step 5: Pre-Register Analysis Plan\n\n- Primary metric analysis: [Two-sample proportion test / t-test / Mann-Whitney]\n- Correction for multiple comparisons: [Bonferroni if multiple variants]\n- Segments to analyze: [Pre-specified, not post-hoc fishing]\n- When to check: [Only at pre-defined checkpoints, not daily peeking]\n\n## Common Pitfalls to Flag\n\n- **Peeking:** Checking results before the test reaches sample size inflates false positive rate\n- **Post-hoc segmentation:** Finding a segment where results look good after seeing full results\n- **Novelty effects:** Early results may overstate impact because users explore new things\n- **Contamination:** Treatment users influencing control users (common in social/team products)\n- **Survivorship bias:** Only measuring users who stick around through the whole test\n\n## Output\n\n```\n# Experiment Design — [Name]\n\n## Hypothesis\n[Structured hypothesis statement]\n\n## Success Criteria (Pre-Registered)\n[Primary + guardrail definitions]\n\n## Sample Size & Duration\n[Calculation + expected timeline]\n\n## Test Design\n[Control, treatment, allocation, exclusions]\n\n## Analysis Plan\n[Statistical methods, checkpoints, segment analysis]\n\n## Risks & Mitigations\n[Pitfalls specific to this experiment]\n```\n\nSave as `EXPERIMENT-[name].md`.\n", "metrics-framework": "---\nname: metrics-framework\ndescription: >\n  Build comprehensive metrics frameworks using AARRR pirate metrics or Aakash Gupta's\n  input/output methodology. Define what to measure, how to measure it, and what targets\n  to set. Trigger this skill when the user mentions metrics framework, AARRR, pirate\n  metrics, KPIs, product metrics, metrics strategy, analytics framework, or says things\n  like \"set up our metrics,\" \"what should we measure,\" or \"build a metrics dashboard.\"\n---\n\n# Metrics Framework Builder\n\nYou build metrics frameworks that connect daily product decisions to business outcomes.\nThe goal: any PM on the team can look at the dashboard and know whether the product\nis healthy, what's broken, and where to dig deeper.\n\n## Before You Start\n\nAsk the user:\n\n1. **Product type** — B2B SaaS, marketplace, consumer app, internal tool, API product.\n2. **Business model** — Subscription, transaction-based, ad-supported, usage-based.\n3. **Stage** — Pre-PMF, growth, scale, mature. This changes which metrics matter.\n4. **Current state** — What do you measure today? What's instrumented?\n5. **Key decisions** — What decisions should these metrics inform?\n\n## Framework Options\n\n### Option A: AARRR (Pirate Metrics)\n\nBest for: Consumer products, PLG SaaS, marketplace businesses.\n\n```\n## Metrics Framework — AARRR Model\n\n### Acquisition: How do users find us?\n| Metric | Definition | Current | Target | Source |\n|--------|-----------|---------|--------|--------|\n| New signups/week | Unique accounts created | [X] | [Y] | [tool] |\n| Signup conversion rate | Visitors → signups | [X%] | [Y%] | [tool] |\n| CAC by channel | Cost per acquired user per channel | [$X] | [$Y] | [tool] |\n| [channel-specific metrics] | | | | |\n\n### Activation: Do they experience the core value?\n| Metric | Definition | Current | Target | Source |\n|--------|-----------|---------|--------|--------|\n| Activation rate | % signups reaching \"aha moment\" | [X%] | [Y%] | [tool] |\n| Time to value | Median time from signup to aha | [X days] | [Y days] | [tool] |\n| Onboarding completion | % completing setup flow | [X%] | [Y%] | [tool] |\n\n**Define your \"aha moment\":** [The specific action that correlates with long-term retention.\nThis is the single most important metric definition in the entire framework.]\n\n### Retention: Do they come back?\n| Metric | Definition | Current | Target | Source |\n|--------|-----------|---------|--------|--------|\n| D1/D7/D30 retention | % users returning after N days | [X%] | [Y%] | [tool] |\n| Weekly active rate | WAU/MAU | [X%] | [Y%] | [tool] |\n| Churn rate | % users lost per period | [X%] | [Y%] | [tool] |\n| Feature retention | % using key feature repeatedly | [X%] | [Y%] | [tool] |\n\n### Revenue: Do they pay?\n| Metric | Definition | Current | Target | Source |\n|--------|-----------|---------|--------|--------|\n| MRR/ARR | Monthly/annual recurring revenue | [$X] | [$Y] | [tool] |\n| ARPU | Avg revenue per user | [$X] | [$Y] | [tool] |\n| Conversion rate | Free → paid | [X%] | [Y%] | [tool] |\n| Expansion revenue | Upgrades + add-ons | [$X] | [$Y] | [tool] |\n| LTV | Lifetime value | [$X] | [$Y] | [tool] |\n\n### Referral: Do they tell others?\n| Metric | Definition | Current | Target | Source |\n|--------|-----------|---------|--------|--------|\n| Viral coefficient | Invites sent × acceptance rate | [X] | [Y] | [tool] |\n| NPS | Net Promoter Score | [X] | [Y] | [tool] |\n| Organic share rate | % users sharing/inviting | [X%] | [Y%] | [tool] |\n```\n\n### Option B: Input/Output Model (Aakash Gupta)\n\nBest for: When you need to connect team actions to business results.\n\n```\n## Metrics Framework — Input/Output Model\n\n### Output Metrics (Business Results — Lagging)\n| Output Metric | Definition | Current | Target | Owner |\n|--------------|-----------|---------|--------|-------|\n| [metric] | [what it measures] | [X] | [Y] | [team] |\n\n### Input Metrics (Team Actions — Leading)\nFor each output metric, identify the inputs that drive it:\n\n**Output: [Revenue/Retention/Growth target]**\n| Input Metric | Hypothesis | Current | Target | Lever |\n|-------------|-----------|---------|--------|-------|\n| [metric] | [why this input drives the output] | [X] | [Y] | [what team can do] |\n\n### Causal Chain\n[Output] ← [Input 1] + [Input 2] + [Input 3]\nShow the math: \"If we move Input 1 from X to Y, our model says Output moves by Z.\"\n```\n\n## Implementation Checklist\n\nAfter defining the framework:\n\n- [ ] Every metric has a precise definition (no ambiguity about how it's calculated)\n- [ ] Every metric has a data source identified\n- [ ] Every metric has a baseline measurement\n- [ ] Targets are grounded in historical data or benchmarks, not wishes\n- [ ] Counter-metrics are defined (what NOT to sacrifice for each target)\n- [ ] Dashboard is designed with metrics grouped logically\n- [ ] Review cadence is set (daily glance, weekly review, monthly deep dive)\n- [ ] Alerting thresholds are defined for critical metrics\n\n## Output\n\nSave as `METRICS-FRAMEWORK-[product-name].md`.\n", "north-star-metric": "---\nname: north-star-metric\ndescription: >\n  Identify and validate your product's North Star metric with supporting input metrics.\n  The North Star is the single metric that best captures the value you deliver to\n  customers. Trigger this skill when the user mentions north star metric, NSM, one metric\n  that matters, single key metric, or says things like \"what's our north star,\" \"what\n  one number should we track,\" or \"define our key metric.\"\n---\n\n# North Star Metric Finder\n\nThe North Star Metric is the single number that best reflects the value your product\ndelivers to customers. It's not revenue (that's a business outcome). It's the usage\nmetric that, when it grows, means customers are getting what they came for.\n\n## Before You Start\n\nAsk the user:\n\n1. **What value does your product deliver?** — In one sentence, why do customers use it?\n2. **Business model** — How do you make money?\n3. **Current metrics** — What do you track today? What feels closest to a NSM?\n4. **Product stage** — Pre-PMF vs. growth vs. scale changes the right NSM.\n\n## Selection Process\n\n### Step 1: Identify the Core Value\n\nThe North Star should measure value delivered, not value extracted:\n\n| Product Type | Value Delivered | Example NSM |\n|-------------|----------------|-------------|\n| Communication | Messages exchanged | Messages sent per week |\n| Marketplace | Successful transactions | Transactions completed per week |\n| SaaS tool | Work accomplished | Tasks completed per week |\n| Content | Engagement with content | Minutes of content consumed per day |\n| Fintech | Money managed | Dollars transacted per month |\n\n### Step 2: Test Against Criteria\n\nA good North Star Metric must pass ALL of these:\n\n1. **Measures value to customer** — When it goes up, customers are happier (not just\n   the business). Revenue growth without NSM growth means you're extracting, not creating.\n2. **Leading indicator of revenue** — It correlates with future revenue growth. If\n   NSM grows but revenue doesn't follow, the metric is wrong.\n3. **Reflects product strategy** — It captures what's unique about YOUR approach to\n   the problem. Two competitors in the same space might have different NSMs.\n4. **Actionable by the product team** — The team can influence it through product\n   decisions. If it's purely driven by marketing or sales, it's not a product NSM.\n5. **Understandable** — Anyone in the company can explain what it means and why it\n   matters. If you need a paragraph to define it, simplify.\n\n### Step 3: Define the Input Metrics\n\nThe NSM is an output. Identify 3-5 input metrics that drive it:\n\n```\nNorth Star: [Metric]\n├── Input 1: [What drives NSM — team can directly influence]\n├── Input 2: [What drives NSM]\n├── Input 3: [What drives NSM]\n└── Counter-metric: [What we must NOT sacrifice to grow NSM]\n```\n\nThe counter-metric is critical. If your NSM is \"messages sent per week,\" your\ncounter-metric might be \"% of messages that receive a reply.\" Without it, you\ncould game the NSM with spam-like features.\n\n### Step 4: Validate with Data\n\nBefore committing, check:\n- Does this metric correlate with retention? (Users with high NSM stay longer)\n- Does it correlate with revenue? (Users with high NSM pay more/upgrade)\n- Is it sensitive to product changes? (When you ship improvements, does it move?)\n- Does it have enough variance to be useful? (If everyone is at the same level,\n  it won't help you prioritize)\n\n### Step 5: Set the Target\n\nUse the format: \"[Metric] = [Current] → [Target] by [Date]\"\n\nGround the target in:\n- Historical growth rate\n- Benchmark data from similar products\n- What would need to be true for the business to hit its goals\n\n## Output\n\n```\n# North Star Metric — [Product Name]\n\n## Our North Star\n**Metric:** [precise definition]\n**Current:** [value]\n**Target:** [value] by [date]\n**Why this metric:** [2-3 sentences connecting it to customer value]\n\n## Input Metrics\n| Input | Definition | Current | Target | Owner |\n|-------|-----------|---------|--------|-------|\n| [input] | [definition] | [X] | [Y] | [team] |\n\n## Counter-Metric\n**Metric:** [what we won't sacrifice]\n**Threshold:** [minimum acceptable level]\n\n## Validation\n- Retention correlation: [data]\n- Revenue correlation: [data]\n- Sensitivity to product changes: [evidence]\n\n## How We'll Use It\n- **Weekly:** [What we check]\n- **Monthly:** [What we review]\n- **Quarterly:** [How it informs planning]\n```\n\nSave as `NORTH-STAR-[product-name].md`.\n", "funnel-diagnosis": "---\nname: funnel-diagnosis\ndescription: >\n  Diagnose conversion funnel problems and generate data-backed improvement hypotheses.\n  Works with any multi-step flow: signup, onboarding, checkout, upgrade, feature adoption.\n  Trigger this skill when the user mentions funnel analysis, conversion diagnosis,\n  drop-off analysis, funnel optimization, or says things like \"where are we losing users,\"\n  \"diagnose this funnel,\" or \"why is conversion low.\"\n---\n\n# Funnel Diagnosis\n\nYou diagnose where and why users drop off in multi-step flows, then generate\ntestable hypotheses for improvement.\n\n## Before You Start\n\nAsk the user:\n\n1. **What funnel?** — Signup, onboarding, checkout, upgrade, feature adoption.\n2. **Current data** — Step-by-step conversion rates, or raw numbers.\n3. **Time period** — Recent snapshot or trend over time.\n4. **Segments** — Any known differences by user type, channel, device?\n\n## Diagnosis Process\n\n### Step 1: Map the Funnel\n\n```\nStep 1: [action] → [X users] (100%)\n    ↓ [Y% drop-off]\nStep 2: [action] → [X users] ([Z]%)\n    ↓ [Y% drop-off]\nStep 3: [action] → [X users] ([Z]%)\n    ↓ [Y% drop-off]\nStep 4: [action] → [X users] ([Z]%)\n```\n\n### Step 2: Identify the Biggest Drop\n\nCalculate both absolute and relative drop-off per step. The biggest optimization\nopportunity isn't always the biggest percentage drop — it's the step where the\nmost recoverable users are lost.\n\n| Step | Users In | Users Out | Drop-off | Recoverable? |\n|------|---------|----------|----------|--------------|\n| [step] | [N] | [N] | [%] | High/Med/Low |\n\n**Recoverable** = users who show intent but don't complete. Low recoverability =\nusers who were never the right audience.\n\n### Step 3: Generate Hypotheses\n\nFor each major drop-off, systematically consider:\n\n**Motivation problems:** Users don't want to complete this step.\n- Value proposition unclear at this point\n- Asked for too much commitment too early\n- Competing alternatives are easier\n\n**Ability problems:** Users can't complete this step.\n- UX is confusing or broken\n- Too many fields / too much information required\n- Technical barriers (load time, browser compatibility, mobile issues)\n\n**Trigger problems:** Users lose momentum.\n- No clear CTA or next step\n- Distraction or interruption\n- Too much time between steps\n\n### Step 4: Prioritize Hypotheses\n\n| Hypothesis | Evidence | Impact if True | Ease to Test | Priority |\n|-----------|----------|---------------|-------------|----------|\n| [hypothesis] | [what supports this] | [potential lift] | [effort] | [1-N] |\n\n### Step 5: Recommend Actions\n\nFor each top hypothesis:\n- **Quick test:** Fastest way to validate (5-user test, copy change, A/B test)\n- **Full solution:** What to build if the hypothesis is confirmed\n- **Expected impact:** Modeled lift based on current data\n\n## Output\n\n```\n# Funnel Diagnosis — [Funnel Name]\n\n## Current State\n[Funnel visualization with conversion rates]\n\n## Key Finding\n[The single biggest opportunity, in one sentence]\n\n## Step-by-Step Analysis\n### [Step with biggest drop-off]\n**Drop-off:** [X%] ([N] users lost)\n**Hypotheses:**\n1. [Hypothesis] — Evidence: [data] — Priority: High\n2. [Hypothesis] — Evidence: [data] — Priority: Medium\n\n[Repeat for each significant drop-off]\n\n## Recommended Action Plan\n1. [First thing to test/fix] — Expected impact: [X%] — Effort: [S/M/L]\n2. [Second] — Expected impact: [X%] — Effort: [S/M/L]\n3. [Third] — Expected impact: [X%] — Effort: [S/M/L]\n\n## Benchmarks\n[How does this funnel compare to industry standards?]\n```\n\nSave as `FUNNEL-DIAGNOSIS-[name]-[date].md`.\n", "hypothesis-generator": "---\nname: hypothesis-generator\ndescription: >\n  Generate competing hypotheses for product problems, evaluate evidence for each, and\n  recommend next steps. Prevents confirmation bias by forcing multiple explanations.\n  Trigger this skill when the user mentions hypothesis generation, root cause analysis,\n  competing hypotheses, diagnostic framework, or says things like \"why is this happening,\"\n  \"generate hypotheses for this problem,\" or \"what could explain this.\"\n---\n\n# Hypothesis Generator\n\nYou generate multiple competing explanations for product problems instead of letting\nthe team anchor on the first plausible story. The most dangerous hypothesis is the\none everyone agrees on before looking at data.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's the observation?** — What specific thing are you trying to explain?\n2. **Available data** — Metrics, user feedback, session recordings, support tickets.\n3. **Current theory** — What does the team already believe? (So we can challenge it.)\n4. **Constraints** — What explanations have been ruled out?\n\n## Process\n\n### Step 1: Frame the Observation Precisely\n\n\"[Metric/behavior] changed from [A] to [B] starting [date], affecting [segment].\"\n\nImprecise framing leads to imprecise hypotheses. Force specificity.\n\n### Step 2: Generate Hypotheses Across Categories\n\nFor each category, generate at least one hypothesis:\n\n**User behavior:** Something changed about what users want or do.\n**Product/UX:** Something in the product is causing this.\n**Technical:** A bug, performance issue, or infrastructure change.\n**External:** Market, competitive, seasonal, or platform changes.\n**Data:** The observation itself is an artifact of measurement.\n**Compositional:** The metric is the same but the user mix changed.\n\n### Step 3: Evaluate Each Hypothesis\n\n| # | Hypothesis | Category | Supporting Evidence | Contradicting Evidence | Testability |\n|---|-----------|----------|-------------------|----------------------|-------------|\n| H1 | [description] | [category] | [what supports it] | [what contradicts it] | [how to test] |\n| H2 | [description] | [category] | [what supports it] | [what contradicts it] | [how to test] |\n\n### Step 4: Rank by Plausibility\n\nScore each hypothesis:\n- **Evidence strength** (1-5): How much data supports this?\n- **Parsimony** (1-5): Is this the simplest explanation?\n- **Consistency** (1-5): Does it fit with everything else we know?\n\n### Step 5: Design Discriminating Tests\n\nThe best tests are ones that would confirm ONE hypothesis while disproving others.\n\n| Test | If Result Is X → Supports | If Result Is Y → Supports |\n|------|-------------------------|-------------------------|\n| [analysis/experiment] | H1 | H3 |\n| [analysis/experiment] | H2 | H1 |\n\n## Output\n\n```\n# Hypothesis Analysis — [Problem Statement]\n\n## Observation\n[Precise description]\n\n## Competing Hypotheses\n### H1: [Title] — Plausibility: [High/Med/Low]\n[Description, evidence for, evidence against]\n\n### H2: [Title] — Plausibility: [High/Med/Low]\n[Description, evidence for, evidence against]\n\n[Continue for all hypotheses]\n\n## Recommended Investigation\n1. [First test to run] — Discriminates between [H1 and H3]\n2. [Second test] — Discriminates between [H2 and H4]\n\n## Current Best Guess\n[Which hypothesis is most likely given current evidence, and confidence level]\n```\n\nSave as `HYPOTHESES-[problem]-[date].md`.\n", "metrics-debugger": "---\nname: metrics-debugger\ndescription: >\n  Debug metrics drops with first principles. When a key metric suddenly changes,\n  systematically identify whether it's a real product issue, a data issue, or an\n  external factor. Trigger this skill when the user mentions metrics drop, metric\n  debugging, KPI drop, dashboard anomaly, or says things like \"our metric just dropped,\"\n  \"why did [metric] go down,\" \"something looks off in the data,\" or \"investigate this\n  metrics change.\"\n---\n\n# Metrics Debugger\n\nA metric just moved unexpectedly. Your job is to figure out why — fast. Is it a real\nproduct problem, a data pipeline issue, an external factor, or a normal fluctuation?\nPanic is expensive; methodical debugging is cheap.\n\n## First Response\n\nWhen a metric drops, resist the urge to hypothesize immediately. Follow this sequence:\n\n### Step 1: Verify the Data\n\nBefore investigating the cause, confirm the drop is real:\n\n- [ ] **Is the data fresh?** Check pipeline latency. Is today's data fully loaded?\n- [ ] **Is the definition consistent?** Did someone change how the metric is calculated?\n- [ ] **Is the source stable?** Any data pipeline failures, schema changes, or ETL errors?\n- [ ] **Is sampling consistent?** If you use sampling, did the sample change?\n- [ ] **Check multiple sources.** Does the drop appear in both your analytics tool AND\n  the database? If only one shows it, it's likely a data issue.\n\nIf the data is wrong, stop. Fix the data. Don't debug a phantom problem.\n\n### Step 2: Scope the Impact\n\n```\n## Impact Assessment\n\n**Metric:** [name]\n**Change:** [X → Y] ([Z% change])\n**When:** [started date/time]\n**Duration:** [how long]\n\n**Affected segments:**\n- By platform: [iOS / Android / Web — which shows the drop?]\n- By geography: [specific regions?]\n- By user type: [new vs. existing, free vs. paid?]\n- By channel: [organic vs. paid, specific referrer?]\n\n**Related metrics:**\n- [Upstream metric]: [changed / stable]\n- [Downstream metric]: [changed / stable]\n- [Correlated metric]: [changed / stable]\n```\n\n### Step 3: Systematic Root Cause Analysis\n\nWork through these categories in order:\n\n**1. Internal product changes:**\n- Any deployments in the timeframe? (Check deploy logs)\n- Feature flag changes?\n- A/B test started or ended?\n- Infrastructure changes (CDN, load balancer, database migration)?\n\n**2. External factors:**\n- Seasonality (holiday, weekend, end of month)?\n- Competitor action (launch, outage, price change)?\n- News event affecting user behavior?\n- App store changes (algorithm, featuring, ranking)?\n- Platform changes (iOS update, Chrome update, API deprecation)?\n\n**3. User behavior shift:**\n- Traffic mix change (different channels sending different users)?\n- User segment shift (enterprise vs. SMB ratio)?\n- Bot traffic change?\n\n**4. Upstream dependency:**\n- Third-party API outage?\n- Payment provider issue?\n- Email deliverability change?\n- Ad platform targeting change?\n\n### Step 4: Quantify and Attribute\n\nOnce you've identified the likely cause:\n\n```\n## Root Cause\n\n**Primary cause:** [description]\n**Confidence:** High / Medium / Low\n**Evidence:** [what confirms this]\n\n**Impact breakdown:**\n- [X%] of the drop is explained by [cause 1]\n- [Y%] of the drop is explained by [cause 2]\n- [Z%] is unexplained / noise\n\n**Is this a one-time event or ongoing?** [assessment]\n```\n\n### Step 5: Recommend Response\n\n| Urgency | Criteria | Action |\n|---------|---------|--------|\n| Critical | Revenue-impacting, user-facing, growing | Fix immediately, war room |\n| High | Significant but contained | Fix this sprint, monitor hourly |\n| Medium | Moderate impact, stable | Investigate and fix within 1-2 weeks |\n| Low | Minor, self-correcting | Monitor, no action needed |\n\n## Output\n\n```\n# Metrics Debug — [Metric Name] [Date]\n\n## Summary\n[2-3 sentences: what happened, why, what to do]\n\n## Investigation Log\n[Step-by-step what you checked and found]\n\n## Root Cause\n[Identified cause with evidence and confidence]\n\n## Recommended Action\n[What to do + urgency level]\n\n## Monitoring Plan\n[What to watch to confirm the fix or detect recurrence]\n```\n\nSave as `METRICS-DEBUG-[metric]-[date].md`.\n", "dashboard-designer": "---\nname: dashboard-designer\ndescription: >\n  Design dashboards that show the right metrics to the right audience with clear hierarchy\n  and actionable layout. Trigger this skill when the user mentions dashboard design,\n  analytics dashboard, metrics dashboard, reporting dashboard, or says things like\n  \"design a dashboard for [audience],\" \"what should we show on the dashboard,\" or\n  \"organize our metrics display.\"\n---\n\n# Dashboard Designer\n\nA dashboard isn't a place to dump every metric you have. It's an information radiator\nthat answers a specific set of questions for a specific audience. If the viewer has\nto think about what they're looking at, the dashboard has failed.\n\n## Before You Start\n\nAsk the user:\n\n1. **Who's the audience?** — Execs, PMs, engineers, ops, customers. Each needs different views.\n2. **What questions should it answer?** — 3-5 specific questions, not \"show everything.\"\n3. **What decisions does it support?** — What will someone DO after looking at it?\n4. **Data available** — What metrics exist and what needs to be built?\n5. **Refresh cadence** — Real-time, hourly, daily, weekly.\n\n## Design Process\n\n### Step 1: Define the Information Hierarchy\n\n**Level 1 (Glance — 3 seconds):** Am I on track? Is anything on fire?\n→ 3-5 headline metrics with trend indicators (up/down/flat)\n\n**Level 2 (Scan — 30 seconds):** What's driving the numbers?\n→ Key breakdowns, top-line charts, segment comparisons\n\n**Level 3 (Analyze — 5 minutes):** Where should I dig deeper?\n→ Detailed tables, drill-down views, time series\n\n### Step 2: Layout Design\n\n```\n┌─────────────────────────────────────────────────┐\n│  LEVEL 1: Headline Metrics (3-5 KPIs)           │\n│  [Metric] [Trend]  [Metric] [Trend]  [Metric]  │\n├──────────────────────┬──────────────────────────┤\n│  LEVEL 2: Primary    │  LEVEL 2: Secondary      │\n│  Chart/Breakdown     │  Chart/Breakdown          │\n│                      │                           │\n├──────────────────────┴──────────────────────────┤\n│  LEVEL 3: Detailed Table / Drill-down            │\n│                                                   │\n└─────────────────────────────────────────────────┘\n```\n\n### Step 3: Metric Selection per Audience\n\n| Audience | Key Questions | Metrics | Cadence |\n|----------|-------------|---------|---------|\n| Exec | Are we growing? Are we profitable? | MRR, growth rate, burn | Weekly |\n| PM | Is the product healthy? What needs attention? | Activation, retention, feature adoption | Daily |\n| Engineering | Is the system healthy? | Uptime, latency, error rate | Real-time |\n| Support | What are users struggling with? | Ticket volume, resolution time, top issues | Daily |\n| Customers | Am I getting value? | Usage, ROI, team activity | On-demand |\n\n### Step 4: Visualization Principles\n\n- **Time series** for trends (line chart)\n- **Bar charts** for comparison across categories\n- **Single numbers** for current state with trend arrows\n- **Tables** for detailed drill-down data\n- **Heatmaps** for patterns across two dimensions\n- **Never use pie charts** for more than 3 segments\n\nColor coding:\n- Green = on track / improving\n- Yellow = watch / declining\n- Red = needs action / below threshold\n\n### Step 5: Alert and Threshold Design\n\nEvery headline metric needs:\n- **Green threshold:** [metric] > [value] — all good\n- **Yellow threshold:** [value] > [metric] > [value] — investigate\n- **Red threshold:** [metric] < [value] — action required\n\n## Output\n\n```\n# Dashboard Design — [Name] for [Audience]\n\n## Purpose\n[What questions this dashboard answers]\n\n## Metrics Inventory\n| Metric | Definition | Source | Refresh | Threshold (R/Y/G) |\n|--------|-----------|--------|---------|-------------------|\n| [metric] | [how calculated] | [data source] | [cadence] | [thresholds] |\n\n## Layout Specification\n[Wireframe with metric placement and chart types]\n\n## Interaction Design\n- [Filters available]\n- [Drill-down paths]\n- [Date range controls]\n- [Export capabilities]\n\n## Technical Requirements\n- Data sources: [list]\n- Refresh frequency: [cadence]\n- Access control: [who sees what]\n- Tool recommendation: [Looker/Tableau/Metabase/custom]\n```\n\nSave as `DASHBOARD-DESIGN-[name].md`.\n", "onboarding-designer": "---\nname: onboarding-designer\ndescription: >\n  Design onboarding flows that guide users to value quickly. Focuses on time-to-value\n  optimization and activation rate improvement. Trigger this skill when the user mentions\n  onboarding, first-time experience, FTUE, activation flow, welcome flow, setup wizard,\n  time to value, or says things like \"design the onboarding,\" \"users are dropping off\n  during setup,\" or \"improve our first-time experience.\"\n---\n\n# Onboarding Designer\n\nGreat onboarding doesn't teach users your product — it helps them accomplish their\ngoal as fast as possible. Every screen, step, and tooltip should either move the user\ncloser to their \"aha moment\" or get out of the way.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's the aha moment?** — The specific action where users first experience value.\n2. **Current onboarding** — What exists today? Where do users drop off?\n3. **User segments** — Different users may need different onboarding paths.\n4. **Constraints** — Required data collection, compliance, technical limitations.\n\n## Design Process\n\n### Step 1: Define the Aha Moment\n\nThe aha moment is the action most correlated with long-term retention. Everything\nin onboarding exists to get users here faster.\n\n\"Users who [specific action] within [time window] retain at [X%] vs [Y%] for those who don't.\"\n\n### Step 2: Map the Critical Path\n\nWhat's the minimum number of steps between signup and aha moment?\n\n```\nCurrent path: Signup → Profile → Settings → Tutorial → Invite → [Aha!]  (6 steps)\nOptimized:    Signup → [Aha!] → Profile → Invite  (4 steps, aha moved earlier)\n```\n\nRules:\n- Move the aha moment as early as possible\n- Defer everything that isn't needed for the aha moment\n- Ask for information only when you need it (progressive profiling)\n- Show value before asking for commitment\n\n### Step 3: Design Each Step\n\nFor every onboarding step:\n\n```\n### Step [N]: [Name]\n\n**Purpose:** Why does this step exist? [If you can't answer, cut it.]\n**User sees:** [What's on screen]\n**User does:** [What action they take]\n**Data collected:** [What you learn — for product analytics or personalization]\n**Success criteria:** [X% of users complete this step]\n**Drop-off mitigation:** [What happens if they try to leave]\n\n**Skip logic:** [Can this step be skipped? Under what conditions?]\n**Progressive disclosure:** [Start simple, reveal complexity as needed]\n```\n\n### Step 4: Design for Segments\n\n| Segment | Aha Moment | Key Path Difference | Personalization |\n|---------|-----------|--------------------|--------------------|\n| [segment] | [their aha] | [different steps?] | [what changes] |\n\n### Step 5: Design Recovery Mechanisms\n\nUsers will abandon onboarding. Plan for it:\n- **Save progress** — Don't make them restart\n- **Re-engagement emails** — Triggered by abandonment, focused on value\n- **Simplified re-entry** — When they come back, show one clear next step\n- **Alternative paths** — Some users prefer self-service exploration over guided flows\n\n## Output\n\n```\n# Onboarding Design — [Product Name]\n\n## Aha Moment\n[Definition and retention correlation data]\n\n## Onboarding Flow\n[Step-by-step design with wireframe descriptions]\n\n## Segment Variations\n[Per-segment adjustments]\n\n## Metrics\n| Metric | Current | Target |\n|--------|---------|--------|\n| Onboarding completion rate | [X%] | [Y%] |\n| Time to aha moment | [X days] | [Y days] |\n| D7 retention (completed onboarding) | [X%] | [Y%] |\n\n## Recovery & Re-engagement Plan\n[Email triggers, re-entry flows]\n```\n\nSave as `ONBOARDING-DESIGN-[product-name].md`.\n", "daily-planner": "---\nname: daily-planner\ndescription: >\n  Generate your daily plan aligned to goals and priorities using Shreyas Doshi's LNO\n  framework. Start every day knowing what to focus on and what to defer. Trigger this\n  skill when the user mentions daily plan, today's priorities, daily focus, morning\n  planning, day planning, or says things like \"plan my day,\" \"what should I focus on\n  today,\" or \"help me prioritize today.\"\n---\n\n# Daily Planner (LNO-Powered)\n\nYou create a daily plan that ensures the user spends their peak energy on Leverage\nwork, handles Neutral tasks efficiently, and minimizes time on Overhead.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's on your plate today?** — Meetings, tasks, deadlines, ongoing work.\n2. **What are your current goals/OKRs?** — Helps identify what's leverage.\n3. **When's your peak energy?** — Morning person? Afternoon person?\n4. **Any hard deadlines today?** — Non-movable commitments.\n5. **How are you feeling?** — Low energy days need a different plan than high energy days.\n\n## Daily Plan Template\n\n```\n# Daily Plan — [Date]\n\n## Today's Focus (One Sentence)\n\"Today I will [one leverage task that would make today a success even if nothing else gets done].\"\n\n## 🔴 LEVERAGE (Peak Energy Block: [time range])\nProtected deep work time. No meetings, no Slack, no email.\n\n1. [Leverage task] — [Why it matters] — [Definition of done]\n   Time allocated: [X hours]\n\n## 🟡 NEUTRAL (Moderate Energy: [time range])\nDo these well but don't agonize.\n\n1. [Task] — Time box: [X min]\n2. [Task] — Time box: [X min]\n3. [Meeting]: [purpose] — Prep needed: [Y/N]\n\n## ⚪ OVERHEAD (Low Energy: [time range])\nBatch and blast. Minimum viable effort.\n\n- [ ] Email triage (15 min max)\n- [ ] Slack catch-up (10 min max)\n- [ ] [Admin task]\n- [ ] [Status update]\n\n## Today's Meetings\n| Time | Meeting | Purpose | My Role | Prep Needed |\n|------|---------|---------|---------|-------------|\n| [time] | [name] | [why] | [present/listen/decide] | [what to prep] |\n\n## Parking Lot\nThings on my mind that are NOT for today:\n- [Item] → [When I'll handle it]\n\n## End of Day Check\n- [ ] Did I complete my leverage task?\n- [ ] Did I protect my deep work block?\n- [ ] What's the ONE thing I need to start tomorrow with?\n```\n\nSave as `DAILY-PLAN-[date].md`.\n", "launch-checklist": "---\nname: launch-checklist\ndescription: >\n  Create comprehensive launch checklists customized to release type (new feature, major\n  release, beta, hotfix). Ensures nothing falls through the cracks across engineering,\n  design, marketing, support, legal, and analytics. Trigger this skill when the user\n  mentions launch checklist, release checklist, go-live checklist, launch readiness,\n  ship checklist, or says things like \"are we ready to launch,\" \"create a launch plan,\"\n  or \"what do we need before we ship.\"\n---\n\n# Launch Checklist Generator\n\nLaunches fail when someone forgets to update the help docs, or the support team\ndoesn't know about the new feature, or analytics aren't instrumented. This skill\ngenerates a comprehensive, customized checklist so nothing falls through the cracks.\n\n## Before You Start\n\nAsk the user:\n\n1. **What's launching?** — New feature, major release, beta, hotfix, pricing change.\n2. **Launch type** — Big bang (all users at once), phased rollout, beta/early access.\n3. **Date** — Fixed or flexible?\n4. **Teams involved** — Engineering, design, marketing, support, sales, legal, ops.\n5. **Risk level** — High (revenue-impacting, customer-facing) or low (minor enhancement).\n\n## Launch Checklist\n\n```\n# Launch Checklist — [Feature/Release Name]\n\n**Target date:** [date] | **Launch type:** [type] | **Risk level:** [High/Med/Low]\n**DRI:** [name] | **Status:** 🔴 Not Ready / 🟡 In Progress / 🟢 Ready\n\n---\n\n## T-2 Weeks: Pre-Launch Prep\n\n### Engineering\n- [ ] Code complete and merged to main\n- [ ] Feature flagged and testable in staging\n- [ ] Load/performance testing complete\n- [ ] Security review complete (if applicable)\n- [ ] Database migrations tested and rollback plan documented\n- [ ] Monitoring and alerting configured\n- [ ] Runbook for on-call team created\n\n### Design\n- [ ] Final designs reviewed and approved\n- [ ] Edge cases and error states designed\n- [ ] Accessibility review complete\n- [ ] Mobile/responsive testing done\n\n### Analytics\n- [ ] Events instrumented and verified in staging\n- [ ] Dashboard created or updated\n- [ ] Success metrics baseline captured\n- [ ] A/B test configured (if applicable)\n\n### Documentation\n- [ ] Help center articles written/updated\n- [ ] API documentation updated (if applicable)\n- [ ] Internal knowledge base updated\n- [ ] Changelog/release notes drafted\n\n---\n\n## T-1 Week: Readiness Check\n\n### QA\n- [ ] Full regression test passed\n- [ ] Cross-browser testing complete\n- [ ] Edge case testing complete\n- [ ] Accessibility testing passed\n- [ ] Sign-off from QA lead\n\n### Support\n- [ ] Support team briefed on new feature\n- [ ] FAQ and troubleshooting guide created\n- [ ] Escalation path defined for new issues\n- [ ] Canned responses updated\n\n### Marketing / Communications\n- [ ] In-app messaging configured\n- [ ] Email announcement drafted and scheduled\n- [ ] Blog post / changelog entry ready\n- [ ] Social media posts scheduled (if applicable)\n- [ ] Sales team notified and collateral updated\n\n### Legal / Compliance (if applicable)\n- [ ] Privacy review complete\n- [ ] Terms of service updates (if needed)\n- [ ] Regulatory compliance verified\n\n### Stakeholders\n- [ ] Final stakeholder sign-off obtained\n- [ ] Executive briefing sent\n- [ ] Partner/integration teams notified\n\n---\n\n## Launch Day\n\n### Go / No-Go Decision\n- [ ] All checklist items above are ✅\n- [ ] On-call engineer identified and available\n- [ ] Rollback plan tested and ready\n- [ ] Communication plan ready (in-app, email, social)\n\n### Execution\n- [ ] Feature flag enabled for target audience\n- [ ] Smoke test in production\n- [ ] Monitoring dashboards open and watched\n- [ ] Communications sent\n\n---\n\n## T+1 Day: Post-Launch\n\n- [ ] Metrics checked against baseline\n- [ ] Error rates and support tickets reviewed\n- [ ] Any urgent bugs triaged\n- [ ] Team retro scheduled (T+1 week)\n\n## T+1 Week: Post-Launch Review\n\n- [ ] Metrics report generated\n- [ ] User feedback collected and reviewed\n- [ ] Bugs and issues cataloged\n- [ ] Retro completed with learnings documented\n- [ ] Feature flag cleanup (remove if fully shipped)\n\n---\n\n## Kill Criteria\nIf any of these occur, immediately roll back:\n- [ ] Error rate exceeds [X%]\n- [ ] [Critical metric] degrades by more than [Y%]\n- [ ] [Support tickets] exceed [N] in first 24 hours\n```\n\nSave as `LAUNCH-CHECKLIST-[name]-[date].md`.\n", "meeting-agenda": "---\nname: meeting-agenda\ndescription: >\n  Generate structured meeting agendas with time blocks, objectives, and expected outcomes.\n  Uses the SCQA framework for framing discussion topics. Trigger this skill when the user\n  mentions meeting agenda, agenda, meeting prep, meeting structure, or says things like\n  \"create an agenda for this meeting,\" \"structure this discussion,\" or \"prep me for\n  this meeting.\"\n---\n\n# Meeting Agenda Generator\n\nEvery meeting should have an agenda. Meetings without agendas are just group therapy\nwith a calendar invite. You create agendas that ensure every meeting ends with\nclear decisions or next steps.\n\n## Before You Start\n\nAsk the user:\n\n1. **Meeting purpose** — Decision meeting, brainstorm, status update, 1:1, kickoff.\n2. **Attendees** — Who's coming and what's their role in the meeting.\n3. **Duration** — 30 min, 45 min, 60 min.\n4. **Topics** — What needs to be covered.\n5. **Desired outcome** — What success looks like when the meeting ends.\n\n## Agenda Template\n\n```\n# [Meeting Title]\n\n**Date:** [date] | **Time:** [time] | **Duration:** [minutes]\n**Organizer:** [name] | **Facilitator:** [name]\n\n## Objective\n[One sentence: what we need to accomplish]\n\n## Pre-reads\n- [Document/link] — [What to review and why] — [5-10 min read time]\n\n## Attendees & Roles\n| Name | Role | Expected Contribution |\n|------|------|---------------------|\n| [name] | Decision maker | Final call on [topic] |\n| [name] | Subject expert | Context on [area] |\n| [name] | Stakeholder | Input on [perspective] |\n\n---\n\n## Agenda\n\n### [Time] — [Topic 1] ([X min]) — [Owner]\n**Frame (SCQA):**\n- **Situation:** [Current state]\n- **Complication:** [What changed or what's the problem]\n- **Question:** [What we need to resolve]\n- **Answer:** [Proposed answer, if any — for discussion]\n\n**Goal:** [Decision / Alignment / Input / Information]\n\n### [Time] — [Topic 2] ([X min]) — [Owner]\n[Same SCQA structure]\n\n### [Time] — Action Items & Next Steps ([5 min])\n- Review decisions made\n- Assign action items with owners and deadlines\n- Confirm next meeting if needed\n\n---\n\n## Parking Lot\n[Topics that came up but aren't for this meeting]\n\n## Post-Meeting\n- [ ] Notes sent within 24 hours\n- [ ] Action items tracked in [tool]\n- [ ] Follow-up meeting scheduled (if needed)\n```\n\n## Meeting Type Templates\n\n**Decision meeting:** Lead with the decision needed, present options with pros/cons,\nallocate 60% of time to discussion, end with explicit decision.\n\n**Brainstorm:** Set constraints upfront, silent brainstorm first (3 min), share and\nbuild, cluster themes, vote on top ideas.\n\n**Status update:** Consider whether this needs to be a meeting or could be async.\nIf meeting: each person gets 2 min max, focus on blockers not accomplishments.\n\n**1:1:** Let the direct report set the agenda. Manager adds topics at the end.\nFocus on coaching, not status.\n\nSave as `AGENDA-[meeting-name]-[date].md`.\n", "stakeholder-update": "---\nname: stakeholder-update\ndescription: >\n  Write executive stakeholder updates using the SCQA (Situation-Complication-Question-Answer)\n  framework. Tailored to audience: executives get outcomes, engineering gets details.\n  Trigger this skill when the user mentions stakeholder update, status update, executive\n  update, weekly update, monthly report, progress report, or says things like \"write my\n  weekly update,\" \"send a status to leadership,\" or \"update stakeholders on progress.\"\n---\n\n# Stakeholder Update Generator\n\nStakeholder updates exist to maintain trust and surface decisions. They're not activity\nlogs — nobody cares how many meetings you had. They're trust-building documents that\nanswer: \"Is this on track, and do you need anything from me?\"\n\n## Before You Start\n\nAsk the user:\n\n1. **Who's the audience?** — Exec, cross-functional peers, board, engineering, customers.\n2. **Cadence** — Weekly, biweekly, monthly, ad hoc.\n3. **What happened?** — Progress, blockers, decisions made, metrics changes.\n4. **What do you need?** — Decisions, resources, air cover, nothing (just informing).\n5. **Tone** — Routine update vs. escalation vs. celebration.\n\n## Update Templates\n\n### Executive Update (SCQA Framework)\n\n```\n# [Project/Product] Update — [Date]\n\n**Overall status:** 🟢 On Track / 🟡 At Risk / 🔴 Off Track\n\n## Situation\n[1-2 sentences: Where we are. Factual, no spin.]\n\n## Complication\n[1-2 sentences: What's changed, what's challenging, what's new.\nIf nothing: \"No new complications since last update.\"]\n\n## Question\n[The key question this update raises — or \"No decision needed this cycle.\"]\n\n## Answer / Recommendation\n[If there's a question: your recommended path forward.\nIf no question: key progress and next milestones.]\n\n---\n\n## Metrics Snapshot\n| Metric | Last Period | This Period | Trend | Target |\n|--------|-----------|------------|-------|--------|\n| [metric] | [value] | [value] | ↑↓→ | [target] |\n\n## Key Accomplishments\n- [Achievement and its impact — not activity, impact]\n- [Achievement]\n\n## Risks & Blockers\n| Risk/Blocker | Impact | Mitigation | Help Needed? |\n|-------------|--------|------------|-------------|\n| [issue] | [impact] | [plan] | [yes/no — what] |\n\n## Next Period Focus\n1. [Priority 1]\n2. [Priority 2]\n3. [Priority 3]\n\n## Asks\n- [Specific ask from specific person, if any]\n```\n\n### Engineering / Cross-Functional Update\n\n```\n# [Project] Engineering Update — [Date]\n\n**Sprint:** [N] | **Velocity:** [points] | **Status:** [On Track / At Risk]\n\n## Shipped This Sprint\n- [Feature/fix] — [Impact] — [PR/ticket link]\n\n## In Progress\n| Item | Owner | Status | ETA | Blocker? |\n|------|-------|--------|-----|----------|\n| [item] | [name] | [%] | [date] | [Y/N] |\n\n## Technical Decisions Made\n- [Decision] — [Rationale] — [Trade-offs accepted]\n\n## Technical Debt / Risks\n- [Issue] — [Severity] — [Plan]\n\n## Next Sprint\n- [Planned work]\n```\n\n## Writing Rules\n\n- **Lead with status.** Green/Yellow/Red in the first line. Don't make readers\n  search for the bottom line.\n- **No surprises.** If status is changing from green to yellow, explain why\n  BEFORE it goes red. Leaders hate surprises.\n- **Be specific about asks.** \"Need help\" is useless. \"Need VP of Eng to\n  approve 2 additional engineers for 3 weeks to hit Q2 deadline\" is actionable.\n- **Own bad news.** Don't hide behind passive voice. \"Timeline slipped because\n  we underestimated the migration complexity\" builds more trust than\n  \"Timeline adjustments were required.\"\n- **Keep it scannable.** Executives read 50 updates a week. Respect their time.\n\nSave as `UPDATE-[project]-[date].md`.\n", "weekly-planner": "---\nname: weekly-planner\ndescription: >\n  Generate weekly priorities aligned to quarterly goals. Know what matters this week and\n  what can wait. Trigger this skill when the user mentions weekly plan, weekly priorities,\n  week planning, sprint planning personal, or says things like \"plan my week,\" \"what\n  should I focus on this week,\" or \"set weekly priorities.\"\n---\n\n# Weekly Planner\n\nYou create a weekly plan that connects daily work to quarterly goals. The best week\nis one where you made meaningful progress on 2-3 things that actually matter — not\none where you attended 20 meetings and answered 100 emails.\n\n## Before You Start\n\nAsk the user:\n\n1. **What are your quarterly goals/OKRs?** — These anchor the week.\n2. **What carried over from last week?** — Unfinished leverage work.\n3. **What's on the calendar this week?** — Meetings, deadlines, commitments.\n4. **Any blockers or dependencies?** — Things you're waiting on.\n5. **Energy forecast** — Any days that will be disrupted (travel, dentist, etc.)?\n\n## Weekly Plan Template\n\n```\n# Weekly Plan — Week of [Date]\n\n## This Week's Theme\n[One sentence: what will make this a successful week?]\n\n## Quarterly Goal Connection\n| Q Goal | This Week's Contribution | Progress |\n|--------|------------------------|----------|\n| [OKR 1] | [What I'll advance] | [On track / Behind / Ahead] |\n| [OKR 2] | [What I'll advance] | [On track / Behind / Ahead] |\n\n## Top 3 Priorities (Must Complete)\n1. **[Priority]** — Connected to: [OKR] — Definition of done: [criteria]\n2. **[Priority]** — Connected to: [OKR] — Definition of done: [criteria]\n3. **[Priority]** — Connected to: [OKR] — Definition of done: [criteria]\n\n## Secondary Items (If Time Allows)\n- [Item] — [Why it matters]\n- [Item] — [Why it matters]\n\n## Day-by-Day Blueprint\n| Day | Focus Block (Leverage) | Meetings | Admin |\n|-----|----------------------|----------|-------|\n| Mon | [deep work] | [meetings] | [overhead] |\n| Tue | [deep work] | [meetings] | [overhead] |\n| Wed | [deep work] | [meetings] | [overhead] |\n| Thu | [deep work] | [meetings] | [overhead] |\n| Fri | [deep work / wrap-up] | [meetings] | [weekly review] |\n\n## Blocked / Waiting On\n| Item | Waiting on | Follow-up date | If no response, then... |\n|------|-----------|---------------|------------------------|\n| [item] | [person] | [date] | [plan B] |\n\n## Saying No This Week\n[Things I'm explicitly NOT doing and why — protects focus]\n\n## Friday Review Prompts\n- Did I complete my top 3 priorities?\n- What moved the needle most?\n- What should I stop doing?\n- What's the #1 priority for next week?\n```\n\nSave as `WEEKLY-PLAN-[date].md`.\n", "ai-opportunity-finder": "---\nname: ai-opportunity-finder\ndescription: >\n  Identify AI/ML opportunities for your product. Evaluates where AI can create genuine\n  user value vs. where it's just hype. Inspired by Nate B Jones's pragmatic approach\n  to AI-first product strategy. Trigger this skill when the user mentions AI strategy,\n  AI opportunities, AI features, ML opportunities, where to use AI, AI product strategy,\n  or says things like \"where should we use AI in our product,\" \"find AI opportunities,\"\n  or \"should we add AI to this feature.\"\n---\n\n# AI Opportunity Finder\n\nNot every product needs AI. But every product team should know where AI would create\ngenuine value vs. where it's a distraction. This skill helps you find real AI\nopportunities — not \"add a chatbot\" but \"here's where AI fundamentally improves the\nuser experience.\"\n\n## Before You Start\n\nAsk the user:\n\n1. **What's the product?** — What it does, who uses it, core workflows.\n2. **Current pain points** — Where do users struggle, wait, or do repetitive work?\n3. **Data available** — What user data, content, or behavioral data exists?\n4. **Team capability** — Do you have ML engineers, or would this require hiring/buying?\n5. **Strategic context** — Is AI a strategic priority, or exploring early?\n\n## Opportunity Identification Framework\n\n### Step 1: Map User Workflows\n\nFor each core workflow, identify where users:\n\n| Workflow Step | User Action | Time Spent | Friction | Data Available? |\n|-------------|-------------|-----------|----------|----------------|\n| [step] | [what they do] | [minutes] | [High/Med/Low] | [Yes/No — what data] |\n\n### Step 2: AI Opportunity Filter\n\nFor each high-friction step, evaluate AI fit:\n\n**Good AI candidates:**\n- Tasks that are repetitive but require judgment (classification, prioritization, summarization)\n- Processes with lots of data but little insight (pattern detection, anomaly detection)\n- Personalization at scale (recommendations, content curation)\n- Content generation with human review (drafts, suggestions, completions)\n- Prediction based on historical patterns (forecasting, risk scoring)\n\n**Bad AI candidates (Nate B Jones principle: don't force it):**\n- Tasks requiring perfect accuracy with no tolerance for errors\n- Workflows where users need to trust and verify every output\n- Problems that are simpler to solve with traditional software\n- Features where AI adds complexity without proportional value\n- Areas with insufficient or biased training data\n\n### Step 3: Evaluate Each Opportunity\n\n| Opportunity | AI Approach | User Value | Data Ready? | Build/Buy | Effort | Risk |\n|------------|-----------|-----------|------------|-----------|--------|------|\n| [opportunity] | [technique] | [benefit] | [Y/N] | [build/buy/partner] | [S/M/L] | [H/M/L] |\n\n### Step 4: Value Assessment\n\nFor each opportunity, answer:\n\n**User value:** Does this save time, reduce errors, or enable something previously impossible?\n**Business value:** Does this increase revenue, reduce cost, or create competitive moat?\n**Feasibility:** Can we build this with available data, team, and infrastructure?\n**Risk:** What happens when the AI is wrong? How bad is a false positive/negative?\n**Trust:** Will users trust AI in this context? How do we build that trust?\n\n### Step 5: Implementation Strategy\n\nFor top opportunities:\n\n**Start with augmentation, not automation.** AI suggests, human decides. This builds\ntrust and generates training data for better models.\n\n**Build the feedback loop.** Every AI feature needs a way for users to correct it.\nThese corrections become training data that makes the model better over time.\n\n**Plan for failure gracefully.** What's the user experience when the AI is wrong?\nIf there's no good fallback, the feature isn't ready.\n\n## Output\n\n```\n# AI Opportunity Assessment — [Product Name]\n\n## Executive Summary\n[2-3 sentences: top opportunities, strategic recommendation, readiness assessment]\n\n## Opportunity Map\n| Priority | Opportunity | Value | Feasibility | Recommended Approach |\n|----------|-----------|-------|-------------|---------------------|\n| 1 | [opportunity] | High | High | [approach] |\n| 2 | [opportunity] | High | Medium | [approach] |\n| 3 | [opportunity] | Medium | High | [approach] |\n\n## Detailed Analysis\n[Per-opportunity deep dive]\n\n## What NOT to Build with AI\n[Opportunities that were evaluated and rejected, with reasoning]\n\n## Prerequisites\n- Data requirements: [what's needed]\n- Team requirements: [hire/upskill/partner]\n- Infrastructure: [what's needed]\n\n## Recommended Roadmap\n- **Phase 1 (0-3 months):** [Quick win AI feature — augmentation mode]\n- **Phase 2 (3-6 months):** [Higher complexity — with feedback loop data from Phase 1]\n- **Phase 3 (6-12 months):** [Full automation where trust has been established]\n```\n\nSave as `AI-OPPORTUNITIES-[product-name].md`.\n", "ai-product-review": "---\nname: ai-product-review\ndescription: >\n  Review PRDs and feature specs from the user's perspective to predict adoption,\n  support burden, and satisfaction impact. Simulates how real users will experience\n  the product before it's built. Trigger this skill when the user mentions product\n  review, user perspective review, adoption prediction, UX review, customer impact\n  review, or says things like \"review this from the user's perspective,\" \"will users\n  actually use this,\" or \"predict how customers will react.\"\n---\n\n# AI Product Review (User Perspective Simulator)\n\nYou review product specs by simulating how actual users will experience the feature.\nNot how the PM HOPES they'll use it — how they WILL use it, including the parts\nwhere they get confused, abandon the flow, or file a support ticket.\n\n## Before You Start\n\nAsk the user:\n\n1. **The spec** — PRD, feature brief, or design mockups.\n2. **Target personas** — Who are the primary users?\n3. **Current behavior** — What do they do today? What are they switching from?\n4. **Success metrics** — What does the PM hope to achieve?\n\n## Review Dimensions\n\n### 1. Adoption Prediction\n\n**Will users discover this?**\n- How will they learn about it? (In-app, email, word of mouth, support)\n- Is the entry point obvious or buried?\n- Does the naming/labeling match what users would search for?\n\n**Will users try it?**\n- Is the value proposition clear in the first 5 seconds?\n- What's the effort to try it? (One click? 10 minutes of setup?)\n- Is there a \"try before you commit\" path?\n\n**Will users keep using it?**\n- Does it solve a recurring need or a one-time problem?\n- Is it faster/better than their current workaround?\n- What would make them revert to the old way?\n\n**Adoption risk score:** High / Medium / Low — with rationale.\n\n### 2. Usability Assessment\n\nWalk through the proposed flow as each persona:\n\n```\n### Persona: [Name]\n**Goal:** [What they're trying to accomplish]\n\nStep 1: [What they see] → [What they'd think] → [What they'd do]\nStep 2: [What they see] → [What they'd think] → [What they'd do]\n...\n**Likely confusion points:** [Where they'd get stuck]\n**Likely frustration points:** [Where they'd get annoyed]\n**Likely delight points:** [Where they'd think \"nice, this is good\"]\n```\n\n### 3. Support Burden Prediction\n\n| Predicted Support Issue | Frequency | Preventable? | Mitigation |\n|------------------------|-----------|-------------|------------|\n| [issue users will contact support about] | High/Med/Low | Yes/No | [how to prevent] |\n\n### 4. Satisfaction Impact\n\n**What will increase satisfaction:**\n- [Specific aspect that users will appreciate]\n\n**What will decrease satisfaction:**\n- [Specific aspect that will frustrate users]\n- [Edge case that will generate complaints]\n\n**Net satisfaction prediction:** Positive / Neutral / Negative\n\n### 5. Competitive Reaction\n\nHow does this compare to how competitors solve the same problem?\n- [Better than competitor X at...]\n- [Worse than competitor Y at...]\n- [Differentiated from everyone by...]\n\n## Output\n\n```\n# Product Review — [Feature Name]\n\n## Summary Verdict\n**Adoption prediction:** [High/Medium/Low confidence of adoption]\n**User satisfaction impact:** [Positive/Neutral/Negative]\n**Support burden:** [High/Medium/Low]\n**Ship recommendation:** [Ship as-is / Ship with changes / Rethink]\n\n## Detailed Review\n[Per-dimension analysis]\n\n## Top 3 Improvements (Highest Impact)\n1. [Change] — [Why it matters to users] — [Effort]\n2. [Change] — [Why] — [Effort]\n3. [Change] — [Why] — [Effort]\n\n## What's Strong\n[Genuine praise for what will work well]\n```\n\nSave as `PRODUCT-REVIEW-[feature-name].md`.\n", "rapid-iteration-planner": "---\nname: rapid-iteration-planner\ndescription: >\n  Plan fast iteration cycles using AI-first workflows. Inspired by Nate B Jones (volume\n  over perfection) and Lenny Rachitsky (AI-first PM workflows using PRDs and Markdown\n  to keep AI agents aligned). Trigger this skill when the user mentions rapid iteration,\n  fast shipping, iteration planning, build-measure-learn, speed of execution, or says\n  things like \"how do we ship faster,\" \"plan our iteration cycles,\" \"help us move\n  quicker,\" or \"set up a rapid prototyping process.\"\n---\n\n# Rapid Iteration Planner\n\nSpeed of iteration beats quality of planning. The fastest path to a good product is\nshipping fast, measuring fast, and learning fast. This skill helps you plan iteration\ncycles that maximize learning per unit of time.\n\n## Core Principles\n\n**From Nate B Jones:** Volume over perfection. Hesitation is the real risk. Ship\nsmall, learn fast, compound the learnings.\n\n**From Lenny Rachitsky:** Use PRDs and Markdown files to keep AI agents aligned\nacross complex builds. Spend more time planning and reviewing than prompting.\nUse a debugging workflow when stuck.\n\n**From Shreyas Doshi:** Focus on leverage tasks. Not all iterations are equal —\nspend iteration cycles on the things with 10x return potential.\n\n## Before You Start\n\nAsk the user:\n\n1. **What are we iterating on?** — Feature, product, workflow, experiment.\n2. **What's the goal?** — Metric to move, hypothesis to validate, MVP to ship.\n3. **Team size and composition** — Who's available?\n4. **Current cadence** — How fast are you shipping today?\n5. **Blockers to speed** — What slows you down? (Approvals, testing, dependencies, perfectionism)\n\n## Iteration Planning Framework\n\n### Step 1: Define the Learning Goal\n\nEach iteration cycle should answer ONE question:\n\n\"By the end of this cycle, we will know [specific thing] because we [shipped/tested/measured X].\"\n\nIf the learning goal is vague, the iteration will be unfocused.\n\n### Step 2: Design the Minimum Viable Test\n\nWhat's the absolute smallest thing we can build to answer our question?\n\n| Approach | Build Time | Learning Quality | Trade-off |\n|----------|-----------|-----------------|-----------|\n| Fake door (button → waitlist) | 1 day | Demand signal only | No usage data |\n| Wizard of Oz (manual backend) | 3 days | Real usage | Doesn't scale |\n| Hardcoded prototype | 1 week | Real usage + some edge cases | Technical debt |\n| Production feature (limited) | 2 weeks | Full signal | Longer cycle |\n\nChoose the approach that maximizes learning per day of engineering.\n\n### Step 3: Plan the Cycle\n\n```\n## Iteration Cycle — [Name]\n\n**Learning goal:** [What we'll know at the end]\n**Approach:** [What we're building]\n**Duration:** [X days]\n**Team:** [Who's working on this]\n\n### Day-by-Day Plan\n| Day | Focus | Deliverable | Decision Point |\n|-----|-------|------------|---------------|\n| D1 | Design + plan | Spec + mockup | Go/no-go on approach |\n| D2-D3 | Build | Working prototype | Check feasibility |\n| D4 | Test + instrument | Deployed + instrumented | Verify data collection |\n| D5 | Measure + learn | Data + insights | Continue / pivot / stop |\n\n### Success Criteria (Pre-Defined)\n- **Ship it:** [Metric] exceeds [threshold]\n- **Iterate:** [Metric] shows promise but below threshold → run another cycle\n- **Kill it:** [Metric] below [floor] → abandon this direction\n\n### What We're NOT Doing (Scope Control)\n- [Thing we're deliberately skipping to move faster]\n- [Thing we'll add in a future cycle if this works]\n```\n\n### Step 4: AI-Accelerated Workflow (Lenny Rachitsky)\n\nUse AI tools to compress cycle time:\n\n1. **PRD in Markdown** — Keep the spec in a `.md` file that AI agents can reference.\n   Update it as you learn. This becomes your source of truth.\n2. **AI for first drafts** — Use AI to generate copy, specs, test cases, and analysis\n   frameworks. Human reviews and refines.\n3. **4x4 Debugging workflow** — When stuck, try 4 different approaches in 4 minutes\n   each. Don't spend an hour on one dead end.\n4. **Automated analysis** — Use AI to analyze experiment results, user feedback, and\n   metrics immediately after each cycle.\n\n### Step 5: Compound the Learnings\n\nAfter each cycle, update your knowledge base:\n\n```\n## Learning Log — [Date]\n\n**Hypothesis:** [What we believed]\n**Test:** [What we did]\n**Result:** [What happened]\n**Learning:** [What we now know]\n**Next action:** [What we'll do differently]\n**Confidence update:** [How has our conviction changed?]\n```\n\n## Output\n\n```\n# Rapid Iteration Plan — [Project Name]\n\n## Strategy\n[Overall approach to iteration: what we're optimizing for and why speed matters]\n\n## Iteration Cycles\n\n### Cycle 1: [Name] — [X days]\n[Cycle plan as above]\n\n### Cycle 2: [Name] — [X days]\n[Dependent on Cycle 1 learnings — describe both branches]\n\n### Cycle 3: [Name] — [X days]\n[Further refinement or pivot]\n\n## Speed Multipliers\n[Specific practices to move faster: AI tools, scope cuts, parallel work]\n\n## Speed Killers to Avoid\n[Patterns that slow teams down: over-speccing, premature optimization, consensus-seeking]\n\n## Metrics\n- Cycle time: [target days per cycle]\n- Learning velocity: [hypotheses validated per month]\n- Ship frequency: [deploys per week]\n```\n\nSave as `ITERATION-PLAN-[project-name].md`.\n"};

const skillDetails = {
  "prd-generator": { icon:"📋", problem:"Teams waste weeks building the wrong thing because the problem wasn't properly defined upfront. Vague requirements lead to engineering rework, stakeholder misalignment, and shipped features that miss the mark.", input:["Problem statement or feature idea","Target user/persona","Strategic context and constraints","Success criteria or OKRs"], output:["Complete 10-section PRD document","Pre-mortem risk assessment table","Input/output metrics framework","Open questions with resolution paths"], howTo:["Describe the problem you're solving and for whom","Claude asks clarifying questions about constraints, evidence, and timing","A structured PRD is generated with Opportunity Solution Tree framing","Pre-mortem section stress-tests for failure modes","Review, iterate, and share with stakeholders"], bestFor:["New feature kickoffs","Cross-functional alignment before engineering","Quarterly planning inputs","Stakeholder review meetings"] },
  "user-stories": { icon:"📝", problem:"Engineers get vague requirements and come back with 10 clarifying questions. Stories without proper acceptance criteria lead to misbuilt features and QA gaps.", input:["Feature description or PRD","Target personas","Scope (MVP/full release/sprint)","Technical constraints"], output:["JTBD-framed epic statements","INVEST-compliant user stories","Given/When/Then acceptance criteria","Priority-ordered backlog with dependencies"], howTo:["Provide a feature description or link to PRD","Claude frames the work as Jobs-to-be-Done epics","Stories are broken down following INVEST principles","Acceptance criteria written in Given/When/Then format","Stories prioritized P0/P1/P2 with dependencies flagged"], bestFor:["Sprint planning preparation","Breaking down PRDs for engineering","Backlog grooming sessions","Handoff from product to engineering"] },
  "tech-spec": { icon:"⚙️", problem:"PRDs describe what to build but not how. Without a tech spec, engineers make inconsistent architecture decisions and miss cross-cutting concerns like security and performance.", input:["PRD or feature description","Current architecture context","Scale and performance requirements","Team and timeline constraints"], output:["Architecture diagram (Mermaid)","Data model with migrations","API endpoint specifications","Testing and rollout strategy"], howTo:["Share your PRD and current architecture context","Claude generates architecture decisions with trade-off documentation","Data models, API designs, and security considerations are specified","Performance targets and observability plan included","Review with engineering leads before implementation"], bestFor:["Engineering design reviews","New service or API creation","System architecture decisions","Complex feature implementations"] },
  "feature-brief": { icon:"📄", problem:"Writing a full PRD for every idea is overkill. But pitching features without structure leads to rambling meetings and no clear decision. You need a fast, one-page format.", input:["Feature idea (one sentence)","Who requested it and why","Target approver/audience"], output:["One-page feature brief","Three Levels assessment (Impact/Execution/Optics)","Effort vs. Impact positioning","Clear ask with decision deadline"], howTo:["Describe your feature idea in one sentence","Claude asks who needs to approve and what data exists","A concise one-pager is generated with Shreyas Doshi's Three Levels","Effort vs. Impact matrix positions the idea clearly","Clear ask section specifies the exact decision needed"], bestFor:["Getting leadership buy-in before full PRD","Quick feature proposals","Resource allocation decisions","Pitch meetings with executives"] },
  "release-notes": { icon:"🚀", problem:"Engineering changelogs are written for developers, not customers. Users see 'refactored query optimizer' and have no idea how their life improved.", input:["List of shipped features and fixes","Target audience","Format (email, in-app, blog)","Breaking changes if any"], output:["Customer-facing release notes","Benefit-oriented headlines","Migration guides for breaking changes","Optional teaser for upcoming features"], howTo:["Share what shipped this cycle — features, fixes, improvements","Specify the audience (end users, developers, admins)","Claude translates technical changes into user benefits","Breaking changes get empathetic migration guides","Review and publish across channels"], bestFor:["Sprint/release completions","Product update emails","Changelog maintenance","Customer communication"] },
  "api-documentation": { icon:"🔌", problem:"Developers abandon integrations when docs are unclear. Missing examples, undocumented error codes, and no quick-start guide mean more support tickets and fewer integrations.", input:["API spec or endpoint list","Authentication method","Rate limits and environments","Target developer audience"], output:["Complete API reference with examples","Quick-start guide (3 steps)","Error code documentation","Rate limit and pagination guides"], howTo:["Share your API endpoints, auth method, and rate limits","Claude generates a complete reference with curl examples","Quick-start guide gets developers to first successful call","Error handling documented as thoroughly as success cases","Review with developer experience in mind"], bestFor:["New API launches","Developer portal content","API versioning documentation","Partner integration guides"] },
  "customer-feedback": { icon:"💬", problem:"You have 500 support tickets, NPS responses, and app reviews — but no clear picture of what to prioritize. Without systematic analysis, the loudest voices drive the roadmap.", input:["Feedback data (tickets, NPS, reviews, surveys)","Source and time period","Known hypotheses","Decision this informs"], output:["Themed analysis with frequency and severity","Insight statements with evidence","Sentiment overview","Prioritized action recommendations"], howTo:["Provide your feedback data and source context","Claude codes feedback bottom-up into themes","Themes are quantified by frequency, severity, and trend","Insight statements connect patterns to business impact","Quick wins and strategic investments recommended"], bestFor:["Quarterly roadmap planning","Churn analysis investigations","NPS deep-dives","Voice of customer programs"] },
  "jtbd-extractor": { icon:"🎯", problem:"Feature requests tell you what customers think they want — not what they need. Building from requests leads to bloated products that don't solve the underlying job.", input:["Research data (interviews, surveys, tickets)","Domain and product area","Decision context"], output:["JTBD statements (functional, emotional, social)","Desired outcome expectations","Opportunity scoring matrix","Innovation opportunity ranking"], howTo:["Share research data — transcripts, surveys, or feature requests","Claude identifies struggle moments, workarounds, and desired outcomes","Job statements formulated across functional, emotional, and social layers","Outcomes scored by importance vs. satisfaction","Highest-opportunity outcomes highlighted for product strategy"], bestFor:["Product strategy and vision work","Feature ideation workshops","Competitive differentiation","New market entry analysis"] },
  "interview-synthesis": { icon:"🔍", problem:"After 10 user interviews you have 20 hours of transcripts and no clear takeaway. Without systematic synthesis, insights stay locked in individual conversations.", input:["Interview transcripts (1-10)","Research questions","Participant context","Interview guide (optional)"], output:["Per-interview insight summaries","Cross-interview pattern matrix","Teresa Torres opportunity hierarchy","Confidence-rated findings with evidence"], howTo:["Upload interview transcripts and research questions","Claude tags key moments: pains, needs, behaviors, surprises","For batch analysis, patterns are identified across interviews","Findings organized using Teresa Torres's opportunity framework","Recommendations rated by confidence with next-steps"], bestFor:["Post-research synthesis sprints","Discovery cycle outputs","Presenting research to stakeholders","Building opportunity backlogs"] },
  "user-personas": { icon:"👤", problem:"Generic personas with stock photos and fake hobbies don't help anyone make product decisions. Teams need behavioral models rooted in real data.", input:["User data (analytics, interviews, surveys)","Number of personas needed","Decisions these will inform","Current team assumptions"], output:["Behavioral persona profiles","Goals, frustrations, and motivations","Product relationship mapping","Design implications per persona"], howTo:["Share available user data and research","Claude identifies distinct behavioral segments","Personas built around behaviors, not demographics","Each persona includes product-specific implications","Summary comparison table for easy reference"], bestFor:["Product strategy alignment","Onboarding flow design","Feature prioritization by segment","Marketing messaging development"] },
  "competitor-analysis": { icon:"⚔️", problem:"Feature comparison checklists don't tell you if a competitive advantage is sustainable. You need to know what's defensible — not just what exists.", input:["Competitor(s) to analyze","Context (battlecard, planning, pitch)","Available data (demos, reviews, pricing)","Your current positioning"], output:["DHM assessment per competitor","Feature capability matrix","Strategic positioning map (2x2)","Win/loss patterns and action items"], howTo:["Name the competitor(s) and analysis context","Claude builds a feature capability comparison","DHM framework evaluates Delight, Hard-to-copy, Margin-enhancing","Positioning map reveals strategic white space","Actionable recommendations: copy, counter, or ignore"], bestFor:["Sales battlecard creation","Board and investor presentations","Strategic planning sessions","Competitive response planning"] },
  "competitive-landscape": { icon:"🗺️", problem:"Analyzing one competitor at a time misses the big picture. You need a bird's-eye view of who's playing in the space, where the white space is, and where the market is headed.", input:["Market or category definition","Known competitors","Purpose (board deck, planning, market entry)","Time horizon"], output:["Tiered player map","Positioning matrices (2x2)","Feature landscape with table stakes","White space and trend analysis"], howTo:["Define the market boundaries and known players","Claude maps all competitors into tiers","Multiple positioning matrices reveal strategic truths","Feature landscape identifies table stakes vs. differentiators","White space analysis highlights unserved opportunities"], bestFor:["Board presentations","New market entry assessment","Annual strategic planning","Investor materials"] },
  "gtm-strategy": { icon:"📡", problem:"Great products fail because the go-to-market was an afterthought. Without a structured GTM, launches fizzle — wrong audience, wrong channels, wrong message.", input:["What's launching and for whom","Business model (PLG/sales-led/hybrid)","Timeline and budget","Competitive context"], output:["Positioning and messaging hierarchy","Growth model (PLG or sales-led)","Channel strategy with budget allocation","Launch timeline with kill criteria"], howTo:["Describe what's launching and your growth model","Claude builds positioning using your differentiation","PLG skills map through Aakash Gupta's 7 layers","Channel strategy allocates effort across acquisition methods","Launch timeline with pre/during/post phases and kill criteria"], bestFor:["New product launches","Feature launches to new segments","Market expansion planning","Pricing and packaging changes"] },
  "stakeholder-simulation": { icon:"🎭", problem:"PMs get blindsided in stakeholder meetings. The engineering lead raises a concern you didn't prep for, the exec asks about opportunity cost, and suddenly you're improvising.", input:["Your proposal (PRD, brief, plan)","Key stakeholders and their roles","The specific ask (approval, resources, buy-in)","Known political dynamics"], output:["Per-stakeholder reaction profiles","Objection-response matrix","Meeting playbook with opening and flow","BATNA and follow-up strategy"], howTo:["Share your proposal and list the key stakeholders","Claude builds behavioral profiles for each stakeholder","Likely questions and objections predicted per person","Prepared responses crafted with evidence","Full meeting playbook including worst-case scenarios"], bestFor:["Leadership review preparation","Cross-functional alignment meetings","Resource allocation requests","Strategic pivots needing buy-in"] },
  "feature-prioritization": { icon:"📊", problem:"Without structured prioritization, the roadmap is driven by whoever is loudest — the biggest customer, the most senior exec, or the most recent support ticket.", input:["Feature list (5-20 items)","Framework preference (RICE/ICE/Weighted)","Strategic context and OKRs","Time horizon"], output:["Scored and ranked feature table","Trade-off narratives for top items","Opportunity cost analysis","Confidence gaps to resolve"], howTo:["List your features and choose a scoring framework","Claude scores each item with documented rationale","Stack rank presented with natural tiers","Opportunity cost check: what are you saying NO to?","Confidence gaps flagged for validation before committing"], bestFor:["Quarterly roadmap planning","Sprint prioritization","Resource allocation decisions","Stakeholder negotiation prep"] },
  "pre-mortem": { icon:"⚠️", problem:"Teams only analyze what went wrong AFTER a project fails. By then it's too late. The time to identify failure modes is before you start — when you can still prevent them.", input:["Project or initiative description","Timeline and team","Known risks","Stakeholder context"], output:["Categorized failure mode inventory","Likelihood × Impact assessment","Detailed mitigation plans for top 5 risks","Risk monitoring plan with leading indicators"], howTo:["Describe the project and timeline","Claude imagines it's failed and generates failure modes across 5 categories","Each failure mode assessed for likelihood, impact, and detectability","Top 5 risks get full mitigation plans with leading indicators","Monitoring cadence and ownership assigned"], bestFor:["Project kickoffs","Pre-launch risk reviews","Quarterly planning check-ins","High-stakes initiatives"] },
  "spec-challenge": { icon:"🔴", problem:"Specs go to engineering with hidden assumptions, missing edge cases, and untested hypotheses. The cost of finding these in production is 100x the cost of finding them in the spec.", input:["PRD, spec, or feature brief"], output:["Critical issues (must fix)","Important gaps (should fix)","Unanswered questions","Genuine strengths identified"], howTo:["Share your PRD or spec","Claude systematically challenges across 7 dimensions","Problem validity, user understanding, and solution scrutiny examined","Metrics, feasibility, business case, and edge cases tested","Issues categorized by severity with fix recommendations"], bestFor:["Pre-engineering review","Self-review before stakeholder meetings","Peer review support","Spec quality improvement"] },
  "lno-task-prioritizer": { icon:"⚡", problem:"PMs try to do a great job on everything and burn out. The secret is deliberately doing a bad job on low-leverage tasks so you can excel on the ones that matter.", input:["Your task list","Current goals/OKRs","Role level and context","Energy patterns"], output:["Tasks categorized as L, N, or O","Energy-mapped daily schedule","Delegation recommendations","Weekly audit framework"], howTo:["List everything on your plate","Claude applies the LNO test to each task","Leverage tasks get your peak energy hours","Overhead tasks get minimum viable effort","Weekly audit checks if your time allocation matches your priorities"], bestFor:["Overwhelm and burnout prevention","Daily planning","Delegation decisions","Quarterly role reflection"] },
  "opportunity-cost-analyzer": { icon:"⚖️", problem:"Teams fill their roadmap with 'good ROI' items and miss transformational opportunities. The question isn't 'is this worth doing?' — it's 'is this the BEST thing we could do?'", input:["Initiative(s) being evaluated","Alternative options","Constrained resources","Strategic goals and time horizon"], output:["Decision space with alternatives","Opportunity cost matrix","Time sensitivity assessment","Regret minimization analysis"], howTo:["Define what you're evaluating and the alternatives","Claude maps all options competing for the same resources","Each option assessed on direct, strategic, learning, and compounding value","Time sensitivity test identifies expiring opportunities","Regret minimization framework cuts through analysis paralysis"], bestFor:["Roadmap trade-off decisions","Resource allocation debates","'Should we pivot?' discussions","Strategic investment decisions"] },
  "metrics-framework": { icon:"📈", problem:"Teams track 50 metrics but can't answer 'is the product healthy?' Without a framework connecting inputs to outputs, dashboards are decoration — not decision tools.", input:["Product type and business model","Stage (pre-PMF/growth/scale)","Current metrics and instrumentation","Key decisions to inform"], output:["AARRR or Input/Output metrics framework","Metric definitions with measurement methods","Baseline + target table","Dashboard design and review cadence"], howTo:["Describe your product, model, and stage","Choose AARRR (funnel) or Input/Output (causal) framework","Claude defines each metric with precise calculation methods","Baselines captured, targets grounded in data","Implementation checklist ensures nothing is unmeasurable"], bestFor:["New product metrics setup","Metrics overhaul and cleanup","Board reporting frameworks","Team OKR definition"] },
  "north-star-metric": { icon:"⭐", problem:"Without a single unifying metric, teams optimize in different directions. Engineering optimizes for performance, marketing for signups, sales for revenue — but nobody owns the user value.", input:["Product value proposition","Business model","Current metrics","Product stage"], output:["Validated North Star Metric","3-5 input metrics that drive it","Counter-metric to prevent gaming","Usage and review cadence"], howTo:["Describe the value your product delivers to users","Claude evaluates candidate metrics against 5 criteria","Input metrics identified that the team can directly influence","Counter-metric defined to prevent perverse optimization","Validation checks against retention, revenue, and sensitivity"], bestFor:["Product strategy alignment","OKR setting","New product launches","Team unification around metrics"] },
  "experiment-designer": { icon:"🧪", problem:"Poorly designed experiments waste weeks and produce untrustworthy results. Without proper sample sizes, pre-registered criteria, and analysis plans, you're just guessing with extra steps.", input:["Hypothesis to test","Primary and guardrail metrics","Traffic volume","Risk tolerance"], output:["Structured hypothesis statement","Sample size calculation with duration","Test design with control/treatment","Pre-registered analysis plan"], howTo:["State your hypothesis and the decision it informs","Claude structures it as a testable statement","Sample size calculated based on baseline and minimum detectable effect","Test design specified with allocation and exclusions","Analysis plan pre-registered to prevent cherry-picking"], bestFor:["Feature experiment planning","Growth team experimentation","Data-informed product decisions","Validating risky bets before full build"] },
  "ab-test-designer": { icon:"🔬", problem:"A/B tests without pre-committed decision rules lead to goalpost-moving. 'Let's wait for more data' becomes the refrain, and the test runs forever without a decision.", input:["Feature being tested","Hypothesis and primary metric","Baseline rate","Daily traffic"], output:["A/B test brief with variants","Pre-committed decision rules","Timeline with checkpoints","'Should we even A/B test this?' assessment"], howTo:["Describe the change and your hypothesis","Claude calculates sample size and test duration","Decision rules pre-committed before any data","Timeline set with specific checkpoints","If the change shouldn't be A/B tested, alternatives suggested"], bestFor:["Feature rollout decisions","UI/UX optimization","Conversion rate improvement","Growth experiments"] },
  "experiment-analyzer": { icon:"📉", problem:"Raw p-values don't answer 'what should we do?' Teams need help interpreting tricky results — inconclusive tests, guardrail violations, and segment-specific effects.", input:["Test design and hypothesis","Raw results per variant","Pre-registered success criteria","Context (outages, campaigns, etc.)"], output:["Sanity check report","Primary + guardrail metric analysis","Ship/don't ship/iterate recommendation","Next experiment suggestions"], howTo:["Share test design and raw results","Claude runs sanity checks (sample ratio, duration, contamination)","Primary and guardrail metrics analyzed with confidence intervals","Clear ship/no-ship recommendation with rationale","If inconclusive, specific guidance on extending or pivoting"], bestFor:["Post-experiment decision making","Data team collaboration","Growth team reviews","Ambiguous result interpretation"] },
  "funnel-diagnosis": { icon:"🔻", problem:"You know conversion is low but don't know where or why. Without systematic funnel diagnosis, teams fix the wrong step or apply generic 'best practices' that don't fit.", input:["Funnel steps with conversion data","Time period and segments","Current state of the flow"], output:["Funnel visualization with drop-offs","Prioritized hypotheses per step","Benchmarks comparison","Recommended action plan with expected impact"], howTo:["Share your funnel data — steps, users, conversion rates","Claude maps the funnel and identifies the biggest drops","For each drop-off, hypotheses generated across motivation, ability, and trigger","Hypotheses prioritized by evidence, impact, and ease to test","Action plan with modeled impact per recommendation"], bestFor:["Onboarding optimization","Checkout flow improvement","Upgrade/conversion funnels","Feature adoption analysis"] },
  "metrics-debugger": { icon:"🔧", problem:"A metric just dropped 15%. Is it a real product issue, a data pipeline bug, or just Tuesday? Panic is expensive — methodical debugging is cheap.", input:["Metric name and magnitude of change","When it started","Available data sources","Recent changes (deploys, campaigns, etc.)"], output:["Data verification checklist","Impact scoping (segments, platforms)","Root cause analysis","Recommended response with urgency level"], howTo:["Report the metric, magnitude, and timing","Claude first verifies the data is real (pipeline, definition, source)","Impact scoped across platforms, segments, and related metrics","Systematic root cause analysis across 4 categories","Response recommended with urgency level and monitoring plan"], bestFor:["Metrics anomaly investigation","Incident response","Data pipeline debugging","Post-deploy metric monitoring"] },
  "hypothesis-generator": { icon:"💡", problem:"Teams anchor on the first plausible explanation and stop looking. Confirmation bias means you build confidence in the wrong answer instead of exploring alternatives.", input:["The observation to explain","Available data","Current team theory","Ruled-out explanations"], output:["Competing hypotheses across categories","Evidence evaluation matrix","Discriminating test recommendations","Current best guess with confidence"], howTo:["Describe the specific observation precisely","Claude generates hypotheses across 6 categories","Each hypothesis evaluated for supporting and contradicting evidence","Discriminating tests designed to confirm one hypothesis while disproving others","Current best guess presented with honest confidence rating"], bestFor:["Product problem diagnosis","Metrics investigation","Strategic decision support","Root cause analysis for complex issues"] },
  "onboarding-designer": { icon:"🎓", problem:"Users sign up excited and abandon during setup. Every unnecessary step between signup and 'aha moment' is a leak in your activation funnel.", input:["Current onboarding flow","Aha moment definition","User segments","Constraints (data collection, compliance)"], output:["Optimized onboarding flow design","Per-step specifications","Segment-specific variations","Recovery and re-engagement plan"], howTo:["Define your aha moment and share current flow","Claude maps the critical path to value","Every step justified — if you can't explain why it exists, it's cut","Segment variations designed for different user types","Recovery mechanisms planned for users who abandon"], bestFor:["Activation rate improvement","New product onboarding","User segment-specific flows","Trial-to-paid conversion"] },
  "dashboard-designer": { icon:"📊", problem:"Dashboards become metric graveyards — every number dumped on one screen. If the viewer has to think about what they're looking at, the dashboard has failed.", input:["Target audience","Questions dashboard should answer","Decisions it supports","Available data sources"], output:["3-level information hierarchy","Layout wireframe","Metric inventory with thresholds","Visualization and interaction specs"], howTo:["Define who views this and what questions they need answered","Claude designs a 3-level hierarchy: glance (3s), scan (30s), analyze (5min)","Layout wireframed with metric placement and chart types","Alert thresholds defined (green/yellow/red) per metric","Tool recommendation and technical requirements specified"], bestFor:["Executive dashboards","Product health monitoring","Team-specific metric views","Customer-facing analytics"] },
  "daily-planner": { icon:"☀️", problem:"PMs start the day with 'I'll just check email' and end it having attended 8 meetings without touching their most important work.", input:["Today's tasks and meetings","Current goals/OKRs","Peak energy time","How you're feeling today"], output:["LNO-categorized daily plan","Time-blocked schedule","Meeting prep notes","End-of-day review prompts"], howTo:["List what's on your plate today","Claude categorizes everything as Leverage, Neutral, or Overhead","Leverage work scheduled during your peak energy block","Overhead batched into low-energy time slots","End-of-day check ensures your leverage task got done"], bestFor:["Morning planning ritual","Overwhelm recovery","Focus block protection","Energy management"] },
  "weekly-planner": { icon:"📅", problem:"Without weekly planning, days are reactive — driven by whoever Slacks you first. The week ends with meetings attended but no progress on what actually matters.", input:["Quarterly goals/OKRs","Last week's carryover","This week's calendar","Blockers and dependencies"], output:["Top 3 weekly priorities tied to OKRs","Day-by-day focus blueprint","Explicit 'saying no' list","Friday review prompts"], howTo:["Share your quarterly goals and this week's calendar","Claude connects weekly work to quarterly outcomes","Top 3 must-complete priorities defined with done criteria","Day-by-day blueprint maps deep work to meeting-free blocks","Friday review prompts close the loop on learning"], bestFor:["Monday morning planning","OKR progress tracking","Calendar audit and protection","Quarterly goal alignment"] },
  "meeting-agenda": { icon:"📋", problem:"Meetings without agendas are group therapy with calendar invites. They end with 'good discussion' but no decisions, no action items, and no progress.", input:["Meeting purpose and type","Attendees and their roles","Duration","Topics to cover"], output:["SCQA-framed agenda","Time-blocked schedule","Pre-read assignments","Post-meeting action template"], howTo:["Specify the meeting type, attendees, and objectives","Claude frames each topic using SCQA structure","Time blocks allocated based on priority and decision needs","Pre-reads assigned so meeting time is for discussion, not presentation","Action items template ensures clear follow-through"], bestFor:["Decision meetings","Stakeholder reviews","Sprint planning","Leadership presentations"] },
  "launch-checklist": { icon:"🚀", problem:"Launches fail when someone forgets to brief support, instrument analytics, or prepare rollback procedures. The cost of a missed checklist item is measured in customer trust.", input:["What's launching","Launch type (big bang/phased/beta)","Date and risk level","Teams involved"], output:["Phased checklist (T-2wk, T-1wk, launch, post)","Per-team responsibilities","Kill criteria and rollback plan","Post-launch review prompts"], howTo:["Describe what's launching, type, and risk level","Claude generates phase-appropriate checklists","Engineering, design, QA, marketing, support, and legal all covered","Kill criteria defined for automatic rollback","Post-launch review ensures learnings are captured"], bestFor:["Feature launches","Major releases","Beta/early access programs","Hotfix deployments"] },
  "stakeholder-update": { icon:"📨", problem:"Status updates that list activities instead of outcomes erode trust. Nobody cares how many meetings you had — they care whether the project is on track.", input:["Project status and audience","Progress, blockers, decisions","Asks or decisions needed","Cadence (weekly/monthly/ad hoc)"], output:["SCQA-structured executive update","Metrics snapshot table","Risk/blocker escalation","Clear asks with specifics"], howTo:["Share what happened and who needs to know","Claude structures the update using SCQA framework","Status led with green/yellow/red in first line","Bad news owned directly — no passive voice","Asks made specific and actionable, not vague"], bestFor:["Weekly executive updates","Cross-functional status reports","Board reporting","Escalation communications"] },
  "ai-opportunity-finder": { icon:"🤖", problem:"Every product team is being told to 'add AI' but most AI features are solutions looking for problems. You need to find where AI genuinely improves user outcomes vs. where it's theater.", input:["Product description and workflows","User pain points","Available data","Team AI/ML capability"], output:["Scored opportunity map","Build/buy/partner recommendations","Phased implementation roadmap","'What NOT to build' list with reasoning"], howTo:["Describe your product, core workflows, and pain points","Claude maps each workflow for AI fit across user value dimensions","Good vs. bad AI candidates evaluated honestly","Top opportunities scored by value, feasibility, and risk","Phased roadmap: augmentation first, automation after trust is built"], bestFor:["AI strategy formulation","Product roadmap AI integration","Technical feasibility assessment","Build vs. buy decisions"] },
  "ai-product-review": { icon:"👁️", problem:"PMs build features assuming users will love them. In reality, users get confused, contact support, and revert to the old way. You need a user-perspective review BEFORE building.", input:["PRD or spec to review","Target personas","Current user behavior","Success metrics"], output:["Adoption prediction (High/Med/Low)","Per-persona usability walkthrough","Support burden forecast","Top 3 highest-impact improvements"], howTo:["Share your spec and target personas","Claude walks through the flow as each persona","Discovery, trial, and retention likelihood assessed","Support burden predicted with specific issue types","Top improvements ranked by user impact and effort"], bestFor:["Pre-build spec validation","Design review preparation","Launch readiness assessment","Customer experience prediction"] },
  "rapid-iteration-planner": { icon:"🏃", problem:"Teams spend 4 weeks planning, 4 weeks building, and 2 weeks debating the results. By then the market has moved. Speed of iteration beats quality of planning.", input:["What you're iterating on","Goal or hypothesis","Team size","Current shipping cadence"], output:["Iteration cycle plans","Minimum viable test design","AI-accelerated workflow","Learning log template"], howTo:["Define what you're iterating on and the learning goal","Claude designs the minimum viable test (fake door → wizard of oz → prototype)","Day-by-day cycle plan with clear decision points","AI workflow shortcuts identified for speed","Learning log template compounds insights across cycles"], bestFor:["MVP validation","Growth experiment cycles","Feature iteration sprints","New market exploration"] }
};

const skills = [
  // 01 - Docs & Planning
  { id: "prd-generator", title: "PRD Generator", category: "docs", catLabel: "Docs & Planning", command: "/prd", desc: "Transform a problem statement into a structured, engineering-ready Product Requirements Document with stakeholder alignment built in.", frameworks: ["Teresa Torres OST", "Shreyas Doshi Pre-Mortem", "Aakash Gupta Input/Output"], triggers: ["PRD", "product requirements", "spec", "feature spec", "requirements document"], tags: ["shreyas", "torres", "aakash"], folder: "01-product-docs-planning/prd-generator" },
  { id: "user-stories", title: "User Stories Generator", category: "docs", catLabel: "Docs & Planning", command: "/stories", desc: "Write clear, engineering-ready user stories with acceptance criteria following INVEST principles, framed by Jobs-to-be-Done.", frameworks: ["INVEST Principles", "Clayton Christensen JTBD"], triggers: ["user stories", "acceptance criteria", "backlog items", "tickets"], tags: [], folder: "01-product-docs-planning/user-stories" },
  { id: "tech-spec", title: "Technical Specification", category: "docs", catLabel: "Docs & Planning", command: "/techspec", desc: "Write technical specifications with architecture decisions, data models, API designs, and implementation plans that bridge PRD to code.", frameworks: [], triggers: ["tech spec", "architecture doc", "system design", "engineering spec"], tags: [], folder: "01-product-docs-planning/tech-spec" },
  { id: "feature-brief", title: "Feature Brief", category: "docs", catLabel: "Docs & Planning", command: "/brief", desc: "Create a one-page feature brief for stakeholder alignment using Shreyas Doshi's Three Levels of Product Work: Impact, Execution, Optics.", frameworks: ["Shreyas Doshi Three Levels"], triggers: ["feature brief", "one-pager", "feature pitch", "concept doc"], tags: ["shreyas"], folder: "01-product-docs-planning/feature-brief" },
  { id: "release-notes", title: "Release Notes", category: "docs", catLabel: "Docs & Planning", command: "/release", desc: "Write customer-facing release notes that highlight value over features. Translate engineering changelogs into benefits users care about.", frameworks: [], triggers: ["release notes", "changelog", "what's new", "product update"], tags: [], folder: "01-product-docs-planning/release-notes" },
  { id: "api-documentation", title: "API Documentation", category: "docs", catLabel: "Docs & Planning", command: "/apidocs", desc: "Write developer-friendly API documentation with authentication, endpoints, request/response examples, error handling, and rate limits.", frameworks: [], triggers: ["API docs", "endpoint documentation", "developer docs", "API reference"], tags: [], folder: "01-product-docs-planning/api-documentation" },
  // 02 - Research & Discovery
  { id: "customer-feedback", title: "Customer Feedback Analyzer", category: "research", catLabel: "Research & Discovery", command: "/feedback", desc: "Analyze and categorize customer feedback into actionable themes using bottom-up affinity mapping. Works with support tickets, NPS, reviews, and surveys.", frameworks: ["Affinity Mapping"], triggers: ["customer feedback", "NPS analysis", "app reviews", "voice of customer"], tags: [], folder: "02-research-discovery/customer-feedback" },
  { id: "jtbd-extractor", title: "JTBD Extractor", category: "research", catLabel: "Research & Discovery", command: "/jtbd", desc: "Extract Jobs-to-be-Done statements from research data to uncover innovation opportunities. Move beyond feature requests to underlying motivations.", frameworks: ["Clayton Christensen JTBD"], triggers: ["jobs to be done", "JTBD", "customer jobs", "outcome-driven innovation"], tags: [], folder: "02-research-discovery/jtbd-extractor" },
  { id: "interview-synthesis", title: "Interview Synthesis", category: "research", catLabel: "Research & Discovery", command: "/synthesize", desc: "Transform interview transcripts into structured insights with quotes, patterns, and recommendations. Handles batch analysis of 5-10 interviews using Teresa Torres's framework.", frameworks: ["Teresa Torres Continuous Discovery"], triggers: ["interview synthesis", "research synthesis", "transcript analysis"], tags: ["torres"], folder: "02-research-discovery/interview-synthesis" },
  { id: "user-personas", title: "User Personas", category: "research", catLabel: "Research & Discovery", command: "/personas", desc: "Create data-backed user personas that go beyond demographics to capture behaviors, motivations, and jobs-to-be-done.", frameworks: [], triggers: ["personas", "user profiles", "user segments", "customer profiles"], tags: [], folder: "02-research-discovery/user-personas" },
  // 03 - Strategy & Competitive
  { id: "competitor-analysis", title: "Competitor Analysis (DHM)", category: "strategy", catLabel: "Strategy & Competitive", command: "/compete", desc: "Comprehensive competitor analysis using Gibson Biddle's DHM framework: Delight, Hard-to-copy, Margin-enhancing — evaluating sustainable advantages.", frameworks: ["Gibson Biddle DHM"], triggers: ["competitor analysis", "competitive intelligence", "battlecard"], tags: ["biddle"], folder: "03-strategy-competitive/competitor-analysis" },
  { id: "competitive-landscape", title: "Competitive Landscape", category: "strategy", catLabel: "Strategy & Competitive", command: "/landscape", desc: "Map multiple competitors into a unified landscape with positioning matrices, feature comparisons, white space analysis, and market segment breakdown.", frameworks: [], triggers: ["competitive landscape", "market map", "industry landscape"], tags: [], folder: "03-strategy-competitive/competitive-landscape" },
  { id: "gtm-strategy", title: "Go-to-Market Strategy", category: "strategy", catLabel: "Strategy & Competitive", command: "/gtm", desc: "Build a complete GTM plan with target segments, channels, messaging, pricing, launch timeline, and Aakash Gupta's 7-layer PLG framework.", frameworks: ["Aakash Gupta PLG Layers"], triggers: ["go to market", "GTM", "launch strategy", "product launch plan"], tags: ["aakash"], folder: "03-strategy-competitive/gtm-strategy" },
  { id: "stakeholder-simulation", title: "Stakeholder Simulation", category: "strategy", catLabel: "Strategy & Competitive", command: "/simulate", desc: "Simulate how different stakeholders will react to your proposal. Anticipate objections, prepare responses, and build a meeting playbook.", frameworks: [], triggers: ["stakeholder simulation", "objection handling", "meeting prep"], tags: [], folder: "03-strategy-competitive/stakeholder-simulation" },
  // 04 - Prioritization & Decisions
  { id: "feature-prioritization", title: "Feature Prioritization", category: "prioritization", catLabel: "Prioritization", command: "/prioritize", desc: "Score and rank features using RICE, ICE, or weighted scoring with clear documentation of trade-offs and Shreyas Doshi's opportunity cost check.", frameworks: ["RICE", "ICE", "Weighted Scoring"], triggers: ["prioritization", "RICE scoring", "feature ranking", "backlog prioritization"], tags: ["shreyas"], folder: "04-prioritization-decisions/feature-prioritization" },
  { id: "pre-mortem", title: "Pre-Mortem", category: "prioritization", catLabel: "Prioritization", command: "/premortem", desc: "Shreyas Doshi's pre-mortem: imagine the project has failed and work backward to identify and mitigate failure modes before they happen.", frameworks: ["Shreyas Doshi Pre-Mortem"], triggers: ["pre-mortem", "risk analysis", "failure analysis", "project risks"], tags: ["shreyas"], folder: "04-prioritization-decisions/pre-mortem" },
  { id: "spec-challenge", title: "Spec Challenger", category: "prioritization", catLabel: "Prioritization", command: "/challenge", desc: "Red-team your PRDs and specs to find blind spots, hidden assumptions, and potential failure scenarios before engineering starts building.", frameworks: [], triggers: ["spec review", "PRD review", "red team", "devil's advocate"], tags: [], folder: "04-prioritization-decisions/spec-challenge" },
  { id: "lno-task-prioritizer", title: "LNO Task Prioritizer", category: "prioritization", catLabel: "Prioritization", command: "/lno", desc: "Categorize tasks as Leverage (10-100x return), Neutral (1x), or Overhead (<1x) using Shreyas Doshi's LNO framework to maximize daily impact.", frameworks: ["Shreyas Doshi LNO"], triggers: ["LNO", "leverage neutral overhead", "energy management"], tags: ["shreyas"], folder: "04-prioritization-decisions/lno-task-prioritizer" },
  { id: "opportunity-cost-analyzer", title: "Opportunity Cost Analyzer", category: "prioritization", catLabel: "Prioritization", command: "/oppcost", desc: "Shift from ROI thinking to opportunity cost thinking. 'Is this the BEST use of our time?' — inspired by Shreyas Doshi and Patrick Collison.", frameworks: ["Shreyas Doshi Opp Cost"], triggers: ["opportunity cost", "trade-off analysis", "resource allocation"], tags: ["shreyas"], folder: "04-prioritization-decisions/opportunity-cost-analyzer" },
  // 05 - Metrics & Experimentation
  { id: "metrics-framework", title: "Metrics Framework", category: "metrics", catLabel: "Metrics & Experiments", command: "/metrics", desc: "Build comprehensive metrics frameworks using AARRR pirate metrics or Aakash Gupta's input/output methodology to connect product decisions to business outcomes.", frameworks: ["AARRR", "Aakash Gupta Input/Output"], triggers: ["metrics framework", "AARRR", "pirate metrics", "KPIs"], tags: ["aakash"], folder: "05-metrics-experimentation/metrics-framework" },
  { id: "north-star-metric", title: "North Star Metric", category: "metrics", catLabel: "Metrics & Experiments", command: "/northstar", desc: "Identify and validate your product's North Star metric — the single number that captures the value you deliver to customers — with supporting input metrics.", frameworks: [], triggers: ["north star metric", "NSM", "one metric that matters"], tags: [], folder: "05-metrics-experimentation/north-star-metric" },
  { id: "experiment-designer", title: "Experiment Designer", category: "metrics", catLabel: "Metrics & Experiments", command: "/experiment", desc: "Design statistically sound experiments with clear hypotheses, success criteria, sample size calculations, and pre-registered analysis plans.", frameworks: [], triggers: ["experiment design", "hypothesis testing", "sample size"], tags: [], folder: "05-metrics-experimentation/experiment-designer" },
  { id: "ab-test-designer", title: "A/B Test Designer", category: "metrics", catLabel: "Metrics & Experiments", command: "/abtest", desc: "Design A/B tests with proper methodology, sample sizes, success criteria, and pre-committed decision rules for ship/no-ship outcomes.", frameworks: [], triggers: ["A/B test", "split test", "feature test", "variant test"], tags: [], folder: "05-metrics-experimentation/ab-test-designer" },
  { id: "experiment-analyzer", title: "Experiment Analyzer", category: "metrics", catLabel: "Metrics & Experiments", command: "/results", desc: "Interpret experiment results with statistical rigor. Handles nuanced scenarios: inconclusive results, guardrail violations, segment-specific effects.", frameworks: [], triggers: ["experiment results", "A/B test results", "test analysis"], tags: [], folder: "05-metrics-experimentation/experiment-analyzer" },
  // 06 - Analysis & Diagnostics
  { id: "funnel-diagnosis", title: "Funnel Diagnosis", category: "diagnostics", catLabel: "Analysis & Diagnostics", command: "/funnel", desc: "Diagnose conversion funnel problems and generate data-backed improvement hypotheses. Works with signup, onboarding, checkout, and upgrade flows.", frameworks: [], triggers: ["funnel analysis", "conversion diagnosis", "drop-off analysis"], tags: [], folder: "06-analysis-diagnostics/funnel-diagnosis" },
  { id: "metrics-debugger", title: "Metrics Debugger", category: "diagnostics", catLabel: "Analysis & Diagnostics", command: "/debug", desc: "Debug metrics drops with first principles. Systematically identify whether it's a real product issue, data pipeline problem, or external factor.", frameworks: [], triggers: ["metrics drop", "KPI drop", "dashboard anomaly"], tags: [], folder: "06-analysis-diagnostics/metrics-debugger" },
  { id: "hypothesis-generator", title: "Hypothesis Generator", category: "diagnostics", catLabel: "Analysis & Diagnostics", command: "/hypotheses", desc: "Generate competing hypotheses for product problems, evaluate evidence for each, and recommend discriminating tests. Prevents confirmation bias.", frameworks: [], triggers: ["hypothesis generation", "root cause analysis", "competing hypotheses"], tags: [], folder: "06-analysis-diagnostics/hypothesis-generator" },
  // 07 - Design & UX
  { id: "onboarding-designer", title: "Onboarding Designer", category: "design", catLabel: "Design & UX", command: "/onboard", desc: "Design onboarding flows that guide users to their 'aha moment' as fast as possible. Focused on time-to-value optimization and activation rate improvement.", frameworks: [], triggers: ["onboarding", "first-time experience", "FTUE", "activation flow"], tags: [], folder: "07-design-ux/onboarding-designer" },
  { id: "dashboard-designer", title: "Dashboard Designer", category: "design", catLabel: "Design & UX", command: "/dashboard", desc: "Design dashboards with the right metrics for the right audience. Three-level hierarchy: glance (3s), scan (30s), analyze (5min).", frameworks: [], triggers: ["dashboard design", "analytics dashboard", "metrics dashboard"], tags: [], folder: "07-design-ux/dashboard-designer" },
  // 08 - Productivity & Operations
  { id: "daily-planner", title: "Daily Planner", category: "productivity", catLabel: "Productivity & Ops", command: "/today", desc: "Generate your daily plan aligned to goals using Shreyas Doshi's LNO framework. Peak energy on Leverage work, batch Overhead at low energy.", frameworks: ["Shreyas Doshi LNO"], triggers: ["daily plan", "today's priorities", "morning planning"], tags: ["shreyas"], folder: "08-productivity-operations/daily-planner" },
  { id: "weekly-planner", title: "Weekly Planner", category: "productivity", catLabel: "Productivity & Ops", command: "/week", desc: "Generate weekly priorities aligned to quarterly goals. Know what matters this week, what can wait, and what you're explicitly saying no to.", frameworks: [], triggers: ["weekly plan", "weekly priorities", "week planning"], tags: [], folder: "08-productivity-operations/weekly-planner" },
  { id: "meeting-agenda", title: "Meeting Agenda", category: "productivity", catLabel: "Productivity & Ops", command: "/agenda", desc: "Generate structured meeting agendas with time blocks, objectives, and SCQA-framed discussion topics. Every meeting ends with clear decisions.", frameworks: ["SCQA Framework"], triggers: ["meeting agenda", "meeting prep", "meeting structure"], tags: [], folder: "08-productivity-operations/meeting-agenda" },
  { id: "launch-checklist", title: "Launch Checklist", category: "productivity", catLabel: "Productivity & Ops", command: "/launch", desc: "Comprehensive launch checklists customized to release type. Covers engineering, design, marketing, support, legal, and analytics readiness.", frameworks: [], triggers: ["launch checklist", "release checklist", "go-live checklist"], tags: [], folder: "08-productivity-operations/launch-checklist" },
  { id: "stakeholder-update", title: "Stakeholder Update", category: "productivity", catLabel: "Productivity & Ops", command: "/update", desc: "Write executive stakeholder updates using SCQA framework. Lead with status, own bad news, be specific about asks.", frameworks: ["SCQA Framework"], triggers: ["stakeholder update", "status update", "executive update"], tags: [], folder: "08-productivity-operations/stakeholder-update" },
  // 09 - AI Product Strategy
  { id: "ai-opportunity-finder", title: "AI Opportunity Finder", category: "ai", catLabel: "AI Product Strategy", command: "/aiopp", desc: "Identify where AI creates genuine user value vs. where it's hype. Pragmatic evaluation inspired by Nate B Jones's AI-first product strategy.", frameworks: ["Nate B Jones AI Strategy"], triggers: ["AI strategy", "AI opportunities", "ML opportunities", "where to use AI"], tags: ["nate"], folder: "09-ai-product-strategy/ai-opportunity-finder" },
  { id: "ai-product-review", title: "AI Product Review", category: "ai", catLabel: "AI Product Strategy", command: "/aireview", desc: "Review PRDs from the user's perspective to predict adoption, support burden, and satisfaction impact before building.", frameworks: [], triggers: ["product review", "adoption prediction", "customer impact review"], tags: [], folder: "09-ai-product-strategy/ai-product-review" },
  { id: "rapid-iteration-planner", title: "Rapid Iteration Planner", category: "ai", catLabel: "AI Product Strategy", command: "/iterate", desc: "Plan fast iteration cycles with AI-first workflows. Volume over perfection (Nate B Jones), PRD+Markdown for AI alignment (Lenny Rachitsky).", frameworks: ["Nate B Jones", "Lenny Rachitsky AI-First"], triggers: ["rapid iteration", "fast shipping", "build-measure-learn"], tags: ["nate", "lenny"], folder: "09-ai-product-strategy/rapid-iteration-planner" }
];

const catColors = {
  docs: 'cat-docs', research: 'cat-research', strategy: 'cat-strategy',
  prioritization: 'cat-prioritization', metrics: 'cat-metrics',
  diagnostics: 'cat-diagnostics', design: 'cat-design',
  productivity: 'cat-productivity', ai: 'cat-ai'
};

let activeFilter = 'all';
let activeFramework = null;
let searchQuery = '';

function renderSkills() {
  const grid = document.getElementById('skillsGrid');
  const empty = document.getElementById('emptyState');
  const title = document.getElementById('mainTitle');
  const subtitle = document.getElementById('mainSubtitle');
  const countEl = document.getElementById('skillCount');

  let filtered = skills;

  if (activeFilter !== 'all') {
    filtered = filtered.filter(s => s.category === activeFilter);
  }
  if (activeFramework) {
    filtered = filtered.filter(s => s.tags.includes(activeFramework));
  }
  if (searchQuery) {
    const q = searchQuery.toLowerCase();
    filtered = filtered.filter(s =>
      s.title.toLowerCase().includes(q) ||
      s.desc.toLowerCase().includes(q) ||
      s.command.toLowerCase().includes(q) ||
      s.triggers.some(t => t.toLowerCase().includes(q)) ||
      s.frameworks.some(f => f.toLowerCase().includes(q)) ||
      s.catLabel.toLowerCase().includes(q)
    );
  }

  countEl.textContent = filtered.length;

  if (activeFilter !== 'all') {
    const cat = filtered[0]?.catLabel || '';
    title.textContent = cat;
    subtitle.textContent = `${filtered.length} skills`;
  } else if (activeFramework) {
    const names = {shreyas:'Shreyas Doshi',lenny:'Lenny Rachitsky',aakash:'Aakash Gupta',torres:'Teresa Torres',biddle:'Gibson Biddle',nate:'Nate B Jones'};
    title.textContent = names[activeFramework] || '';
    subtitle.textContent = `${filtered.length} skills using this framework`;
  } else if (searchQuery) {
    title.textContent = 'Search Results';
    subtitle.textContent = `${filtered.length} skills found`;
  } else {
    title.textContent = 'All Skills';
    subtitle.textContent = '37 skills across 9 categories';
  }

  if (filtered.length === 0) {
    grid.innerHTML = '';
    empty.style.display = 'block';
    return;
  }
  empty.style.display = 'none';

  grid.innerHTML = filtered.map(s => `
    <div class="skill-card" onclick="openModal('${s.id}')">
      <div class="skill-card-top">
        <span class="skill-badge ${catColors[s.category]}">${s.catLabel}</span>
        <span class="skill-command">${s.command}</span>
      </div>
      <div class="skill-title">${s.title}</div>
      <div class="skill-desc">${s.desc}</div>
      <div class="skill-footer">
        <div class="skill-frameworks">
          ${s.frameworks.slice(0,2).map(f => `<span class="framework-tag">${f}</span>`).join('')}
        </div>
        <div class="skill-action">
          View
          <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M5 12h14M12 5l7 7-7 7"/></svg>
        </div>
      </div>
    </div>
  `).join('');
}

function openModal(id) {
  const s = skills.find(x => x.id === id);
  if (!s) return;
  const d = skillDetails[id] || {};
  const path = `claude-skills-library/${s.folder}`;
  document.getElementById('modalContent').innerHTML = `
    <div class="detail-header">
      <div class="detail-icon">${d.icon || '📦'}</div>
      <div class="detail-meta">
        <span class="detail-badge ${catColors[s.category]}">${s.catLabel}</span>
        <div class="detail-title">${s.title}</div>
        <div class="detail-command">/${s.id}</div>
      </div>
    </div>

    ${d.problem ? `
    <div class="detail-section">
      <div class="detail-section-title">The Problem</div>
      <div class="problem-card">${d.problem}</div>
    </div>` : ''}

    <div class="detail-section">
      <div class="detail-section-title">Skill Summary</div>
      <div style="color:#cdd6f4;line-height:1.7;font-size:0.95rem;">${s.desc}</div>
    </div>

    ${d.input && d.output ? `
    <div class="detail-section">
      <div class="detail-section-title">Input & Output</div>
      <div class="io-grid">
        <div class="io-card">
          <div class="io-card-title">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M12 5v14M5 12l7-7 7 7"/></svg>
            What you provide
          </div>
          <ul class="io-card-list">${d.input.map(i => `<li>${i}</li>`).join('')}</ul>
        </div>
        <div class="io-card">
          <div class="io-card-title">
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M12 19V5M5 12l7 7 7-7"/></svg>
            What you get
          </div>
          <ul class="io-card-list">${d.output.map(o => `<li>${o}</li>`).join('')}</ul>
        </div>
      </div>
    </div>` : ''}

    ${d.howTo ? `
    <div class="detail-section">
      <div class="detail-section-title">How It Works</div>
      <ol class="steps-list">${d.howTo.map((step, i) => `<li><span class="step-num">${i+1}</span><span>${step}</span></li>`).join('')}</ol>
    </div>` : ''}

    ${d.bestFor ? `
    <div class="detail-section">
      <div class="detail-section-title">Best Used For</div>
      <div class="bestfor-grid">${d.bestFor.map(b => `<span class="bestfor-tag">${b}</span>`).join('')}</div>
    </div>` : ''}

    <div class="detail-divider"></div>

    <div class="detail-section">
      <div class="detail-section-title">How to Use This Skill</div>
      <ol class="steps-list steps-list-setup">
        <li>
          <span class="step-num">1</span>
          <span>Download <code class="inline-code">${s.id}.zip</code> using the button below</span>
        </li>
        <li>
          <span class="step-num">2</span>
          <span>Unzip and move the <code class="inline-code">${s.id}/</code> folder into your project's <code class="inline-code">.claude/skills/</code> directory</span>
        </li>
        <li>
          <span class="step-num">3</span>
          <span>Type <code class="inline-code">/${s.id}</code> in Claude to run the skill</span>
        </li>
      </ol>
    </div>

    <div class="detail-install">
      <button class="download-btn" onclick="downloadSkill('${s.id}', this)">
        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 17v2a2 2 0 002 2h12a2 2 0 002-2v-2M7 11l5 5 5-5M12 4v12"/></svg>
        Download ${s.id}.zip
      </button>
      <div class="install-alt">
        <span class="install-alt-label">Or unzip manually and copy:</span>
        <div class="install-code-row">
          <code id="installCode">unzip ${s.id}.zip -d ~/.claude/skills/</code>
          <button class="copy-btn" onclick="copyInstall(this)">Copy</button>
        </div>
      </div>
    </div>

    ${s.frameworks.length ? `
    <div class="detail-section">
      <div class="detail-section-title">Frameworks</div>
      <div class="detail-frameworks">
        ${s.frameworks.map(f => `<div class="detail-framework">${f}</div>`).join('')}
      </div>
    </div>` : ''}

    <div class="detail-section">
      <div class="detail-section-title">Trigger Keywords</div>
      <div class="detail-triggers">
        ${s.triggers.map(t => `<span class="detail-trigger">${t}</span>`).join('')}
      </div>
    </div>
  `;
  document.getElementById('modalOverlay').classList.add('open');
  document.body.style.overflow = 'hidden';
}

function closeModal() {
  document.getElementById('modalOverlay').classList.remove('open');
  document.body.style.overflow = '';
}

function copyInstall(btn) {
  const code = document.getElementById('installCode').textContent;
  navigator.clipboard.writeText(code).then(() => {
    btn.textContent = 'Copied!';
    btn.classList.add('copied');
    setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
  });
}

function buildSkillMd(id) {
  if (skillFiles[id]) return skillFiles[id];
  // Fallback: generate from metadata if file not embedded
  const s = skills.find(x => x.id === id);
  const d = skillDetails[id];
  if (!s || !d) return '';
  let md = `---\nname: ${s.id}\ndescription: >\n  ${s.desc}\n---\n\n# ${s.title}\n\n`;
  if (d.problem) md += `## The Problem\n\n${d.problem}\n\n`;
  md += `## Inputs\n\n`;
  d.input.forEach((inp, i) => { md += `${i+1}. ${inp}\n`; });
  md += `\n## Outputs\n\n`;
  d.output.forEach(out => { md += `- ${out}\n`; });
  md += `\n## Steps\n\n`;
  d.howTo.forEach((step, i) => { md += `${i+1}. ${step}\n`; });
  return md;
}

async function downloadSkill(id, btn) {
  const s = skills.find(x => x.id === id);
  if (!s) return;
  const md = buildSkillMd(id);
  if (!md) return;

  const origHTML = btn ? btn.innerHTML : '';
  if (btn) {
    btn.innerHTML = `<svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><path d="M12 6v6l4 2"/></svg> Preparing...`;
    btn.style.pointerEvents = 'none';
  }

  try {
    const zip = new JSZip();
    zip.file(`${s.id}/SKILL.md`, md);
    const blob = await zip.generateAsync({ type: 'blob' });

    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `${s.id}.zip`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);

    if (btn) {
      btn.innerHTML = `<svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M20 6L9 17l-5-5"/></svg> Downloaded ${s.id}.zip`;
      btn.style.background = 'var(--green)';
      btn.style.pointerEvents = '';
      setTimeout(() => { btn.innerHTML = origHTML; btn.style.background = ''; }, 3000);
    }
  } catch (err) {
    if (btn) {
      btn.innerHTML = origHTML;
      btn.style.pointerEvents = '';
    }
    console.error('Download failed:', err);
  }
}

// Filter handlers
document.querySelectorAll('[data-filter]').forEach(btn => {
  btn.addEventListener('click', () => {
    document.querySelectorAll('[data-filter]').forEach(b => b.classList.remove('active'));
    document.querySelectorAll('[data-framework]').forEach(b => b.classList.remove('active'));
    btn.classList.add('active');
    activeFilter = btn.dataset.filter;
    activeFramework = null;
    renderSkills();
  });
});

document.querySelectorAll('[data-framework]').forEach(btn => {
  btn.addEventListener('click', () => {
    document.querySelectorAll('[data-filter]').forEach(b => b.classList.remove('active'));
    document.querySelectorAll('[data-framework]').forEach(b => b.classList.remove('active'));
    btn.classList.add('active');
    activeFramework = btn.dataset.framework;
    activeFilter = 'all';
    renderSkills();
  });
});

// Search
document.getElementById('searchInput').addEventListener('input', (e) => {
  searchQuery = e.target.value;
  if (searchQuery) {
    document.querySelectorAll('[data-filter]').forEach(b => b.classList.remove('active'));
    document.querySelectorAll('[data-framework]').forEach(b => b.classList.remove('active'));
    activeFilter = 'all';
    activeFramework = null;
  } else {
    document.querySelector('[data-filter="all"]').classList.add('active');
  }
  renderSkills();
});

// Modal close
document.getElementById('modalClose').addEventListener('click', closeModal);
document.getElementById('modalOverlay').addEventListener('click', (e) => {
  if (e.target === e.currentTarget) closeModal();
});
document.addEventListener('keydown', (e) => { if (e.key === 'Escape') closeModal(); });

// Initial render
renderSkills();
</script>
</body>
</html>
